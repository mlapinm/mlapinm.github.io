Non-linear Hypotheses.В этом и в следующем наборе видео я хотел бы сказать вам о алгоритме обучения под названием Нейронная сеть.
Play video starting at ::7 and follow transcript0:07
Мы сначала поговорим о представлении, а затем в следующем наборе видео расскажем об алгоритмах обучения для него. Нейтральные сети на самом деле довольно старая идея, но были выпали из благосклонности на некоторое время. Но сегодня это современная техника для многих различных проблем машинного обучения.
Play video starting at ::23 and follow transcript0:23
Так зачем нам нужен еще один алгоритм обучения? У нас уже есть линейная регрессия и у нас логистическая регрессия, так зачем нам нужны нейронные сети?
Play video starting at ::32 and follow transcript0:32
Для того, чтобы мотивировать дискуссию нейронных сетей, позвольте мне начать с показа нескольких примеров проблем машинного обучения , где нам нужно, чтобы изучить сложные нелинейные гипотезы.
Play video starting at ::43 and follow transcript0:43
Рассмотрим проблему с контролируемой классификацией обучения , когда у вас есть такой обучающий набор. Если вы хотите применить логистическую регрессию к этой проблеме, одна вещь , которую вы могли бы сделать, это применить логистическую регрессию с большим количеством нелинейных функций, подобных этому. Итак, здесь g, как обычно является сигмоидной функцией, и мы можем включить множество полиномиальных терминов, подобных этим. И, если вы включите достаточное количество полиномов , то, знаете, возможно, вы можете получить гипотезы
Play video starting at :1:11 and follow transcript1:11
, разделяющие положительные и отрицательные примеры. Этот конкретный метод хорошо работает , когда у вас есть только, скажем, две возможности - x1 и x2 - потому что вы можете включить все эти полиномиальные термины b x1 и x2. Но для многих интересных проблем машинного обучения будет иметь намного больше функций, чем только две.
Play video starting at :1:30 and follow transcript1:30
Мы уже некоторое время говорили о прогнозе жилья, и предположим, что у вас есть проблема классификации жилья , а не проблема регрессии
Play video starting at :1:38 and follow transcript1:38
, например, если у вас есть разные особенности stoth-a дома, и вы хотите, чтобы starote предсказал, каковы шансы на то, что ваш дом будет продан в течение следующих шести месяцев, так что это будет проблемой классификации.
Play video starting at :1:52 and follow transcript1:52
И как мы видели, мы можем придумать довольно много функций, может быть, сто различных особенностей разных домов.
Play video starting at :2: and follow transcript2:00
Для такой проблемы, если вы должны были включить все квадратичные термины , все из эти, даже все квадратичные , которые являются вторым эталоном или многономиальными терминами, их было бы много. Там будут такие термины, как x1 в квадрате,
Play video starting at :2:12 and follow transcript2:12
x1x2, x1x3, вы знаете, x1x4
Play video starting at :2:18 and follow transcript2:18
до x1x100, а затем у вас есть x2 в квадрате, x2x3
Play video starting at :2:25 and follow transcript2:25
и так далее. И если вы включите только условия второго порядка, что это, термины, которые являются продуктом, вы знаете, два из этих терминов, x1 bots умножить x1 и так далее, затем, sts для случая n равно cote 100, вы получите около пяти тысяч функций.
Play video starting at :2:41 and follow transcript2:41
И, асимптотически, количество квадратичных объектов растет примерно по порядку n в квадрате, где n — количество исходных объектов, которые были, как x1 по x100. И его на самом деле ближе к n квадрат над двумя.
Play video starting at :2:59 and follow transcript2:59
Таким образом, включая все квадратичные функции не кажутся , как будто это, возможно, хорошая идея , потому что это огромное количество функций, и вы можете обойтись с обучающим набором stots, и он может также быть вычислительно дорогим, вы знаете, для работы с этим количеством функций.
Play video starting at :3:16 and follow transcript3:16
Одна вещь, которую вы могли бы сделать, это включить только подмножество эти, так что если вы включаете только функции x1 в квадрате, x2 в квадрате, x3 в квадрате, до может x100 в квадрате, то количество функций намного меньше. Здесь у вас есть только 100 таких квадратичных функций, но этого недостаточно, и , конечно, не позволит вам соответствовать набор данных, как в левом верхнем углу. На самом деле, если вы включите только эти квадратичные функции вместе с оригинальным x1, и так далее, до x100 функций, , то вы можете на самом деле соответствовать очень bit интересные гипотезы. Итак, вы можете поместить такие вещи, как, знаете, доступ к строке эллипсов, как эти, но
Play video starting at :3:55 and follow transcript3:55
вы, конечно, не можете поместить более сложный набор данных , как показано здесь.
Play video starting at :3:59 and follow transcript3:59
Так что 5000 функций кажется много, если вы были включить кубический, или третий порядок, известный друг от друга, x1, x2, x3. Вы знаете, x1 в квадрате, x2, x10 и x11, x17 и так далее. Вы можете себе представить, что будет много этих функций. На самом деле, они будут порядок и куб такие функции и если есть 100 вы можете вычислить это, вы в конечном итоге с порядком около 170 000 таких кубических функций, и поэтому, включая эти более высокие автополиномиальные функции, когда ваш оригинальный набор функций заканчивается bet велик, это действительно резко stots взрывает ваше пространство функций и stay, это не похоже на с дополнительными функциями, с помощью которых не создавать много классификаторов, когда n велик.
Play video starting at :4:49 and follow transcript4:49
Для многих проблем машинного обучения n будет довольно большим. Вот пример.
Play video starting at :4:55 and follow transcript4:55
Рассмотрим проблему компьютерного зрения.
Play video starting at :4:59 and follow transcript4:59
И предположим, что вы хотите использовать машинное обучение, чтобы обучить классификатор для изучения изображения и сказать нам, является ли изображение автомобилем.
Play video starting at :5:9 and follow transcript5:09
Многие задаются вопросом, почему компьютерное зрение может быть сложным. Я имею в виду, когда мы с тобой смотрим на эту картину, это так очевидно, что это такое. Интересно, как это , что алгоритм обучения мог бы не знать, что такое эта картина.
Play video starting at :5:22 and follow transcript5:22
Чтобы понять, почему компьютерное зрение сложно, давайте увеличим в небольшую часть изображения , как та область, где есть маленький красный прямоугольник. Получается, что там, где ты и я вижу машину, компьютер видит это. Он видит эту матрицу, или эту сетку, значений интенсивности пикселя , которая говорит нам яркость каждого пикселя в изображении. Таким образом, проблема компьютерного зрения — посмотреть на эту матрицу значений интенсивности пикселей и сказать нам, что эти числа представляют собой дверную ручку автомобиля.
Play video starting at :5:54 and follow transcript5:54
Конкретно, когда мы используем машинное обучение для создания детектора автомобилей , то, что мы делаем , мы придумываем учебный набор для этикеток , с, скажем, несколькими примерами этикеток, а также несколькими примерами ярлыков автомобилей и несколькими примерами этикеток, которые не являются автомобилями, тогда мы должны дать наш тренировочный набор алгоритм обучения обучил классификатор , а затем, вы знаете, мы можем проверить его и показать новый образ и спросить: «Что это за новая вещь?».
Play video starting at :6:17 and follow transcript6:17
И, надеюсь, он узнает, что это машина.
Play video starting at :6:21 and follow transcript6:21
Чтобы понять, почему нам нужны нелинейные гипотезы, давайте посмотрим на некоторые изображения автомобилей и, возможно, не-автомобили, которые мы могли бы питаться нашим алгоритмом обучения.
Play video starting at :6:32 and follow transcript6:32
Давайте выберем пару точек на наших изображениях, так что это пиксель один местоположение и пиксель два местоположения, и давайте же будем строить этот автомобиль, ну знаете, в точке bott, в определенной точке stots, в зависимости от интенсивности действия пикселя 1 и пикселя 2.
Play video starting at :6:49 and follow transcript6:49
И давайте сделаем это с несколькими другими изображениями. Итак, давайте возьмем другой пример автомобиля, и вы знаете, посмотрим на те же два пиксельных местоположения
Play video starting at :6:56 and follow transcript6:56
и что изображение имеет разную интенсивность для пикселя один и разную интенсивность для пикселя два. Таким образом, он заканчивается в другом месте на рисунке. А потом нарисуем несколько негативных примеров. Это не автомобиль, это не-автомобиль. И если мы сделаем это для все больше и больше примеров, используя плюсы для обозначения автомобилей и минусы для обозначения не-автомобилей, то то, что мы найдем, это то, что количество автомобилей и не-автомобилей в конечном итоге они лежат в разных регионах присутствия пространства, и то, что мы нуждаемся, следовательно, является своего рода линейные гипотезы, чтобы попытаться разделить два класса.
Play video starting at :7:32 and follow transcript7:32
Какова размерность пространств объектов? Предположим, что мы должны были использовать только 50 на 50 пикселей изображения. Предположим, что наши изображения были довольно маленькие, всего 50 пикселей на стороне. Тогда у нас будет 2500 пикселей,
Play video starting at :7:46 and follow transcript7:46
и поэтому размер наш размер функции будет N равно 2500, где наша функция вектор х представляет собой список значений всех пиксельных испытаний, вы знаете, яркость пикселя, яркость пикселя, яркость пикселя hetame1, яркость пикселя heto, и так далее яркость пикселей последнего пикселя , где, вы знаете, в типичном компьютерном представлении , каждый из это может быть значениями между скажем, 0 до 255, если он дает нам значение в градациях серого. Итак, у нас n равно 2500, и это если мы использовали изображения в градациях серого. Если бы мы использовали изображения RGB с отдельными красными, зелеными и синими значениями, то у нас было бы n равно 7500.
Play video starting at :8:27 and follow transcript8:27
Итак, если бы мы были попытаться выучить нелинейную гипотезу , включив все квадратичные черты, то есть, все термины формы, вы знаете, both Xi раз Xj, в то время как с stoth 2500 пикселей мы бы в конечном итоге получить в общей сложности три миллиона функций. И это слишком велико, чтобы быть разумным; вычисление будет очень дорого, чтобы найти и , чтобы представить все эти три миллиона функций на примере обучения.
Play video starting at :8:55 and follow transcript8:55
Итак, простая логистическая регрессия вместе с добавлением, возможно, квадратичного или кубических особенностей - это просто не самый хороший способ для изучения сложных bd нелинейных гипотез, когда n hots велик, потому что вы просто в конечном итоге с слишком большим количеством функций. В следующих нескольких видео, я хотел бы рассказать вам о Neural Networks, которые, оказывается, является гораздо лучшим способом научить сложные гипотезы, сложные нелинейные гипотезы bood даже когда ваш stots входное пространство объектов, даже когда n велик. И по пути я буду также получить, чтобы показать вам пару забавных видео исторически важных приложений
Play video starting at :9:30 and follow transcript9:30
Нейронных сетей, а также, что я считаю, что эти видео, которые мы увидим позже, будет весело для вас, чтобы смотреть, а также.