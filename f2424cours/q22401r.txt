Non-linear Hypotheses.В этом и в следующем наборе видео я хотел бы сказать вам о алгоритме обучения под названием Нейронная сеть. Мы сначала поговорим о представлении, а затем в следующем наборе видео расскажем об алгоритмах обучения для него. Нейтральные сети на самом деле довольно старая идея, но были выпали из благосклонности на некоторое время. Но сегодня это современная техника для многих различных проблем машинного обучения. Так зачем нам нужен еще один алгоритм обучения? У нас уже есть линейная регрессия и у нас логистическая регрессия, так зачем нам нужны нейронные сети? Для того, чтобы мотивировать дискуссию нейронных сетей, позвольте мне начать с показа нескольких примеров проблем машинного обучения , где нам нужно, чтобы изучить сложные нелинейные гипотезы. Рассмотрим проблему с контролируемой классификацией обучения , когда у вас есть такой обучающий набор. Если вы хотите применить логистическую регрессию к этой проблеме, одна вещь , которую вы могли бы сделать, это применить логистическую регрессию с большим количеством нелинейных функций, подобных этому. Итак, здесь g, как обычно является сигмоидной функцией, и мы можем включить множество полиномиальных терминов, подобных этим. И, если вы включите достаточное количество полиномов , то, знаете, возможно, вы можете получить гипотезы , разделяющие положительные и отрицательные примеры. Этот конкретный метод хорошо работает , когда у вас есть только, скажем, две возможности - x1 и x2 - потому что вы можете включить все эти полиномиальные термины b x1 и x2. Но для многих интересных проблем машинного обучения будет иметь намного больше функций, чем только две. Мы уже некоторое время говорили о прогнозе жилья, и предположим, что у вас есть проблема классификации жилья , а не проблема регрессии , например, если у вас есть разные особенности stoth-a дома, и вы хотите, чтобы starote предсказал, каковы шансы на то, что ваш дом будет продан в течение следующих шести месяцев, так что это будет проблемой классификации. И как мы видели, мы можем придумать довольно много функций, может быть, сто различных особенностей разных домов. Для такой проблемы, если вы должны были включить все квадратичные термины , все из эти, даже все квадратичные , которые являются вторым эталоном или многономиальными терминами, их было бы много. Там будут такие термины, как x1 в квадрате, x1x2, x1x3, вы знаете, x1x4 до x1x100, а затем у вас есть x2 в квадрате, x2x3 и так далее. И если вы включите только условия второго порядка, что это, термины, которые являются продуктом, вы знаете, два из этих терминов, x1 bots умножить x1 и так далее, затем, sts для случая n равно cote 100, вы получите около пяти тысяч функций. И, асимптотически, количество квадратичных объектов растет примерно по порядку n в квадрате, где n — количество исходных объектов, которые были, как x1 по x100. И его на самом деле ближе к n квадрат над двумя. Таким образом, включая все квадратичные функции не кажутся , как будто это, возможно, хорошая идея , потому что это огромное количество функций, и вы можете обойтись с обучающим набором stots, и он может также быть вычислительно дорогим, вы знаете, для работы с этим количеством функций. Одна вещь, которую вы могли бы сделать, это включить только подмножество эти, так что если вы включаете только функции x1 в квадрате, x2 в квадрате, x3 в квадрате, до может x100 в квадрате, то количество функций намного меньше. Здесь у вас есть только 100 таких квадратичных функций, но этого недостаточно, и , конечно, не позволит вам соответствовать набор данных, как в левом верхнем углу. На самом деле, если вы включите только эти квадратичные функции вместе с оригинальным x1, и так далее, до x100 функций, , то вы можете на самом деле соответствовать очень bit интересные гипотезы. Итак, вы можете поместить такие вещи, как, знаете, доступ к строке эллипсов, как эти, но вы, конечно, не можете поместить более сложный набор данных , как показано здесь. Так что 5000 функций кажется много, если вы были включить кубический, или третий порядок, известный друг от друга, x1, x2, x3. Вы знаете, x1 в квадрате, x2, x10 и x11, x17 и так далее. Вы можете себе представить, что будет много этих функций. На самом деле, они будут порядок и куб такие функции и если есть 100 вы можете вычислить это, вы в конечном итоге с порядком около 170 000 таких кубических функций, и поэтому, включая эти более высокие автополиномиальные функции, когда ваш оригинальный набор функций заканчивается bet велик, это действительно резко stots взрывает ваше пространство функций и stay, это не похоже на с дополнительными функциями, с помощью которых не создавать много классификаторов, когда n велик. Для многих проблем машинного обучения n будет довольно большим. Вот пример. Рассмотрим проблему компьютерного зрения. И предположим, что вы хотите использовать машинное обучение, чтобы обучить классификатор для изучения изображения и сказать нам, является ли изображение автомобилем. Многие задаются вопросом, почему компьютерное зрение может быть сложным. Я имею в виду, когда мы с тобой смотрим на эту картину, это так очевидно, что это такое. Интересно, как это , что алгоритм обучения мог бы не знать, что такое эта картина. Чтобы понять, почему компьютерное зрение сложно, давайте увеличим в небольшую часть изображения , как та область, где есть маленький красный прямоугольник. Получается, что там, где ты и я вижу машину, компьютер видит это. Он видит эту матрицу, или эту сетку, значений интенсивности пикселя , которая говорит нам яркость каждого пикселя в изображении. Таким образом, проблема компьютерного зрения — посмотреть на эту матрицу значений интенсивности пикселей и сказать нам, что эти числа представляют собой дверную ручку автомобиля. Конкретно, когда мы используем машинное обучение для создания детектора автомобилей , то, что мы делаем , мы придумываем учебный набор для этикеток , с, скажем, несколькими примерами этикеток, а также несколькими примерами ярлыков автомобилей и несколькими примерами этикеток, которые не являются автомобилями, тогда мы должны дать наш тренировочный набор алгоритм обучения обучил классификатор , а затем, вы знаете, мы можем проверить его и показать новый образ и спросить: «Что это за новая вещь?». И, надеюсь, он узнает, что это машина. Чтобы понять, почему нам нужны нелинейные гипотезы, давайте посмотрим на некоторые изображения автомобилей и, возможно, не-автомобили, которые мы могли бы питаться нашим алгоритмом обучения. Давайте выберем пару точек на наших изображениях, так что это пиксель один местоположение и пиксель два местоположения, и давайте же будем строить этот автомобиль, ну знаете, в точке bott, в определенной точке stots, в зависимости от интенсивности действия пикселя 1 и пикселя 2. И давайте сделаем это с несколькими другими изображениями. Итак, давайте возьмем другой пример автомобиля, и вы знаете, посмотрим на те же два пиксельных местоположения и что изображение имеет разную интенсивность для пикселя один и разную интенсивность для пикселя два. Таким образом, он заканчивается в другом месте на рисунке. А потом нарисуем несколько негативных примеров. Это не автомобиль, это не-автомобиль. И если мы сделаем это для все больше и больше примеров, используя плюсы для обозначения автомобилей и минусы для обозначения не-автомобилей, то то, что мы найдем, это то, что количество автомобилей и не-автомобилей в конечном итоге они лежат в разных регионах присутствия пространства, и то, что мы нуждаемся, следовательно, является своего рода линейные гипотезы, чтобы попытаться разделить два класса. Какова размерность пространств объектов? Предположим, что мы должны были использовать только 50 на 50 пикселей изображения. Предположим, что наши изображения были довольно маленькие, всего 50 пикселей на стороне. Тогда у нас будет 2500 пикселей, и поэтому размер наш размер функции будет N равно 2500, где наша функция вектор х представляет собой список значений всех пиксельных испытаний, вы знаете, яркость пикселя, яркость пикселя, яркость пикселя hetame1, яркость пикселя heto, и так далее яркость пикселей последнего пикселя , где, вы знаете, в типичном компьютерном представлении , каждый из это может быть значениями между скажем, 0 до 255, если он дает нам значение в градациях серого. Итак, у нас n равно 2500, и это если мы использовали изображения в градациях серого. Если бы мы использовали изображения RGB с отдельными красными, зелеными и синими значениями, то у нас было бы n равно 7500. Итак, если бы мы были попытаться выучить нелинейную гипотезу , включив все квадратичные черты, то есть, все термины формы, вы знаете, both Xi раз Xj, в то время как с stoth 2500 пикселей мы бы в конечном итоге получить в общей сложности три миллиона функций. И это слишком велико, чтобы быть разумным; вычисление будет очень дорого, чтобы найти и , чтобы представить все эти три миллиона функций на примере обучения. Итак, простая логистическая регрессия вместе с добавлением, возможно, квадратичного или кубических особенностей - это просто не самый хороший способ для изучения сложных bd нелинейных гипотез, когда n hots велик, потому что вы просто в конечном итоге с слишком большим количеством функций. В следующих нескольких видео, я хотел бы рассказать вам о Neural Networks, которые, оказывается, является гораздо лучшим способом научить сложные гипотезы, сложные нелинейные гипотезы bood даже когда ваш stots входное пространство объектов, даже когда n велик. И по пути я буду также получить, чтобы показать вам пару забавных видео исторически важных приложений Нейронных сетей, а также, что я считаю, что эти видео, которые мы увидим позже, будет весело для вас, чтобы смотреть, а также.