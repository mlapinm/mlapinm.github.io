Learning With Large Datasets.В следующих нескольких видео мы поговорим о крупномасштабном машинном обучении. То есть алгоритмы, но просмотр с большими наборами данных. Если вы оглянетесь назад на последние 5 или 10 лет истории машинного обучения. Одна из причин того, что алгоритмы обучения работают намного лучше, чем даже сказать, 5 лет назад, - это просто объем данных, которые у нас есть сейчас и на которых мы можем обучать наши алгоритмы. В следующих нескольких видео мы поговорим об алгоритмах работы, когда у нас есть такие массивные наборы данных.
Play video starting at ::32 and follow transcript0:32
Так почему же мы хотим использовать такие большие наборы данных? Мы уже видели, что одним из лучших способов получить высокопроизводительную систему машинного обучения, является использование алгоритма обучения с низким уклоном и обучение его на большом количестве данных. Итак, одним из ранних примеров, которые мы уже видели, был этот пример классификации между запутанными словами. Итак, на завтрак, я съел два (ДВА) яйца, и мы видели в этом примере, такие результаты, где, вы знаете, до тех пор, пока вы питаете алгоритм много данных, это, кажется, очень хорошо. И вот такие результаты привели к тому, что в машинном обучении говорят, что часто это не тот, кто имеет лучший алгоритм, который выигрывает. Это тот, кто имеет больше всего данных. Таким образом, вы хотите учиться на больших наборах данных, по крайней мере, когда мы можем получить такие большие наборы данных. Но обучение с большими наборами данных имеет свои уникальные проблемы, в частности, вычислительные проблемы. Предположим, что ваш тренировочный набор M равен 100,000,000. И это на самом деле довольно реалистично для многих современных наборов данных. Если вы посмотрите на набор данных переписи США, если в США есть, знаете, 300 миллионов человек, вы обычно можете получить сотни миллионов записей. Если посмотреть на объем трафика, который получают популярные сайты, вы легко получаете обучающие наборы, которые намного больше, чем сотни миллионов примеров. И предположим, что вы хотите обучить модель линейной регрессии, или, может быть, логистическую регрессионную модель, в этом случае это правило градиентного спуска. И если вы посмотрите на то, что вам нужно сделать, чтобы вычислить градиент, , который является этим термином здесь, тогда, когда M сто миллионов, вам нужно выполнить суммирование более ста миллионов терминов, для того, чтобы вычислить эти производные термины и выполнить один шаг приличного. Из-за вычислительных затрат на суммирование более ста миллионов записей , чтобы вычислить только один шаг градиентного спуска, в следующих нескольких видео мы говорили о методах либо для замены этого чем-то другим, либо для поиска более эффективных способов вычисления этого производная. К концу этой последовательности видео о крупномасштабном машинном обучении, вы знаете, как подходить модели, линейная регрессия, логистическая регрессия, нейронные сети и так далее даже сегодняшние наборы данных с, скажем, сотнями миллионов примеров. Конечно, прежде чем мы вкладываем усилия в обучение модели со сто миллионами примеров, Мы также должны спросить себя, ну, почему бы не использовать всего тысячу примеров. Может быть, мы можем случайным образом выбрать подмножества из тысячи примеров из ста миллионов примеров и обучить наш алгоритм всего на тысяче примеров. Таким образом, прежде чем инвестировать усилия в фактическое развитие и программное обеспечение, необходимое для обучения этих массивных моделей часто является хорошей проверкой здравомыслия, если обучение всего на тысяче примеров может сделать так же хорошо. Способ проверки здравомыслия использования гораздо меньшего набора обучения может сделать так же хорошо, , то есть если использовать гораздо меньший набор тренировок n равен 1000 размер, , который может сделать так же хорошо, это обычный метод построения кривых обучения, так что если вы должны были построить кривые обучения и если тренировочная цель должна была выглядеть так, это J train theta. И если ваша цель перекрестной проверки, Jcv из theta будет выглядеть так, , то это выглядит как высокодисперсный алгоритм обучения, , и мы будем более уверены, что добавление дополнительных примеров обучения улучшит производительность. В то время как, напротив, если вы должны были построить кривые обучения, , если ваша цель обучения должна была выглядеть так, и если ваша цель перекрестной проверки должна была выглядеть так, , то это выглядит как классический алгоритм обучения с высоким уклоном. И в последнем случае, вы знаете, если вы должны были построить это до, скажем, m равно 1000 и так что m равно 500 до m равно 1000, то кажется маловероятным, что увеличение m до ста миллионов сделает гораздо лучше и тогда вы будете просто хорошо придерживаться n равно 1000, , а не инвестируя много усилий, чтобы выяснить, как масштаб алгоритма. Конечно, если бы вы были в ситуации, показанной фигурой справа, , то одна естественная вещь, чтобы сделать это добавить дополнительные функции, или добавить дополнительные скрытые единицы в вашу нейросеть и так далее, , чтобы вы в конечном итоге с ситуацией ближе к той, что слева, где, возможно, это до n равно 1000, , и это дает вам больше уверенности в том, что попытка добавить инфраструктуру для изменения алгоритма , чтобы использовать гораздо больше тысячи примеров, которые могут быть хорошим использованием вашего времени. Поэтому в крупномасштабном машинном обучении нам нравится придумывать вычислительно разумные способы, или вычислительно эффективные способы, чтобы иметь дело с очень большими наборами данных. В следующих нескольких видео мы увидим две основные идеи. Первый называется стохастическим градиентным спуском, а второй называется Map Уменьшить, для просмотра с очень большими наборами данных. И после того, как вы узнали об этих методах, надеюсь, что это позволит вам масштабировать ваши алгоритмы обучения до больших данных и позволит вам получить гораздо лучшую производительность во многих различных приложениях.