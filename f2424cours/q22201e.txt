Multiple Features.in this video we will start to talk about a new version of linear regression that's more powerful. One that works with multiple variables
Play video starting at ::8 and follow transcript0:08
or with multiple features.
Play video starting at ::10 and follow transcript0:10
Here's what I mean.
Play video starting at ::12 and follow transcript0:12
In the original version of linear regression that we developed, we have a single feature x, the size of the house, and we wanted to use that to predict why the price of the house and this was
Play video starting at ::25 and follow transcript0:25
our form of our hypothesis.
Play video starting at ::28 and follow transcript0:28
But now imagine, what if we had not only the size of the house as a feature or as a variable of which to try to predict the price, but that we also knew the number of bedrooms, the number of house and the age of the home and years. It seems like this would give us a lot more information with which to predict the price.
Play video starting at ::47 and follow transcript0:47
To introduce a little bit of notation, we sort of started to talk about this earlier, I'm going to use the variables X subscript 1 X subscript 2 and so on to denote my, in this case, four features and I'm going to continue to use Y to denote the variable, the output variable price that we're trying to predict.
Play video starting at :1:11 and follow transcript1:11
Let's introduce a little bit more notation.
Play video starting at :1:13 and follow transcript1:13
Now that we have four features
Play video starting at :1:16 and follow transcript1:16
I'm going to use lowercase "n"
Play video starting at :1:19 and follow transcript1:19
to denote the number of features. So in this example we have n4 because we have, you know, one, two, three, four features.
Play video starting at :1:28 and follow transcript1:28
And "n" is different from our earlier notation where we were using "n" to denote the number of examples. So if you have 47 rows "M" is the number of rows on this table or the number of training examples.
Play video starting at :1:45 and follow transcript1:45
So I'm also going to use X superscript "I" to denote the input features of the "I" training example.
Play video starting at :1:55 and follow transcript1:55
As a concrete example let say X2 is going to be a vector of the features for my second training example. And so X2 here is going to be a vector 1416, 3, 2, 40 since those are my four features that I have
Play video starting at :2:17 and follow transcript2:17
to try to predict the price of the second house.
Play video starting at :2:20 and follow transcript2:20
So, in this notation, the
Play video starting at :2:24 and follow transcript2:24
superscript 2 here.
Play video starting at :2:26 and follow transcript2:26
That's an index into my training set. This is not X to the power of 2. Instead, this is, you know, an index that says look at the second row of this table. This refers to my second training example.
Play video starting at :2:39 and follow transcript2:39
With this notation X2 is a four dimensional vector. In fact, more generally, this is an in-dimensional feature back there.
Play video starting at :2:51 and follow transcript2:51
With this notation, X2 is now a vector and so, I'm going to use also Xi subscript J to denote the value of the J,
Play video starting at :3:2 and follow transcript3:02
of feature number J and the training example.
Play video starting at :3:7 and follow transcript3:07
So concretely X2 subscript 3, will refer to feature number three in the x factor which is equal to 2,right? That was a 3 over there, just fix my handwriting. So x2 subscript 3 is going to be equal to 2.
Play video starting at :3:26 and follow transcript3:26
Now that we have multiple features,
Play video starting at :3:29 and follow transcript3:29
let's talk about what the form of our hypothesis should be. Previously this was the form of our hypothesis, where x was our single feature, but now that we have multiple features, we aren't going to use the simple representation any more.
Play video starting at :3:44 and follow transcript3:44
Instead, a form of the hypothesis in linear regression
Play video starting at :3:49 and follow transcript3:49
is going to be this, can be theta 0 plus theta 1 x1 plus theta 2 x2 plus theta 3 x3
Play video starting at :3:58 and follow transcript3:58
plus theta 4 X4. And if we have N features then rather than summing up over our four features, we would have a sum over our N features.
Play video starting at :4:8 and follow transcript4:08
Concretely for a particular
Play video starting at :4:11 and follow transcript4:11
setting of our parameters we may have H of
Play video starting at :4:17 and follow transcript4:17
X 80 + 0.1 X1 + 0.01x2 + 3x3 - 2x4. This would be one
Play video starting at :4:25 and follow transcript4:25
example of a hypothesis and you remember a hypothesis is trying to predict the price of the house in thousands of dollars, just saying that, you know, the base price of a house is maybe 80,000 plus another open 1, so that's an extra, what, hundred dollars per square feet, yeah, plus the price goes up a little bit for each additional floor that the house has. X two is the number of floors, and it goes up further for each additional bedroom the house has, because X three was the number of bedrooms, and the price goes down a little bit with each additional age of the house. With each additional year of the age of the house.
Play video starting at :5:8 and follow transcript5:08
Here's the form of a hypothesis rewritten on the slide. And what I'm gonna do is introduce a little bit of notation to simplify this equation.
Play video starting at :5:17 and follow transcript5:17
For convenience of notation, let me define x subscript 0 to be equals one.
Play video starting at :5:23 and follow transcript5:23
Concretely, this means that for every example i I have a feature vector X superscript I and X superscript I subscript 0 is going to be equal to 1. You can think of this as defining an additional zero feature. So whereas previously I had n features because x1, x2 through xn, I'm now defining an additional sort of zero
Play video starting at :5:47 and follow transcript5:47
feature vector that always takes on the value of one.
Play video starting at :5:52 and follow transcript5:52
So now my feature vector X becomes this N+1 dimensional
Play video starting at :5:58 and follow transcript5:58
vector that is zero index.
Play video starting at :6:2 and follow transcript6:02
So this is now a n+1 dimensional feature vector, but I'm gonna index it from 0 and I'm also going to think of my parameters as a vector. So, our parameters here, right that would be our theta zero, theta one, theta two, and so on all the way up to theta n, we're going to gather them up into a parameter vector written theta 0, theta 1, theta 2, and so on, down to theta n. This is another zero index vector. It's of index signed from zero.
Play video starting at :6:32 and follow transcript6:32
That is another n plus 1 dimensional vector.
Play video starting at :6:37 and follow transcript6:37
So, my hypothesis cannot be written theta 0x0 plus theta 1x1+ up to theta n Xn.
Play video starting at :6:48 and follow transcript6:48
And this equation is the same as this on top because, you know, eight zero is equal to one.
Play video starting at :6:58 and follow transcript6:58
Underneath and I now take this form of the hypothesis and write this as either transpose x, depending on how familiar you are with inner products of vectors if you write what theta transfers x is what theta transfer and this is theta zero, theta one, up to theta N. So this thing here is theta transpose and this is actually a N plus one by one matrix. [It should be a 1 by (n+1) matrix] It's also called a row vector
Play video starting at :7:34 and follow transcript7:34
and you take that and multiply it with the vector X which is X zero, X one, and so on, down to X n.
Play video starting at :7:43 and follow transcript7:43
And so, the inner product that is theta transpose X is just equal to this. This gives us a convenient way to write the form of the hypothesis as just the inner product between our parameter vector theta and our theta vector X. And it is this little bit of notation, this little excerpt of the notation convention that let us write this in this compact form. So that's the form of a hypthesis when we have multiple features. And, just to give this another name, this is also called multivariate linear regression.
Play video starting at :8:15 and follow transcript8:15
And the term multivariable that's just maybe a fancy term for saying we have multiple features, or multivariables with which to try to predict the value Y.