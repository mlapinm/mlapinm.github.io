Implementation Note: Unrolling Parameters.В предыдущем видео мы говорили о том, как использовать обратное распространение для вычисления производных вашей функции затрат. В этом видео я хочу быстро рассказать вам об одной реализационной детали разворачивания ваших параметров из матриц в векторы, которые нам необходимы для использования расширенных процедур оптимизации. Конкретно, скажем вы реализовали функцию затрат , которая принимает этот вход, знаете, параметры тета и возвращает функцию затрат и возвращает производные. Затем вы можете передать это в расширенный алгоритм авторизации fminunc и fminunc , кстати, не единственный. Существуют и другие продвинутые алгоритмы авторизации. Но то, что все они делают, это взять эти входные явно функцию стоимости, и некоторое начальное значение тета. И оба, и эти процедуры предполагают, что тета и начальное значение тета, что это векторы параметров, может быть, Rn или Rn плюс 1. Но это векторы, и он также предполагает, что, вы знаете, ваша функция стоимости вернет как второе возвращаемое значение этого градиента , который также является Rn и Rn плюс 1. Так же вектор. Это сработало нормально, когда мы использовали логистическую прогрессию, но теперь, когда мы используем нейронную сеть , наши параметры являются больше не векторами, но вместо этого они являются этими матрицами, где для sts полной нейронной сети мы будем иметь матрицы параметров theta 1, theta 2, theta 3 , которые мы можем представить в Октаве как эти матрицы тета 1, тета 2, тета 3. И аналогичным образом эти градиентные термины, которые, как ожидалось, вернутся. Ну, в предыдущем видео мы показали, как вычислить эти градиентные матрицы, которые были заглавная D1, заглавная D2, капитал D3, которые мы предлагаем может представлять октаву в виде матриц D1, D2, D3. В этом видео я хочу быстро рассказать вам о идее о том, как взять эти матрицы и развернуть их в векторы. Так что они в конечном итоге находятся в формате, подходящем для , перейдя в тету здесь, чтобы получить для градиента там. Конкретно предположим, что у нас есть нейронная сеть с одним входным слоем с десятью единицами, скрытым слоем с десятью единицами и одним выходным слоем с bit только одной единицей, так что s1 stote - количество единиц в слое 1, а s2 - это число единиц в третьем слое. В этом случае размерность ваших матриц тета тета и D будет , заданным этими выражениями. Например, theta one переходит к матрице 10 на 11 и так далее. Итак, если вы хотите конвертировать между этими матрицами. векторы. То, что вы можете сделать, это взять ваш тета 1, тета 2, тета 3, и написать этот кусок кода, и это будет иметь взять все элементы b ваши три тета матрицы и stote взять все элементы, которые являются тета один, все элементы heta 2, все основные элементы theta 3, huts и развернуть их и поместить все элементы в большой длинный вектор. Который является TheTavec и аналогично вторая команда возьмет все ваши D-матрицы и развернет их в большой вектор и назовет их dVEC. И, наконец, , если вы хотите вернуться от векторных представлений к матричным представлениям. То, что вы делаете, чтобы вернуться к тета один сказать, это взять TheTavec и вытащить из первых 110 элементов. Таким образом, theta 1 имеет 110 элементов, потому что это матрица 10 на 11, так что вытягивает первые 110 элементов , а затем вы можете использовать команду reshape для изменения формы их обратно в theta 1. И аналогичным образом, чтобы получить назад theta 2, вы вытаскиваете из следующих 110 элементов и измените его. И для theta 3, вы вытаскиваете последние одиннадцать элементов и запускаете изменить форму, чтобы вернуть theta 3. Вот быстрая демонстрация Octave этого процесса. Итак, для этого примера давайте установим theta 1 равным равным единице 10 по 11, так что это матрица всех. И просто для того, чтобы это было легче увидеть, давайте установим, что будет 2 раз, 10 по 11 и давайте же установить theta 3 равно 3 b раз 1 из 1 на 11. Итак, это 3 отдельные матрицы: тета 1, тета 2, тета 3. Мы хотим поместить все это как вектор. TheTavec равно тета 1; тета 2 тета 3. Правильно, это двоеточие посередине и как так и теперь thetavec является будет очень длинным вектором. Это 231 элемент. Если я покажу его, я найду , что это очень длинный вектор с всеми элементами первой матрицы , всеми элементами второй матрицы, затем всеми элементами третьей матрицы. И если я хочу вернуть мои исходные матрицы, я могу изменить форму TheTavec. Давайте вытащим первые 110 элементов и изменим их в матрицу 10 на 11. Это возвращает мне theta 1. И если я тогда вытащу следующие 110 элементов. Так это индексы от 111 до 220. Я возвращаю все мои 2. И если я пойду от 221 до последний элемент, который является элементом 231, и изменим форму в 1 на 11, я вернусь theta 3. Чтобы сделать этот процесс действительно конкретным, вот как мы используем идею разворачивания для реализации нашего алгоритма обучения. Допустим, что у вас есть какое-то начальное значение параметров тета 1, тета 2, тета 3. То, что мы собираемся сделать , это взять их и развернуть их в длинный вектор мы будем называть начальную тету для пройти в fminunc в качестве этой начальной настройки параметров тета. Другое, что нам нужно сделать, это реализовать функцию затрат. Вот моя реализация функции затрат. Функция стоимости собирается дать нам ввод, TheTavec, который будет все моих векторов параметров, что в форма, которая была развернута в вектор. Итак, первое, что я собираюсь сделать , это я собираюсь использовать TheTavec, и я собираюсь использовать функции изменения формы. Поэтому я вытащу элементы из TheTavec и использую reshape , чтобы вернуть мои матрицы исходных параметров , theta 1, theta 2, theta 3. Так что это будут матрицы, которые я собираюсь получить. Таким образом, это дает мне более удобную форму, в которой использовать эти матрицы, чтобы я мог запускать вперед распространение и обратное распространение для вычисления моих деривативов и вычислять мою функцию стоимости j theta. И, наконец, я могу затем взять мои производные и развернуть их, чтобы сохранить элементы в том же порядке, что и я, когда я разворачиваю свои теты. Но я собираюсь развернуть D1, D2, D3, чтобы получить GradientVEC , который теперь может вернуть моя функция затрат. Он может возвращать вектор этих производных. Итак, надеюсь, у вас теперь есть хорошее представление о том, как конвертировать туда и обратно между матричным представлением параметров по сравнению с векторным представлением параметров. Преимущество представления матрицы заключается в том, что когда ваши параметры хранятся в виде матриц , это более удобно, когда вы делаете вперед распространение и back распространение, и это легче, когда ваши параметры хранятся в виде матриц hote, чтобы воспользоваться преимуществами , своего рода, векторизованных реализаций. В то время как преимущество векторного представления, когда вы имеете как TheTavec или DveC, это то, что , когда вы используете передовые алгоритмы оптимизации. Эти алгоритмы, как правило, предполагают, что у вас есть все ваши параметры, развернутые в большой длинный вектор. И поэтому с тем, что мы просто прошли, надеюсь, вы теперь можете быстро конвертировать между ними по мере необходимости.