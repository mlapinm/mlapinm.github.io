Examples and Intuitions II.
В этом видео я бы хотел продолжить работу с нашим примером, чтобы показать, как нейронная сеть может вычислить сложную нелинейную гипотезу.
В последнем видео мы увидели, как можно использовать нейронную сеть для вычисления функции x1 AND x2, и функции x1 OR x2 когда x1 и x2 двоичные: это когда они принимают значения 0 и 1 Мы также можем иметь сеть для вычисления отрицания, то есть вычислить функцию NOT х1.
Позвольте мне просто записать способы, связанные с этой сетью.
В этом случае у нас есть только одно входное значение x1 и смещение +1.
И если я сопоставлю им веса +10 и -20, тогда моя гипотеза вычисляет это h (x) = (10-20 x1).
Поэтому, когда x1 = 0, моя гипотеза будет вычислено g(10 - 20 * 0) это просто 10.
И это фактически 1, и когда х равен 1, это будет g (-10), что фактически равно 0.
И если вы посмотрите на эти значения, то это NOT x1 функция.
Элементы включают отрицание, основная идея вставить большой отрицательный вес перед переменной которую вы хотите отрицать.
Минус 20 умножить на х1 и это общая идея о том, как вы, в итоге, отрицаете x1.
И так в примере, который я надеюсь, что вы можете понять сами.
Если вы хотите вычислить функцию (NOT х1) AND (NOT х2), в её частях вы будете помещать большие отрицательные веса перед x1 и x2, то это должно получиться.
Таким образом, вы получаете нейронную сеть с одной выходной единицей, чтобы вычислить это.
Хорошо, итак эта логическая функция (NOT х1) AND (NOT х2), будет равна 1 тогда и только тогда, если x1 = x2 = 0 Хорошо, так как это логическая функция, то NOT x1 означает, что x1 должен быть 0 и NOT x2 означает, что x2 должен быть равен 0 тоже.
Таким образом, эта логическая функция равна 1, тогда и только тогда если оба x1 и x2 равны 0 и надеюсь, вы сможете понять, как создать небольшую нейронную сеть, чтобы вычислить и эту логическую функцию.
Теперь возьмем три части, которые у нас есть и соберем вместе: сеть для вычисления x1 AND x2, и сеть для вычисления (NOT x1) AND (NOT x2).
И последнюю сеть, для вычисления x1 OR x2, мы должны сложить вместе эти три части, чтобы получить вычисление функции x1 XNOR x2.
И просто чтобы напомнить вам, если это х1, х2, эта функция, которую мы хотим вычислить, будет иметь отрицательные примеры здесь и здесь, и иметь положительные примеры тут и тут.
И так ясно, что это потребует нелинейной границы для решения чтобы отделить положительные и отрицательные примеры.
Давайте нарисуем сеть.
Я собираюсь взять свои данные +1, x1, x2 и создать здесь свою первую скрытую ячейку.
Я собираюсь назвать это a(2)1, потому что это мой первая скрытая ячейка.
И я собираюсь скопировать веса из красной сети, x1 и x2.
Это -30, 20, 20.
Далее позвольте мне создать вторую скрытую ячейку, которую я собираюсь назвать а(2)2.
Это вторая скрытая ячейка второго слоя.
Я собираюсь скопировать сеть "морской волны", это в середине Я буду иметь веса 10 -20 -20.
И так, давайте возьмем некоторые значения таблицы "истинности".
Для красной сети, мы знаем, что вычисление x1 и x2, это будет примерно 0 0 0 1, в зависимости от значений x1 и x2, а для a(2)2 - сеть "морской волны".
Что мы знает?
Функция (NOT x1) AND (NOT x2), которая возвращает 1 0 0 0, для четырех значений x1 и x2.
Наконец, я собираюсь создать свою ячейку на выходе, мою выходную единицу, которая является a(3)1.
Это еще один вывод h(x), и я собираюсь скопировать для этого старую сеть.
И мне здесь понадобится единица смещения +1, так что рисуем и Я собираюсь скопировать веса из зеленой сети.
Так что это -10, 20, 20, и мы знаем заранее, что это вычисляет функцию OR.
Итак, давайте заполним записи в таблице "истинности".
Первая запись 0 OR 1, что даёт нам 1, следующая 0 OR 0 это 0, 0 OR 0 снова 0, 1 OR 0 даёт 1 И, таким образом, h (x) равно 1, когда либо x1, либо x2 равны нулю, либо когда x1 и x2 оба 1 и конкретно h (x) выводит 1 именно в этих двух местах, а затем выводит 0 в других случаях.
И, таким образом, эта нейронная сеть, которая имеет входной слой, один скрытый слой и один выходной слой, мы получаем нелинейную границу решения, которая вычисляет эту функцию XNOR.
И наиболее важно, что во входном слое, у нас просто есть четыре входа.
Потом у нас идет скрытый слой, который вычислил несколько более сложные функции от входных данных, которые он показал здесь, это немного более сложные функции.
А потом, добавив еще в другом слое, мы получаем еще более сложную нелинейную функцию.
И это интуитивно понятно, как нейронные сети могут вычислять довольно сложные функции.
Когда у вас есть несколько слоев, у вас есть относительно простая функция на входе второго слоя.
На третьем слое, который я могу построить, чтобы вычислять более сложные функции, и дальше ещё слой который может вычислять еще более сложные функции.
Чтобы завершить это видео, я хочу показать вам забавный пример применения нейронной сети, который отражает эту способность более глубоких слоев, вычислять более сложные функции.
Я хочу показать вам видео заказчика, моего хорошего друга Янн ЛеКун.
Янн - профессор Нью-Йоркского университета и он был одним из первых исследователей нейронных сетей и это своего рода легенда в этой области, и его идеи используются в все виды продуктов и приложений по всему миру сейчас.
Я хочу показать вам видео с некоторых его ранних работ, в которых он использовал нейронную сеть для распознавания почерка, для распознавания рукописных цифр.
Вы можете помните в начале этого урока я сказал, что один из самых ранних успехов нейронных сетей попытка использовать его для чтения почтовых кодов чтобы помочь почтовым службам считывать коды.
Так что это одна из попыток, это один из алгоритмов, используемых для решения этой проблемы.
В видео, которое я покажу вам, эта область является областью ввода, в которой символ отображаемый на холсте, показывается сети.
Этот столбец показывает визуализацию свойств, вычисленных в первом скрытом слое сети.
- - Обнаружены разные ребра, линии и т.
д.
Это визуализация следующего скрытого слоя.
Это труднее увидеть, труднее понять глубокие, скрытые слои и это визуализация того, что следующий скрытый слой вычисляет.
Вы, вероятно, испытываете трудности с пониманием того, что происходит гораздо дальше первого скрытого слоя, но в итоге, все эти изученные свойства передаются на верхний уровень.
И здесь показан окончательный ответ, это окончательное прогнозирующее значение того какую рукописную цифру нейронная сеть думает, что ей показывают.
Итак, давайте посмотрим на видео.
МУЗЫКА Надеюсь, вам понравилось видео, и это, надеюсь, дало вам некоторое понимание что источник с довольно сложными функциями, которым нейронные сети могут обучаться.
Тут просто берется на входе изображение, набор пикселей и первый скрытый слой вычисляет некоторый набор функций.
Следующий скрытый слой вычисляет еще более сложные свойства и еще более сложные свойства.
И эти свойства затем могут быть использованы в последнем слое логистических классификаторов для точных прогнозов без чисел, которые видит сеть.