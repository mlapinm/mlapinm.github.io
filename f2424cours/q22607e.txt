Deciding What to Do Next Revisited.We've talked about how to evaluate learning algorithms, talked about model selection, talked a lot about bias and variance. So how does this help us figure out what are potentially fruitful, potentially not fruitful things to try to do to improve the performance of a learning algorithm.
Play video starting at ::15 and follow transcript0:15
Let's go back to our original motivating example and go for the result.
Play video starting at ::21 and follow transcript0:21
So here is our earlier example of maybe having fit regularized linear regression and finding that it doesn't work as well as we're hoping. We said that we had this menu of options. So is there some way to figure out which of these might be fruitful options? The first thing all of this was getting more training examples. What this is good for, is this helps to fix high variance.
Play video starting at ::45 and follow transcript0:45
And concretely, if you instead have a high bias problem and don't have any variance problem, then we saw in the previous video that getting more training examples,
Play video starting at ::54 and follow transcript0:54
while maybe just isn't going to help much at all. So the first option is useful only if you, say, plot the learning curves and figure out that you have at least a bit of a variance, meaning that the cross-validation error is, you know, quite a bit bigger than your training set error. How about trying a smaller set of features? Well, trying a smaller set of features, that's again something that fixes high variance.
Play video starting at :1:17 and follow transcript1:17
And in other words, if you figure out, by looking at learning curves or something else that you used, that have a high bias problem; then for goodness sakes, don't waste your time trying to carefully select out a smaller set of features to use. Because if you have a high bias problem, using fewer features is not going to help. Whereas in contrast, if you look at the learning curves or something else you figure out that you have a high variance problem, then, indeed trying to select out a smaller set of features, that might indeed be a very good use of your time. How about trying to get additional features, adding features, usually, not always, but usually we think of this as a solution
Play video starting at :1:54 and follow transcript1:54
for fixing high bias problems. So if you are adding extra features it's usually because
Play video starting at :2:1 and follow transcript2:01
your current hypothesis is too simple, and so we want to try to get additional features to make our hypothesis better able to fit the training set. And similarly, adding polynomial features; this is another way of adding features and so there is another way to try to fix the high bias problem.
Play video starting at :2:21 and follow transcript2:21
And, if concretely if your learning curves show you that you still have a high variance problem, then, you know, again this is maybe a less good use of your time.
Play video starting at :2:30 and follow transcript2:30
And finally, decreasing and increasing lambda. This are quick and easy to try, I guess these are less likely to be a waste of, you know, many months of your life. But decreasing lambda, you already know fixes high bias.
Play video starting at :2:45 and follow transcript2:45
In case this isn't clear to you, you know, I do encourage you to pause the video and think through this that convince yourself that decreasing lambda helps fix high bias, whereas increasing lambda fixes high variance.
Play video starting at :2:59 and follow transcript2:59
And if you aren't sure why this is the case, do pause the video and make sure you can convince yourself that this is the case. Or take a look at the curves that we were plotting at the end of the previous video and try to make sure you understand why these are the case.
Play video starting at :3:15 and follow transcript3:15
Finally, let us take everything we have learned and relate it back to neural networks and so, here is some practical advice for how I usually choose the architecture or the connectivity pattern of the neural networks I use.
Play video starting at :3:30 and follow transcript3:30
So, if you are fitting a neural network, one option would be to fit, say, a pretty small neural network with you know, relatively few hidden units, maybe just one hidden unit. If you're fitting a neural network, one option would be to fit a relatively small neural network with, say,
Play video starting at :3:48 and follow transcript3:48
relatively few, maybe only one hidden layer and maybe only a relatively few number of hidden units. So, a network like this might have relatively few parameters and be more prone to underfitting.
Play video starting at :4: and follow transcript4:00
The main advantage of these small neural networks is that the computation will be cheaper.
Play video starting at :4:5 and follow transcript4:05
An alternative would be to fit a, maybe relatively large neural network with either more hidden units--there's a lot of hidden in one there--or with more hidden layers.
Play video starting at :4:16 and follow transcript4:16
And so these neural networks tend to have more parameters and therefore be more prone to overfitting.
Play video starting at :4:22 and follow transcript4:22
One disadvantage, often not a major one but something to think about, is that if you have a large number of neurons in your network, then it can be more computationally expensive.
Play video starting at :4:33 and follow transcript4:33
Although within reason, this is often hopefully not a huge problem.
Play video starting at :4:36 and follow transcript4:36
The main potential problem of these much larger neural networks is that it could be more prone to overfitting and it turns out if you're applying neural network very often using a large neural network often it's actually the larger, the better
Play video starting at :4:50 and follow transcript4:50
but if it's overfitting, you can then use regularization to address overfitting, usually using a larger neural network by using regularization to address is overfitting that's often more effective than using a smaller neural network. And the main possible disadvantage is that it can be more computationally expensive.
Play video starting at :5:10 and follow transcript5:10
And finally, one of the other decisions is, say, the number of hidden layers you want to have, right? So, do you want one hidden layer or do you want three hidden layers, as we've shown here, or do you want two hidden layers?
Play video starting at :5:23 and follow transcript5:23
And usually, as I think I said in the previous video, using a single hidden layer is a reasonable default, but if you want to choose the number of hidden layers, one other thing you can try is find yourself a training cross-validation, and test set split and try training neural networks with one hidden layer or two hidden layers or three hidden layers and see which of those neural networks performs best on the cross-validation sets. You take your three neural networks with one, two and three hidden layers, and compute the cross validation error at Jcv and all of them and use that to select which of these is you think the best neural network.
Play video starting at :6:2 and follow transcript6:02
So, that's it for bias and variance and ways like learning curves, who tried to diagnose these problems. As far as what you think is implied, for one might be truthful or not truthful things to try to improve the performance of a learning algorithm.
Play video starting at :6:16 and follow transcript6:16
If you understood the contents of the last few videos and if you apply them you actually be much more effective already and getting learning algorithms to work on problems and even a large fraction, maybe the majority of practitioners of machine learning here in Silicon Valley today doing these things as their full-time jobs.
Play video starting at :6:35 and follow transcript6:35
So I hope that these pieces of advice on by experience in diagnostics
Play video starting at :6:42 and follow transcript6:42
will help you to much effectively and powerfully apply learning and get them to work very well.