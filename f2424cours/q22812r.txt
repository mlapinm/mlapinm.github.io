Principal Component Analysis Problem Formulation.Для проблемы уменьшения размерности, безусловно, самым популярным, на сегодняшний день наиболее часто используемым алгоритмом является то, что называется анализом компонентов , или PCA. В этом видео я хотел бы начать говорить о постановке проблемы для PCA. Другими словами, давайте попробуем сформулировать, точно, то, что мы хотели бы сделать PCA. Допустим, у нас есть такой набор данных. Итак, это набор данных примеров x и R2, и скажем, я хочу уменьшить размерность данных от двумерных до одномерных. Другими словами, я хотел бы найти строку, на которую можно проецировать данные. То, что кажется хорошей строкой, на которую можно проецировать данные, это строка, как эта, может быть довольно хорошим выбором. И причина, по которой мы считаем, что это может быть хорошим выбором, заключается в том, что если вы посмотрите на то, где проектируемые версии точечных шкал, так что я беру эту точку и проецировать ее здесь. Пойми, эта точка проецируется здесь, сюда, сюда, сюда. Мы находим, что расстояние между каждой точкой и проектируемой версией довольно мало. То есть, эти сегменты синей линии довольно короткие. То, что PCA делает формально, так это то, что он пытается найти нижнюю размерную поверхность, действительно линию в данном случае, на которую проецировать данные так , что сумма квадратов этих маленьких сегментов синей линии сводится к минимуму. Длина этих сегментов синей линии, , которую иногда называют ошибкой проецирования. И так, что PCA делает, это пытается найти поверхность, на которую проецировать данные так , чтобы минимизировать это. В стороне, прежде чем применять PCA, стандартная практика заключается в том, чтобы сначала выполнить среднюю нормализацию при масштабировании объектов, так что объекты x1 и x2 должны иметь нулевое среднее и иметь сопоставимые диапазоны значений. Я уже сделал это для этого примера, но я вернусь к этому позже и расскажу больше о масштабировании функций и нормализации в контексте PCA позже.
Play video starting at :1:58 and follow transcript1:58
Но возвращаясь к этому примеру, в отличие от красной линии, которую я только что нарисовал, вот другая строка, на которую я мог бы проецировать свои данные, которая является этой пурпурной линией. И, как мы увидим, эта пурпурная линия является гораздо худшим направлением , на которое проецировать мои данные, верно? Поэтому, если бы я проецировал свои данные на пурпурную линию, мы бы получили набор таких точек. И ошибки проецирования, то есть эти сегменты синей линии, будут огромными. Таким образом, эти точки должны перемещаться на огромное расстояние, чтобы проецироваться на пурпурную линию. Вот почему PCA, анализ основных компонентов, выберет что-то вроде красной линии, а не пурпурной линии.
Play video starting at :2:42 and follow transcript2:42
Давайте напишем проблему PCA немного более формально. Цель PCA, если мы хотим уменьшить данные от двумерных до одномерных, мы попытаемся найти вектор, который является вектор u1, который будет Rn, так что это будет R2 в этом случае. Я собираюсь найти направление, на которое проецировать данные, поэтому это минимизировать ошибку проецирования. Итак, в этом примере я надеюсь, что PCA найдет этот вектор, который я хочу вызвать u (1), так что, когда я проецирую данные на строку, которую я определяю, расширяя этот вектор, я заканчиваю довольно маленькими ошибками реконструкции. И эта ссылка данных, которая выглядит так. И, кстати, я должен упомянуть, что когда PCA дает мне u (1) или -u (1), не имеет значения. Так что если это дает мне положительный вектор в этом направлении, это нормально. Если это дает мне противоположный вектор, обращенный в противоположном направлении, так , что будет как минус u (1). Давайте нарисуем это синим, верно? Но это дает положительный u (1) или отрицательный u (1), это не имеет значения, потому что каждый из этих векторов определяет ту же красную линию, на которую я проецирую свои данные.
Play video starting at :3:54 and follow transcript3:54
Таким образом, это случай сокращения данных с двумерных до одномерных. В более общем случае у нас есть n-мерные данные и мы хотим уменьшить их до k-измерений. В этом случае мы хотим найти не только один вектор, на который проецировать данные, но мы хотим найти k-измерения, на которые проецировать данные. Чтобы свести к минимуму эту ошибку проецирования. Вот пример. Если у меня есть 3D облако точек, как это, то, возможно, я хочу найти векторы. Так что найдите пару векторов. И я назову эти векторы. Давайте нарисуем их красным цветом. Я собираюсь найти пару векторов, выдержанных от происхождения. Вот u (1), и вот мой второй вектор, u (2). И вместе эти два вектора определяют плоскость, или они определяют 2D поверхность, верно? Как это с 2D поверхностью, на которую я собираюсь проецировать свои данные. Для тех из вас, кто знаком с линейной алгеброй, для в этом году они действительно эксперты в линейной алгебре, формальное определение этого , что мы собираемся найти набор векторов u (1), u (2), может быть до u (k). И то, что мы собираемся сделать, это проецировать данные на линейное подпространство, охватываемое этим набором k векторов. Но если вы не знакомы с линейной алгеброй, просто подумайте об этом как о нахождении k направлений вместо одного направления, на которое проецировать данные. Таким образом, нахождение k-мерной поверхности действительно находит 2D-плоскость в данном случае, показано на этом рисунке, где мы можем определить положение точек в плоскости с помощью k направлений. И поэтому для PCA мы хотим найти k векторов, на которые проецировать данные. Более формально в PCA мы хотим найти такой способ проецировать данные таким образом, чтобы свести к минимуму расстояние проекции, которое является расстоянием между точками и проекциями. И так в этом 3D-примере тоже. Учитывая точку, мы бы взяли точку и проецировали ее на эту 2D поверхность.
Play video starting at :5:55 and follow transcript5:55
С этим покончено. И поэтому ошибка проекции будет, расстояние между точкой и , где она проецируется вниз на мою 2D поверхность. И что делает PCA, это я пытаюсь найти линию, или плоскость, или что-то еще, , на которую проецировать данные, чтобы попытаться свести к минимуму квадратную проекцию, , что 90 градусов или ортогональная ошибка проекции. Наконец, один вопрос, который мне иногда задают, заключается в том, как PCA относится к линейной регрессии ? Потому что, объясняя PCA, я иногда рисую диаграммы вроде этих и , которые немного похожи на линейную регрессию.
Play video starting at :6:30 and follow transcript6:30
Оказывается, PCA не линейная регрессия, и , несмотря на некоторое косметическое сходство, на самом деле это совершенно разные алгоритмы. Если бы мы делали линейную регрессию, то, что мы сделали бы, слева мы бы пытались предсказать значение какой-то переменной y, учитывая некоторые информационные функции x. И поэтому линейная регрессия, что мы делаем, мы подстраиваем прямую линию таким образом , чтобы минимизировать квадратную погрешность между точками и эту прямую линию. И то, что мы минимизируем, будет квадратная величина этих синих линий. И обратите внимание, что я рисую эти синие линии вертикально. Эти синие линии являются вертикальным расстоянием между точкой и значением, предсказанным гипотезой. В то время как, напротив, в PCA, он пытается минимизировать величину этих синих линий, которые рисуются под углом. Это действительно самые короткие ортогональные расстояния. Самое короткое расстояние между точкой x и этой красной линией.
Play video starting at :7:27 and follow transcript7:27
И это дает очень разные эффекты в зависимости от набора данных. И в более общем плане, когда вы делаете линейную регрессию, есть эта различающаяся переменная y, которую мы пытаемся предсказать. Вся эта линейная регрессия, а также принимая все значения x и пытаются использовать это для предсказания y. В то время как в PCA нет различия, или нет специальной переменной y, которую мы пытаемся предсказать. И вместо этого у нас есть список функций, x1, x2 и так далее, вплоть до xn и , все эти функции обрабатываются одинаково, поэтому ни один из них не является особенным. В качестве одного из последних примеров, если у меня есть трехмерные данные и , я хочу сократить данные с 3D до 2D, поэтому, возможно, я хочу найти два направления, u (1) и u (2), на которые проецировать мои данные. Тогда у меня есть три функции, x1, x2, x3 и , все они рассматриваются одинаково. Все они обрабатываются симметрично, и нет специальной переменной y, которую я пытаюсь предсказать. И поэтому PCA не является линейной регрессией, и , хотя на каком-то косметическом уровне они могут выглядеть связанными, это на самом деле очень разные алгоритмы. Надеюсь, теперь вы понимаете, что делает PCA. Он пытается найти нижнюю размерную поверхность, на которую можно проецировать данные, поэтому , чтобы свести к минимуму эту квадратную ошибку проекции. Чтобы свести к минимуму квадратное расстояние между каждой точкой и местоположением, где она проецируется. В следующем видео мы начнем говорить о том, как на самом деле найти эту нижнюю размерную поверхность, на которую можно проецировать данные.