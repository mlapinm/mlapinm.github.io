Gradient Descent.Ранее мы определили функцию затрат J. В этом видео я хотел бы рассказать вам о методе, называемом градиентным спуском, который применяется для минимизации функции стоимости J. Градиентный спуск оказывается более общим алгоритмом и применяется не только для линейной регрессии.На самом деле, он повсеместно используется и в машинном обучении.Далее в рамках курса мы будем применять метод градиентного спуска для минимизации и других функций, не только функции стоимости J для линейной регрессии. Итак, в этом видео я собираюсь рассказать вам о методе градиентного спуска минимизации некой произвольной функции J. В последующих видео мы возьмем этот алгоритм и применим его к той конкретной функции стоимости J, которую мы сформировали для линейной регрессии.Итак, сформулируем задачу.Пусть имеется некоторая функция J, зависящая от параметров тета нулевое и тета первое. Может, это функция затрат в задаче линейной регрессии.Может быть, это какая-то другая функция, минимум которой мы хотим найти.И мы хотим придумать алгоритм, минимизирующий эту функцию от J по параметрам тета нулевое и тета первое.Собственно говоря, метод градиентного спуска применим к более широкому классу функций.Представьте, что у вас есть функция J от параметров тета нулевое, тета первое, тета второе и так далее, до тета энного.И вы хотите минимизировать ее значение по всем этим параметрам. Алгоритм градиентного спуска решает и эту, более общую задачу, но ради краткости и простоты обозначений в течение этого видео я буду использовать только два параметра.Вот в чем идея градиентного спуска.Сперва мы наугад возьмем некоторые начальные значения для тета нулевого и тета первого.Большой разницы нет, но полагают тета нулевое равным нулю и тета первое тоже равным нулю.Просто зададим им начальное значение 0.В ходе работы градиентного спуска мы будем понемногу менять их значения, пытаясь уменьшить значение J, рассчитывая на то, чтобы в конце концов оказаться в точке минимума или локального минимума.Давайте посмотрим, как работает градиентный спуск, на графике.Допустим, я пытаюсь найти минимум этой функции.Обратите внимание на оси.Тета нулевое и тета первое по горизонтальным осям, J по вертикальной оси.То есть высота поверхности соответствует значению функции J, и мы хотим найти ее минимум.Начнем с некоторой точки, соответствующей паре (тета нулевое, тета первое).Представьте, что вы взяли какие-то значения для тета нулевого и тета первого, и нашли соответствующую им точку поверхности.Так?Допустим, ваша пара (тета нулевое, тета первое) дает вам вот эту точку.Мои начальные значения равны нулю, хотя иногда начина ют и с других значений.Теперь.Представьте себе, что эта поверхность - изображение холмистой местности.Например, полного растительности парка, с двумя холмами... Нечто в этом духе. И я хочу, чтобы вы вообразили, что в самом деле стоите в этой точке на склоне холма в вашем парке.Метод градиентного спуска предполагает, что мы оборачиваемся вокруг себя на 360 градусов, осматриваем окрестности, и задаем себе вопрос: "Если я собираюсь сделать маленький шаг в каком-нибудь направлении, и я хочу спуститься вниз как можно быстрее, в каком направлении мне следует сделать этот мой шаг? Если мне нужно вниз, если я в некотором роде хочу физически спуститься вниз по этому холму как можно быстрее?" Оказывается, что если мы стоим в этой точке на холме, то, осмотревшись вокруг, мы обнаружим, что лучше всего сделать свой шаг примерно в этом направлении.Так?И теперь вы уже в этой точке холма. Вы снова осматриваетесь и спрашиваете себя, в каком направлении сделать маленький шаг, чтобы оказаться еще чуточку ниже. Определившись, вы делаете шаг сюда и продолжаете идти.Вы знаете, что делать на новом месте: осмотреться, выбрать направление, которое позволит вам поскорее спуститься, сделать шаг, затем еще шаг, и так далее, пока вы, наконец, не окажетесь в этом локальном минимуме.Градиентный спуск обладает интересным свойством.В первый раз мы начинали градиентный спуск с вот этой точки, так? Вот с этой начальной точки.Теперь представьте, что мы начали градиентный спуск всего на пару шагов правее. То есть начальной точкой алгоритма стала вот эта, чуть выше и правее.Если повторить процесс: встать здесь, осмотреться,сделать шаг в направлении самого крутого уклона, вот сюда,снова осмотреться, сделать еще шаг, и так далее,окажется, что, если начать всего в двух шагах правее, градиентный спуск приведет нас во второй локальный минимум, справа.То есть, если вы начнете в первой точке, вы окажетесь в этом локальном минимуме,но если вы чуть-чуть измените начальное положение, то окажетесь совсем в другом локальном минимуме.Об этом свойстве алгоритма градиентного спуска мы немного поговорим позже.Итак, по графику мы получили интуитивное представление об алгоритме.Давайте разберемся с математической стороной дела.Это определение алгоритма градиентного спуска.Мы просто будем повторять этот шаг.Пока алгоритм не сойдется в одной точке.Мы будем менять значение параметра тета с индексом j, для j равного 0 и 1, вычитая из него вот это выражение с коэффициентом альфа.Давайте разберемся.В этом выражении есть несколько деталей, которые я хочу прояснить. Во-первых, вот это обозначение: двоеточие и знак равенства. Мы будем использовать «:=» для обозначения присваивания, то есть это оператор присваивания.То есть, если я пишу «a := b», с точки зрения компьютера это означает «взять значение переменной b и заменить на него значение переменной a». Тем самым, значение a станет равно значению b. Это присваивание.Я могу написать и «a := a + 1».Это означает «взять значение переменной a и увеличить его на единицу». Если же я использую только знак равенства и напишу «a = b», это будет утверждение факта.Если я пишу «a = b», это означает, что я утверждаю, что значение a равно значению b. То есть слева у меня здесь — компьютерная операция, устанавливающая значение переменной а, а справа — утверждение: я утверждаю, что значения a и b совпадают.Таким образом, хоть я и могу написать «a := a + 1», что означает «увеличить значение a на 1»,надеюсь, я никогда не напишу «a = a + 1». Потому что это просто неверно. a и a + 1 никогда не могут быть равны.Это была первая часть определения.Числовой коэффициент альфа называется скоростью обучения.Он определяет, насколько большой шаг мы делаем на каждой итерации градиентного спуска.Так, большое значение альфа соответствует решительному спуску: мы будем стараться делать огромные шаги вниз по склону.При очень маленьком значении альфа мы будем спускаться крохотными шажками.Я еще вернусь к обсуждению этого коэффициента и расскажу о том, как выбирать значение альфа. И, наконец, вот этот элемент.Это производная. Сейчас я не буду говорить о ней подробно, но позже я выведу ее и объясню, откуда она взялась.Некоторые из вас знакомы с дифференциальным исчислением лучше других, но даже если вы не знаете, что это, не беспокойтесь, я расскажу об этом выражении все, что вам будет необходимо.Осталась лишь одна тонкость в устройстве алгоритма градиентного спуска. На каждом шаге мы будем изменять тета нулевое и тета первое.Это присваивание относится к тета нулевому и тета первому.Мы изменяем значение обоих параметров.И тонкость состоит в том, что, применяя градиентный спуск, выполняя это присваивание, вы должны обновить значения тета нулевого и тета первого одновременно.Вот что я имею в виду. Это присваивание означает: вычесть из тета нулевого некую величину и вычесть из тета первого некую другую величину. И это должно быть сделано так, чтобы вы сначала вычисляли правую часть,вычисляли эту некую величину и для тета нулевого, и для тета первого, а затем уже обновляли значения тета нулевого и тета первого.Еще раз продемонстрирую, что я имею в виду.Здесь приведено корректное описание работы алгоритма, учитывающее необходимость одновременного изменения.Сначала я присвою это переменной temp0, а это — переменной temp1.То есть, фактически, вычислю правые части.А затем, уже после того, как я вычислил правые части и сохранил их величины в переменных temp0 и temp1, я обновляю и значение тета нулевого, и значение тета первого, потому что это необходимо для правильной работы алгоритма.Сопоставьте это с неверной реализацией алгоритма, в которой параметры обновляются не одновременно.Здесь, в некорректной реализации, мы вычисляем temp0, затем заменяем этим новым значением тета нулевое,Затем обновляем тета_1.И разница между описаниями алгоритма, приведенными слева и справа, в том, что — вглядитесь в этот шаг, — здесь мы уже изменили тета нулевое, и при вычислении этой производной мы будем использовать уже новое значение тета нулевого, то есть получим другую величину для temp1, не такую, как в описании слева. Потому что мы уже заменили старое значение тета нулевого.Таким образом, реализация градиентного спуска, приведенная справа, некорректна.Я не хочу сейчас разбираться, почему одновременное обновление необходимо. Оказывается, что обычно при реализации алгоритма, мы еще поговорим об этом, использовать одновременное обновление — более естественный подход.И при обсуждении градиентного спуска всегда подразумевается одновременное обновление параметров.Если вы будете использовать последовательное обновление, возможно, ваш алгоритм все равно сработает. Но описание, приведенное справа, это просто не то, что называют алгоритмом градиентного спуска, это другой алгоритм с другими свойствами. Который по некоторым причинам может вести себя немного более странно.Так что для градиентного спуска вам следует использовать одновременное обновление.Итак, мы в общих чертах разобрали алгоритм градиентного спуска.В следующем видео мы подробно разберемся с производной, которую я написал, но никак пока не объяснил.Если вы проходили курс дифференциального исчисления и знакомы с частными производными, — это, собственно, она и есть, частная производная.Если вы не знакомы с дифференциальным исчислением, ничего страшного.Следующее видео даст вам интуитивное представление о ней, и все остальное, что необходимо для ее вычисления, даже если вы не проходили производные или не знакомы с частными производными.И таким образом, я надеюсь, со следующим видео у вас появится понимание того, как применять градиентный спуск.