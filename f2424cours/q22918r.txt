Vectorization: Low Rank Matrix Factorization.В последних нескольких видео мы говорили о алгоритме совместной фильтрации. В этом видео я собираюсь сказать немного о реализации векторизации этого алгоритма. А также поговорим немного о других вещах, которые вы можете сделать с этим алгоритмом. Например, одна из вещей, которые вы можете сделать, это, учитывая один продукт вы можете найти другие продукты, которые связаны с этим , так что для примера , пользователь недавно смотрел на один продукт. Есть ли другие сопутствующие продукты , которые вы могли бы рекомендовать этому пользователю? Давайте посмотрим, что мы можем сделать с этим. То, что я хотел бы сделать, это работать альтернативным способом записывать прогнозы алгоритма совместной фильтрации. Для начала, вот наш набор данных с нашими пять фильмов и то, что я собираюсь сделать, это взять все рейтинги всех пользователей bet и сгруппировать их в stots матрицу. Итак, здесь у нас есть пять фильмов и четыре пользователя , и поэтому эта матрица y будет 5 на 4 матрицы. Просто вы знаете, принимая все элементов, все эти данные. Включая вопросительные знаки и группируя их в эту матрицу. И, конечно, элементы этой матрицы элемента (i, j) эта матрица на самом деле то, что мы ранее писали как y надстрочный i, j. Это оценка, присвоенная фильму i stots пользователем j. Учитывая эту матрицу y из всех рейтинговых оценок, которые у нас есть, альтернативный способ записи из всех прогнозных рейтингов алгоритма. И, в частности, если вы посмотрите на то, что определенный пользователь прогнозирует на определенный фильм, то, что пользователь j прогнозирует на фильме i, дается по этой формуле. И так, если у вас есть матрица предсказанных оценок, то, что вы бы имели, это следующая матрица , где i, j запись. Таким образом, это соответствует рейтингу , который мы предсказываем, используя j даст фильму i точно равен тому, что theta j транспонировать XI, и поэтому, вы знаете, это матрица, где этот первый элемент представляет собой один-один элемент представляет собой прогнозируемый рейтинг пользователя один heta или фильм один и этот элемент , это один-два элемент является прогнозируемым рейтингом пользователя два на фильме один, и так далее, и это предсказанный рейтинг пользователя одного sto на последнем фильме и storie, если вы хотите, вы знаете, значение и этот рейтинг то, что мы предсказали бы для этого значения и так далее. Теперь, учитывая эту матрицу прогнозных рейтингов , есть более простой или векторизованный способ их написания. В частности, если я определяю матрицу x, и это будет просто , как матрица, которую мы имели ранее для линейной регрессии, чтобы быть вроде x1 транспонирование x2 b транспонирование вниз к sth x nm транспонирование. Так что я беру все функции для моих фильмов и стек их в ряд. Так что если вы думаете о каждый фильм как один пример и стек все функции разных фильмов и строк. И если мы также, чтобы найти матрицу заглавной тета, и то, что я собираюсь сделать, это взять каждый из для каждого пользовательского параметра vectors и складывать их в строки, вот так. Итак, это theta 1, который является вектором параметров для первого пользователя. И, вы знаете, theta 2, и так, вы должны складывать их в строки, как это, чтобы определить матричный капитал тета тета, и поэтому у меня есть векторы параметров b nu, все уложенные в строки, как это. Теперь, учитывая это определение для матрицы x и это определение для матрицы theta для того, чтобы иметь векторизованный способ вычисления матрицы bet-matrix всех предсказаний, вы можете просто вычислить x раз в матрице theta transpose, и rete, который дает вам векторизованный способ вычисления этой матрицы здесь. Чтобы дать алгоритм совместной фильтрации , который вы использовали другое имя. Алгоритм, который мы используем , также называется факторизацией матрицы низкого ранга И поэтому, если вы слышите, как люди говорят о матрице низкого ранга факторизация, это по сути именно алгоритм, о котором мы говорили. И этот термин происходит от свойства , что эта матрица х раз тета транспонирование имеет hR математическое свойство в линейной алгебре называется, что это bot- матрица низкого ранга и stots, так что это то, что приводит к этому имени низкий рейтинг матрицы факторизации для этих алгоритмов, из-за этого низкого ранг свойство этой матрицы x тета транспонирование. Если вы не знаете, что означает низкий ранг, или если вы не знаете, что такое матрица низкого ранга, не беспокойтесь об этом. Вам действительно не нужно это знать, чтобы использовать этот алгоритм. Но если вы эксперт в линейной алгебре , это то, что дает этот алгоритм, это другое название матричной факторизации низкого ранга. Наконец, запустив алгоритм совместной фильтрации , вот что-то еще, что вы можете сделать , который использует изученные функции для того, чтобы найти связанные фильмы. Специально для каждого продукта i действительно для каждого фильма i, мы узнали векторный объект xi. Итак, вы знаете, когда вы изучаете некоторые функции без действительно знаю , что может заранее, что различные функции будут, но если вы запустить алгоритм и идеально функции будут, как правило, захватить то, что есть stots важные аспекты этих фильмы или различные продукты или что у вас. Каковы важные аспекты, которые вызывают некоторым пользователям нравятся определенные фильмы и вызывают некоторых пользователей для разных наборов фильмов. Так что, может быть, вы закончите изучение функции, вы знаете, где x1 равно романтику, x2 равно действие, похожее на более раннее видео, и, возможно, вы обучили другую функцию x3, который stots степень, до которой это комедия. Тогда какая-то особенность x4, которая, ну знаешь, другая вещь. И у вас есть N функции все вместе, и после вы узнали особенности, это на самом деле часто довольно сложно пойти в к выученным функциям и придумать both с понятной человеком интерпретацией stots того, что эти функции на самом деле есть. Но на практике, вы знаете, функции , хотя эти функции могут быть трудно визуализировать. Может быть трудно понять, что именно эти функции. Как правило, он будет изучать функции, которые очень значимы для захвата того, что является самым важным или наиболее характерным свойствами в фильме, что заставляет вас любить или не любить его. Итак, теперь скажем, мы хотим решить следующую проблему. Скажем, у вас есть какой-то конкретный фильм i, и вы хотите , чтобы найти другие фильмы j , которые связаны с этим фильмом. И так хорошо, зачем тебе это делать? Правильно, может быть, у вас есть пользователь , который просматривает фильмы, и они в настоящее время смотрят фильм j, чем какой разумный фильм, чтобы рекомендовать им смотреть после того, как они закончены с фильмом j? Или если кто-то недавно приобрел фильм j, ну, что такое другой фильм, который было бы разумно рекомендовать им рассмотреть вопрос о покупке. Итак, теперь, когда вы получили эти векторы функций, это дает нам очень удобный способ, чтобы измерить, как похожи два фильма. В частности, фильм i имеет векторный xi. и поэтому, если вы можете найти другой фильм, j, так что , что расстояние между xi и xj небольшое, то это довольно b сильный признак того, что, знаете, фильмы sts j и i как-то похожи. По крайней мере, в том смысле, что некоторым из них нравится фильм i, возможно, более вероятно, понравится фильм j. Итак, просто подытожить, если ваш пользователь смотрит на какой-то фильм i, и если вы хотите найти 5 наиболее похожие фильмы к этому фильму, чтобы рекомендовать им stoth 5 новых фильмов, что вы делаете, это найти пять фильмов о кино j, с самым маленьким расстоянием между между этими различными фильмами. И это может дать вам несколько различных фильмов, чтобы рекомендовать вашему пользователю. Итак, с этим, надеюсь, вы теперь знаете, как использовать векторизованную реализацию для вычисления всех прогнозируемых оценок всех пользователей и всех фильмов, а также как делать stots вещи, такие как использование выученных функций, чтобы найти то, что может быть кино, и что может быть продуктами которые не связаны друг с другом.