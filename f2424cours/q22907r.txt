Multivariate Gaussian Distribution.В этом и следующем видео я хотел бы рассказать вам об одном возможном расширении алгоритма обнаружения аномалий , который мы разработали до сих пор. Это расширение использует что-то, называемое многомерным распределением Гауссова, и у него есть некоторые преимущества, и некоторые недостатки , и он может иногда ловить некоторые аномалии, которых не делал предыдущий алгоритм. Чтобы мотивировать это, давайте начнем с примера. Предположим, что так наши немаркированные данные выглядят как то, что я построил здесь. И я собираюсь использовать пример мониторинга машин в дата-центре, мониторинга компьютеров в дата-центре. Итак, мои две функции - x1 , что является загрузкой процессора и x2 , что, возможно, является использованием памяти. Так что, если я возьму мои две функции, x1 и x2, и я моделирую их как гауссос, то вот сюжет моих функций X1, вот сюжет о том, как мои функции X2, stoth2, и поэтому, если я вписываюсь в stet-Gaussian к этому, может быть, я получу Гауссова, как это, зависит от параметров mu 1, и сигма в квадрате 1, и вот моя память используется, и, вы знаете, может быть, я получу гауссовскую , которая выглядит так, и это мой P из X 2, bet, который зависит от mu 2 и sigma в квадрате 2. И так вот как алгоритм обнаружения аномалий модели X1 и X2. Теперь предположим, что в наборах тестов у меня есть пример , который выглядит так. Расположение этого зеленого креста , поэтому значение X 1 составляет около 0,4, а значение X 2 — около 1,5. Теперь, если вы посмотрите на данные, это выглядит так, да, большая часть данных лежит в этом регионе, и так, что зеленый крест довольно далеко от всех данных, которые я видел. Похоже, что это должно быть поднято как аномалия. Итак, в моих данных , в моих, в данных моих хороших примерах, это выглядит так, как, знаете, загрузка процессора и использование памяти , они сортируют линейно растут друг с другом. Так что, если у меня есть машина , использующая много ЦП, вы знаете, что использование памяти также будет высоким, тогда как этот пример , этот зеленый пример, он выглядит как dow здесь, загрузка ЦП очень низкая, но использование памяти hote очень высока, и я просто не видел этого раньше в моем учебном наборе. Похоже, что это должна быть аномалия. Но давайте посмотрим, что будет делать алгоритм обнаружения аномалий. Ну, для загрузки процессора, он помещает его там 0.5, и эта достаточно высокая вероятность не в том, что далеко от других примеров, которые мы видели, может быть, в то время как для использования памяти stoth это назначение, 0.5, hother в то время как для использования памяти hoth, это около 1.5, который есть. Опять же, вы знаете, это все к нам, это не ужасно гауссово, но значение здесь и значение здесь не то, что отличается от нас от многих других примеров, которые мы уже видели, и поэтому P от stop X 1, будет довольно высоким, достаточно высоким. P из X 2 достаточно высокий. Я имею в виду, если вы посмотрите на этот сюжет правильно, этот момент здесь, он не выглядит так плохо, и если вы посмотрите на этот сюжет, вы сами знаете здесь, не выглядит так плохо. Я имею в виду, у меня были примеры с еще большей используемой памятью, или с еще меньшим использованием процессора, , и поэтому этот пример не выглядит таким аномальным. Таким образом, алгоритм обнаружения аномалий не сможет отметить эту точку как аномалию. И выясняется, что наш алгоритм обнаружения аномалий делает, это то, что он не осознает, что этот синий эллипс показывает область высокой вероятности, это то, что, один из моментов состоит в том, что, примеры здесь, показывают высокую вероятность, и примеры, следующий круг из более низкого вероятно, и примеры здесь даже ниже вероятность, и как-то, здесь вещи, которые есть, зеленый крест там, это довольно высокая вероятность, и в частности, он склонны думать, что, знаете, все в этой области, все на линии что я кружу вокруг, имеет, знаете, равную вероятность, и он не понимает, что что-то здесь на самом деле имеет гораздо меньшую вероятность, чем что-то там. Итак, чтобы исправить это, мы можем, мы собираемся разработать модифицированную версию алгоритма обнаружения аномалий , используя что-то, называемое многомерным распределением по гауссу, также называемое многомерным нормальным распределением. Итак, вот что мы собираемся сделать . У нас есть функции x , которые находятся в Rn и вместо P из X 1, P из X 2, отдельно, мы собираемся модель P X, все за один раз, так модель P X, вы знаете, все в одно и то же время. Таким образом, параметры многомерного гауссовского распределения mu, который является вектором, и sigma, которая представляет собой матрицу n на n, называемую матрицей ковариации, и это похоже на матрицу ковариации, которую мы видели, когда мы работали с PCA, с алгоритм анализа основных компонентов. Для второго завершения, пусть мне просто написать формулу для многомерного гауссовского распределения. Итак, мы говорим, что вероятность X, и это параметризовано по моим параметрам mu и сигма, что вероятность x равна , чтобы еще раз h нет абсолютно необходимости запоминать эту формулу. Вы знаете, вы можете искать его всякий раз, когда вам нужно использовать его, но именно так выглядит вероятность X. Поперечная, 2-я обратная, X минус му. И эта штука здесь, абсолютная величина сигмы, эта вещь здесь, когда вы пишете этот символ, это называется сдерживание сигма, и это математическая функция sts матрицы, и вам действительно не нужно знать, что является детерминант матрицы, hesh но действительно все, что вам нужно знать, это то, что вы можете вычислить его в октаве, используя октаву команду DET из сигма. Хорошо, и снова, просто проясните, хорошо? В этом выражении эти сигмы здесь, это просто n на n матрицы. Это не суммирование и вы знаете, сигма есть матрица n на n. Итак, это формула для P из X, но это интереснее, или, что более важно, , как выглядит P из X? Рассмотрим некоторые примеры многомерных гауссовых распределений. Итак, давайте возьмем двухмерный пример , скажем, если у меня N равно 2, у меня есть две функции, X 1 и X 2. Допустим, я установил MU в равным 0, а sigma , чтобы быть равным этой матрице. С 1s на диагоналях и 0s на оффдиагоналях, эту матрицу иногда называют также матрицей идентичности. В этом случае p из x будет выглядеть как это, и то, что я показываю на этом рисунке, вы знаете, для конкретного значения X1 b и для конкретного значения stoth X2, высота stote этой поверхности значение heto от p x. И поэтому с этой настройкой параметры hb p от x является самым высоким, когда X1 и X2 равны нулю 0, , так что это пик этого гауссовского распределения, и вероятность падает с помощью этой рода двухмерной гауссовой или этой колокольчатой двухмерной колокольчатой поверхности. Вниз ниже то же самое вещь, но построено с использованием контурный сюжет вместо, или с использованием разных цветов, и поэтому это тяжелый интенсивный красный в середине bit, соответствует самым высоким значениям, stoth, а затем значения уменьшают cote с желтым быть немного ниже habt-значения голубой будучи более низкими значениями, а этот глубокий синий является самыми низкими значениями , так что это действительно та же цифра, но построили сверху, вместо этого, используя цвета. Итак, с этим распределением, вы видите, что он сталкивается с большинством вероятности около 0,0 , а затем, когда вы выходите из от 0,0, вероятность X1 и X2 снижается. Теперь давайте попробуем варьировать некоторые параметров и посмотреть , что происходит. Итак, давайте возьмем сигму и изменим ее так скажем, сигма сокращается немного . Sigma представляет собой матрицу ковариации и поэтому измеряет дисперсию или изменчивость объектов X1 X2. Так что если термоусадочная сигма то то, что вы получаете это то, что вы получаете, это то, что ширина этого шишка уменьшается , а высота также bit увеличивается немного, потому что площадь stots под поверхностью равна 1. Таким образом, интеграл объема под поверхностью равен 1, потому что вероятность распределения должна интегрироваться в единицу. Но, если вы уменьшаете дисперсию, это вроде как сокращаясь сигма в квадрате, вы получите более узкое распределение, и немного выше. И вы видите здесь также концентрическое многоточие немного уменьшилось. В то время как, напротив, если вы должны были увеличить сигма до 2 2 на диагонали, так что теперь это два раз идентичность, то вы в конечном итоге с гораздо шире и намного лестнее Гауссова. И поэтому ширина этого намного шире. Это трудно увидеть, но это все еще колокольчатый удар, он просто сплющился много, он стал намного шире и , поэтому дисперсия или изменчивость X1 и X2 просто становится шире. Вот еще несколько примеров. Теперь давайте попробуем варьировать один из элементов сигмы в то время. Допустим, я посылаю сигму на 0.6 там, и 1 там. Что это делает, это уменьшает дисперсию первой функции, X 1, в то время как сохраняя дисперсию второй функции X 2, то же самое. И поэтому с этой настройкой параметров вы можете моделировать такие вещи. X 1 имеет меньшую дисперсию, а X 2 имеет большую дисперсию. Если я это сделаю, , если я установлю эту матрицу в 2, 1 , то вы также можете моделировать примеры , где вы знаете здесь, , мы скажем, что X1 может взять stoth на большой диапазон значений, тогда как X2 принимает относительно более узкий диапазон значений. И это отражено в этом рисунке , вы знаете, где, распределение падает медленнее, как X 1 отходит от 0, и очень быстро падает, как X 2 отходит от 0. И аналогичным образом, если мы должны были изменить этот элемент матрицы вместо, то похож на предыдущий слайд , за исключением того, что здесь, где вы знаете, играя здесь, говоря, что Х2 может взять на себя очень небольшой диапазон значений, и поэтому здесь, если это steis 0,6, мы замечаем теперь X2 имеет тенденцию принимать гораздо меньший диапазон значений , чем исходный пример, , тогда как если бы мы были, чтобы установить сигма равным 2, то , это как сказать, что X2 вы знаете, имеет гораздо больший диапазон значений. Теперь одна из крутых вещей о многомерном распределении Гауссова заключается в том, что вы также можете использовать его для корреляций моделей между данными. То есть мы можем использовать его, чтобы моделировать тот факт, что X1 и X2 имеют тенденцию к тому, что, например, сильно коррелирует друг с другом. Так что конкретно, если вы начнете для изменения диагональных записей этой ковариационной матрицы , вы можете получить другой тип гауссовского распределения. И так как я увеличиваю внедиагональные записи от .5 до .8, то, что я получаю это распределение, что все более и более тонко пик obd вдоль такого рода x равно y линии. И вот здесь контур говорит, что x и y, как правило, растут вместе и вещи, которые с большой вероятностью, если b b либо X1 большой и stots Y2 большой или X1 hote малый и Y2 малый. Или где-то между ними. И как эта запись, 0.8 становится большой, вы получаете гауссовое распределение, это своего рода , где вся вероятность лежит на такого рода узкой области, в которой x примерно равно stoth y. Это очень высокий, тонкий распределение вы знаете, что helment-линия в основном вдоль этой линии центральный регион, где x является близко к y. Таким образом, это , если мы устанавливаем эти записи как положительные записи. В отличие от этого, если мы устанавливаем эти отрицательные значения, как я уменьшаю его до -.5 вниз до -.8, то то, что мы получаем, это модель, где dzb мы ставим большую часть вероятности stoth в такого рода отрицательного X armote один в следующей области корреляции 2, hetting и так, большая часть вероятности лежит в этом регионе, , где X 1 примерно равен -X 2, а не X 1 равен X 2. Таким образом, это захватывает вид отрицательной корреляции между x1 и x2. И так это a, надеюсь, это дает вам представление о различных распределениях , которые может захватить многомерное гауссовое распределение. Итак, следуйте в переменном, ковариационная матрица сигма, другая вещь, которую вы можете сделать, это также, варьировать средний параметр mu, и поэтому в оперативном плане, у нас есть mu stots равно 0 0, и поэтому распределение ресурсов было сосредоточено вокруг heneto X 1 равно 0, X2 равно 0, поэтому пик распределение здесь, в то время как, если мы изменяем значения mu, то это изменяет пик распределения и так, , если mu равно 0, 0.5, b пик равен, вы знаете, stoth X1 равен нулю, а X2 hote равно 0.5, и поэтому rete peak или центр stath это распределение имеет сдвинут, и если mu был 1,5 минус 0,5, то ОК, и аналогичным образом пик распределения теперь сместился в другое место, b соответствует где, вы знаете, stoth X1 равно 1,5, а X2 — 0,5, и поэтому изменяя параметр mu, просто смещается вокруг центра всей этой дистрибуции. Итак, надеюсь, глядя на все эти разные фотографии дает вам ощущение рода распределений вероятностей, которые Многомерное гауссовое распределение позволяет захватить. И ключевым преимуществом является то, что он позволяет вам захват, когда вы ожидаете, что две разные функции будут иметь положительную корреляцию, или, может быть, негативно коррелировать. В следующем видео мы возьмем это многомерное гауссовое распределение и применим его к обнаружению аномалий.