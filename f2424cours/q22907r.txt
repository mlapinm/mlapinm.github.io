Multivariate Gaussian Distribution.В этом и следующем видео я хотел бы рассказать вам об одном возможном расширении алгоритма обнаружения аномалий , который мы разработали до сих пор. Это расширение использует что-то, называемое многомерным распределением Гауссова, и у него есть некоторые преимущества, и некоторые недостатки , и он может иногда ловить некоторые аномалии, которых не делал предыдущий алгоритм.
Play video starting at ::21 and follow transcript0:21
Чтобы мотивировать это, давайте начнем с примера.
Play video starting at ::25 and follow transcript0:25
Предположим, что так наши немаркированные данные выглядят как то, что я построил здесь. И я собираюсь использовать пример мониторинга машин в дата-центре, мониторинга компьютеров в дата-центре. Итак, мои две функции - x1 , что является загрузкой процессора и x2 , что, возможно, является использованием памяти.
Play video starting at ::41 and follow transcript0:41
Так что, если я возьму мои две функции, x1 и x2, и я моделирую их как гауссос, то вот сюжет моих функций X1, вот сюжет о том, как мои функции X2, stoth2, и поэтому, если я вписываюсь в stet-Gaussian к этому, может быть, я получу Гауссова, как это, зависит от параметров mu 1, и сигма в квадрате 1, и вот моя память используется, и, вы знаете, может быть, я получу гауссовскую , которая выглядит так, и это мой P из X 2, bet, который зависит от mu 2 и sigma в квадрате 2. И так вот как алгоритм обнаружения аномалий модели X1 и X2.
Play video starting at :1:19 and follow transcript1:19
Теперь предположим, что в наборах тестов у меня есть пример , который выглядит так.
Play video starting at :1:25 and follow transcript1:25
Расположение этого зеленого креста , поэтому значение X 1 составляет около 0,4, а значение X 2 — около 1,5. Теперь, если вы посмотрите на данные, это выглядит так, да, большая часть данных лежит в этом регионе, и так, что зеленый крест довольно далеко от всех данных, которые я видел. Похоже, что это должно быть поднято как аномалия. Итак, в моих данных , в моих, в данных моих хороших примерах, это выглядит так, как, знаете, загрузка процессора и использование памяти , они сортируют линейно растут друг с другом. Так что, если у меня есть машина , использующая много ЦП, вы знаете, что использование памяти также будет высоким, тогда как этот пример , этот зеленый пример, он выглядит как dow здесь, загрузка ЦП очень низкая, но использование памяти hote очень высока, и я просто не видел этого раньше в моем учебном наборе. Похоже, что это должна быть аномалия.
Play video starting at :2:13 and follow transcript2:13
Но давайте посмотрим, что будет делать алгоритм обнаружения аномалий. Ну, для загрузки процессора, он помещает его там 0.5, и эта достаточно высокая вероятность не в том, что далеко от других примеров, которые мы видели, может быть, в то время как для использования памяти stoth это назначение, 0.5, hother в то время как для использования памяти hoth, это около 1.5, который есть. Опять же, вы знаете, это все к нам, это не ужасно гауссово, но значение здесь и значение здесь не то, что отличается от нас от многих других примеров, которые мы уже видели, и поэтому P от stop X 1, будет довольно высоким, достаточно высоким. P из X 2 достаточно высокий. Я имею в виду, если вы посмотрите на этот сюжет правильно, этот момент здесь, он не выглядит так плохо, и если вы посмотрите на этот сюжет, вы сами знаете здесь, не выглядит так плохо. Я имею в виду, у меня были примеры с еще большей используемой памятью, или с еще меньшим использованием процессора, , и поэтому этот пример не выглядит таким аномальным.
Play video starting at :3:5 and follow transcript3:05
Таким образом, алгоритм обнаружения аномалий не сможет отметить эту точку как аномалию. И выясняется, что наш алгоритм обнаружения аномалий делает, это то, что он не осознает, что этот синий эллипс показывает область высокой вероятности, это то, что, один из моментов состоит в том, что, примеры здесь, показывают высокую вероятность, и примеры, следующий круг
Play video starting at :3:26 and follow transcript3:26
из более низкого вероятно, и примеры здесь даже ниже вероятность, и как-то, здесь вещи, которые есть, зеленый крест там, это довольно высокая вероятность,
Play video starting at :3:34 and follow transcript3:34
и в частности, он склонны думать, что, знаете, все в этой области, все на линии что я кружу вокруг, имеет, знаете, равную вероятность, и он не понимает, что что-то здесь на самом деле имеет гораздо меньшую вероятность, чем что-то там.
Play video starting at :3:55 and follow transcript3:55
Итак, чтобы исправить это, мы можем, мы собираемся разработать модифицированную версию алгоритма обнаружения аномалий , используя что-то, называемое многомерным распределением по гауссу, также называемое многомерным нормальным распределением.
Play video starting at :4:7 and follow transcript4:07
Итак, вот что мы собираемся сделать . У нас есть функции x , которые находятся в Rn и вместо P из X 1, P из X 2, отдельно, мы собираемся модель P X, все за один раз, так модель P X, вы знаете, все в одно и то же время.
Play video starting at :4:20 and follow transcript4:20
Таким образом, параметры многомерного гауссовского распределения mu, который является вектором, и sigma, которая представляет собой матрицу n на n, называемую матрицей ковариации,
Play video starting at :4:29 and follow transcript4:29
и это похоже на матрицу ковариации, которую мы видели, когда мы работали с PCA, с алгоритм анализа основных компонентов.
Play video starting at :4:37 and follow transcript4:37
Для второго завершения, пусть мне просто написать формулу
Play video starting at :4:40 and follow transcript4:40
для многомерного гауссовского распределения. Итак, мы говорим, что вероятность X, и это параметризовано по моим параметрам mu и сигма, что вероятность x равна , чтобы еще раз h нет абсолютно необходимости запоминать эту формулу.
Play video starting at :4:56 and follow transcript4:56
Вы знаете, вы можете искать его всякий раз, когда вам нужно использовать его, но именно так выглядит вероятность X.
Play video starting at :5:3 and follow transcript5:03
Поперечная, 2-я обратная, X минус му.
Play video starting at :5:7 and follow transcript5:07
И эта штука здесь,
Play video starting at :5:10 and follow transcript5:10
абсолютная величина сигмы, эта вещь здесь, когда вы пишете этот символ, это называется сдерживание сигма, и это математическая функция sts матрицы, и вам действительно не нужно знать, что является детерминант матрицы, hesh но действительно все, что вам нужно знать, это то, что вы можете вычислить его в октаве, используя октаву команду DET из
Play video starting at :5:33 and follow transcript5:33
сигма. Хорошо, и снова, просто проясните, хорошо? В этом выражении эти сигмы здесь, это просто n на n матрицы. Это не суммирование и вы знаете, сигма есть матрица n на n.
Play video starting at :5:46 and follow transcript5:46
Итак, это формула для P из X, но это интереснее, или, что более важно,
Play video starting at :5:53 and follow transcript5:53
, как выглядит P из X? Рассмотрим некоторые примеры многомерных гауссовых распределений.
Play video starting at :6:2 and follow transcript6:02
Итак, давайте возьмем двухмерный пример , скажем, если у меня N равно 2, у меня есть две функции, X 1 и X 2.
Play video starting at :6:9 and follow transcript6:09
Допустим, я установил MU в равным 0, а sigma , чтобы быть равным этой матрице. С 1s на диагоналях и 0s на оффдиагоналях, эту матрицу иногда называют также матрицей идентичности.
Play video starting at :6:21 and follow transcript6:21
В этом случае p из x будет выглядеть как это, и то, что я показываю на этом рисунке, вы знаете, для конкретного значения X1 b и для конкретного значения stoth X2, высота stote этой поверхности значение heto от p x. И поэтому с этой настройкой параметры hb p от x
Play video starting at :6:40 and follow transcript6:40
является самым высоким, когда X1 и X2 равны нулю 0, , так что это пик этого гауссовского распределения,
Play video starting at :6:46 and follow transcript6:46
и вероятность падает с помощью этой рода двухмерной гауссовой или этой колокольчатой двухмерной колокольчатой поверхности.
Play video starting at :6:55 and follow transcript6:55
Вниз ниже то же самое вещь, но построено с использованием контурный сюжет вместо, или с использованием разных цветов, и поэтому это тяжелый интенсивный красный в середине bit, соответствует самым высоким значениям, stoth, а затем значения уменьшают cote с желтым быть немного ниже habt-значения голубой будучи более низкими значениями, а этот глубокий синий является самыми низкими значениями , так что это действительно та же цифра, но построили сверху, вместо этого, используя цвета.
Play video starting at :7:21 and follow transcript7:21
Итак, с этим распределением,
Play video starting at :7:23 and follow transcript7:23
вы видите, что он сталкивается с большинством вероятности около 0,0 , а затем, когда вы выходите из от 0,0, вероятность X1 и X2 снижается.
Play video starting at :7:36 and follow transcript7:36
Теперь давайте попробуем варьировать некоторые параметров и посмотреть , что происходит. Итак, давайте возьмем сигму и изменим ее так скажем, сигма сокращается немного . Sigma представляет собой матрицу ковариации и поэтому измеряет дисперсию или изменчивость объектов X1 X2. Так что если термоусадочная сигма то то, что вы получаете это то, что вы получаете, это то, что ширина этого шишка уменьшается
Play video starting at :7:57 and follow transcript7:57
, а высота также bit увеличивается немного, потому что площадь stots под поверхностью равна 1. Таким образом, интеграл объема под поверхностью равен 1, потому что вероятность распределения должна интегрироваться в единицу. Но, если вы уменьшаете дисперсию,
Play video starting at :8:12 and follow transcript8:12
это вроде как сокращаясь сигма в квадрате, вы получите более узкое распределение, и немного выше. И вы видите здесь также концентрическое многоточие немного уменьшилось. В то время как, напротив, если вы должны были увеличить сигма
Play video starting at :8:29 and follow transcript8:29
до 2 2 на диагонали, так что теперь это два раз идентичность, то вы в конечном итоге с гораздо шире и намного лестнее Гауссова. И поэтому ширина этого намного шире. Это трудно увидеть, но это все еще колокольчатый удар, он просто сплющился много, он стал намного шире и , поэтому дисперсия или изменчивость X1 и X2 просто становится шире.
Play video starting at :8:50 and follow transcript8:50
Вот еще несколько примеров. Теперь давайте попробуем варьировать один из элементов сигмы в то время. Допустим, я посылаю сигму на 0.6 там, и 1 там.
Play video starting at :9:1 and follow transcript9:01
Что это делает, это уменьшает дисперсию
Play video starting at :9:5 and follow transcript9:05
первой функции, X 1, в то время как сохраняя дисперсию второй функции X 2, то же самое. И поэтому с этой настройкой параметров вы можете моделировать такие вещи. X 1 имеет меньшую дисперсию, а X 2 имеет большую дисперсию. Если я это сделаю, , если я установлю эту матрицу в 2, 1 , то вы также можете моделировать примеры , где вы знаете здесь,
Play video starting at :9:28 and follow transcript9:28
, мы скажем, что X1 может взять stoth на большой диапазон значений, тогда как X2 принимает относительно более узкий диапазон значений. И это отражено в этом рисунке , вы знаете, где, распределение падает медленнее, как X 1 отходит от 0, и очень быстро падает, как X 2 отходит от 0.
Play video starting at :9:49 and follow transcript9:49
И аналогичным образом, если мы должны были изменить этот элемент матрицы вместо, то похож на предыдущий слайд
Play video starting at :9:57 and follow transcript9:57
, за исключением того, что здесь, где вы знаете, играя здесь, говоря, что Х2 может взять на себя очень небольшой диапазон значений, и поэтому здесь, если это steis 0,6, мы замечаем теперь X2
Play video starting at :10:9 and follow transcript10:09
имеет тенденцию принимать гораздо меньший диапазон значений , чем исходный пример,
Play video starting at :10:14 and follow transcript10:14
, тогда как если бы мы были, чтобы установить сигма равным 2, то , это как сказать, что X2 вы знаете, имеет гораздо больший диапазон значений.
Play video starting at :10:22 and follow transcript10:22
Теперь одна из крутых вещей о многомерном распределении Гауссова заключается в том, что вы также можете использовать его для корреляций моделей между данными. То есть мы можем использовать его, чтобы моделировать тот факт, что X1 и X2 имеют тенденцию к тому, что, например, сильно коррелирует друг с другом. Так что конкретно, если вы начнете для изменения диагональных записей этой ковариационной матрицы , вы можете получить другой тип гауссовского распределения.
Play video starting at :10:46 and follow transcript10:46
И так как я увеличиваю внедиагональные записи от .5 до .8, то, что я получаю это распределение, что все более и более тонко пик obd вдоль такого рода x равно y линии. И вот здесь контур говорит, что x и y, как правило, растут вместе и вещи, которые с большой вероятностью, если b b либо X1 большой и stots Y2 большой или X1 hote малый и Y2 малый. Или где-то между ними. И как эта запись, 0.8 становится большой, вы получаете гауссовое распределение, это своего рода , где вся вероятность лежит на такого рода узкой области, в которой x примерно равно stoth y. Это очень высокий, тонкий распределение вы знаете, что helment-линия в основном вдоль этой линии
Play video starting at :11:33 and follow transcript11:33
центральный регион, где x является близко к y. Таким образом, это , если мы устанавливаем эти записи как положительные записи. В отличие от этого, если мы устанавливаем эти отрицательные значения, как я уменьшаю его до -.5 вниз до -.8, то то, что мы получаем, это модель, где dzb мы ставим большую часть вероятности stoth в такого рода отрицательного X armote один в следующей области корреляции 2, hetting и так, большая часть вероятности лежит в этом регионе, , где X 1 примерно равен -X 2, а не X 1 равен X 2. Таким образом, это захватывает вид отрицательной корреляции между x1
Play video starting at :12:10 and follow transcript12:10
и x2. И так это a, надеюсь, это дает вам представление о различных распределениях , которые может захватить многомерное гауссовое распределение.
Play video starting at :12:18 and follow transcript12:18
Итак, следуйте в переменном, ковариационная матрица сигма, другая вещь, которую вы можете сделать, это также, варьировать средний параметр mu, и поэтому в оперативном плане, у нас есть mu stots равно 0 0, и поэтому распределение ресурсов было сосредоточено вокруг heneto X 1 равно 0, X2 равно 0, поэтому пик распределение здесь, в то время как, если мы изменяем значения mu, то это изменяет пик распределения и так, , если mu равно 0, 0.5, b пик равен, вы знаете, stoth X1 равен нулю, а X2 hote равно 0.5, и поэтому rete peak или центр stath это распределение имеет сдвинут,
Play video starting at :12:56 and follow transcript12:56
и если mu был 1,5 минус 0,5, то ОК,
Play video starting at :13:1 and follow transcript13:01
и аналогичным образом пик распределения теперь сместился в другое место, b соответствует где, вы знаете, stoth X1 равно 1,5, а X2 — 0,5, и поэтому изменяя параметр mu, просто смещается вокруг центра всей этой дистрибуции. Итак, надеюсь, глядя на все эти разные фотографии дает вам ощущение рода распределений вероятностей, которые Многомерное гауссовое распределение позволяет захватить. И ключевым преимуществом является то, что он позволяет вам захват, когда вы ожидаете, что две разные функции будут иметь положительную корреляцию, или, может быть, негативно коррелировать.
Play video starting at :13:37 and follow transcript13:37
В следующем видео мы возьмем это многомерное гауссовое распределение и применим его к обнаружению аномалий.