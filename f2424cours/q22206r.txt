Normal Equation.В этом видео мы поговорим о нормальном уравнении, которое для некоторых случаев линейной регрессии дает гораздо лучший способ найти оптимальные значения параметров тета. Пока что для задач линейной регрессии мы пользовались методом градиентного спуска, в котором для поиска минимума функции затрат J от тэта запускался итеративный алгоритм. В результате множества шагов, множества итераций градиентного спуска, алгоритм сходился к глобальному минимуму. Нормальное уравнение же дает нам возможность найти вектор тета аналитически, без необходимости прогонять этот итеративный алгоритм, а просто вычислить оптимальное значение тета за раз, то есть фактически за один шаг вы получаете оптимальное значение.
Play video starting at ::49 and follow transcript0:49
Оказывается, что у нормального уравнения есть свои достоинства и свои недостатки, но прежде чем обсуждать, когда стоит его использовать, давайте попробуем понять общую идею того, как работает этот метод. Давайте возьмем пример из лекций этой недели и представим себе крайне упрощенную функцию стоимости J(тета) - просто функцию от действительного числа "тета". То есть, сейчас представьте себе, что тета - это просто скаляр, принимающий вещественные значения. Обычное число, а не вектор. Представьте, что у нас есть функция стоимость J, которая является квадратичной от этого действительного числа, параметра тета - то есть, J(тета) выглядит вот так. Итак, как найти минимум квадратичной функции? Те из вас, кто знает немного матанализа, наверное, помнят, что для нахождения минимума функции нужно взять ее производные и приравнять производные к нулю. Так что вы берете производную от J по параметру тета. Получается некоторая формула, которую я не буду сейчас выводить. Вы приравниваете эту производную к нулю, и это позволяет вам получить значение параметра тета, которое минимизирует J(тета). Это был простой пример, когда мы рассматривали просто действительное число. Но в задаче, которая нас интересует, тета уже не просто вещественное число, а вектор размерности n+1, и функция затрат J - это функция от этого вектора, от чисел тета_0 до тета_m.И функция затрат выглядит как-то так, некоторая квадратичная функция - как в правой части. Как нам найти минимум функции стоимости J? Матанализ говорит нам, что один способ сделать это - взять частную производную от J относительно каждого из параметров тета по очереди, и затем приравнять их все к нулю. Есть сделать это и найти значения тета_0, тета_1 и так далее до тета_n, получится вектор параметров, минимизирующий функцию затрат J. Если серьезно браться за анализ и вычислять все значения параметров тета_0 ... тета_n, оказывается, что нужно будет считать довольно много производных. Но в этом видео я на самом деле не планирую заниматься вычислением производных - это долго и довольно утомительно, я просто хочу рассказать вам, что нужно сделать, чтобы реализовать эти вычисления, чтобы вы могли найти значения вектора тета, соответствующие нулевым значениям частных производных. Или, что одно и то же, значения параметров тета, которые минимизируют функцию J(тета). Я понимаю, что некоторые мои комментарии понятны только тем из вас, кто знаком с матанализом. Но не беспокойтесь, если вы незнакомы или лишь немного знакомы с матанализом. Я просто расскажу вам все, что вам нужно знать, чтобы реализовать эти алгоритмы и заставить их работать. Для своего примера я возьму ситуацию, когда у меня, скажем, обучающая выборка состоит из m = 4 примеров.
Play video starting at :3:50 and follow transcript3:50
Чтобы составить нормальное уравнение, я сделаю следующее. Я возьму свой набор данных, вот они, мои четыре обучающих примера. В данном случае давайте представим себе, что эти четыре примера - это все мои данные. Дальше я беру этот набор данных и добавляю еще один столбец, который соответствует моему дополнительному признаку, x0, который всегда принимает значение 1. А дальше я строю матрицу X, которая фактически содержит все характеристики моей обучающей выборки. Конкретно, вот они все характеристики выборки, я возьму все эти числа и подставлю их в матрицу X, хорошо? Так сказать, копирую данные столбец за столбцом, а затем сделаю нечто похожее для характеристики "y". Я беру эти значения, которые мне нужно предсказать, и строю теперь вектор, вот так, и называю его вектор "y". То есть, X будет матрицей
Play video starting at :4:59 and follow transcript4:59
размерности m*(n+1), а "y" будет вектором размерности m, где m - число обучающих примеров, а n - число характеристик выборки. n+1 получается из-за той дополнительной характеристики x_0, которую я добавил. Наконец, если взять матрицу X и вектор "у", и вычислить вот такое выражение, сделать тета равным (X транспонированное умножить на X) в минус первой эту - обратное от X транспонированного на X, умноженное на X транспонированное, умножить на "y", то получится значение т ета, минимизирующее значение функции затрат. На этом слайде написано много всего, и всё на единственном примере набора данных. Давайте я теперь напишу это всё в более общей форме, а потом в этом же видео я поясню еще немного про это уравнение.
Play video starting at :5:57 and follow transcript5:57
Если пока еще не до конца понятно, как это делать. В общем случае, пусть у нас будет m обучающих примеров: от (х^1, y^1) до (x^m, y^m), и n характеристик. То есть каждый из обучающих примеров может представлять собой вот такой вектор, то есть вектор характеристик выборки с размерностью n+1. Построим матрицу X, также называемую матрицей плана, следующим образом. Каждый обучающий пример дает мне вектор характеристик, такой что, вектор размерности n+1. Единственный способ, как можно составить матрицу факторов "X", это составить ее вот так. Возьмем первый обучающий пример, вектор x^1, транспонируем его, получится вот такая длинная плоская штуковина, и скажем, что x^1 – это первый ряд матрицы факторов. Возьмем второй, x^2, транспонируем, поставим во второй ряд матрицы X. Продолжим этот процесс до последнего обучающего примера. Транспонируем, и это последняя строка в матрице "X". Таким образом получается матрица размерности m на n+1. В качестве конкретного примера рассмотрим простейший случай, когда у нас есть только одна характеристика выборки, отличная от вектора x_0, всегда состоящего из единиц. Тогда если мой вектор характеристик x(i) состоит из x0 = 1 и некой действительной характеристики, как например размер дома, тогда матрица плана, X, будет равна вот этому: В первый ряд я фактически подставляю вот этот транспонированный x(i). В строке будет стоять 1 и затем x^(1)_1. Во второй строке 1 и затем x^1_2 и так далее до строки 1 и затем x^1_m. Таким образом получится матрица размерности m на 2. Итак, мы разобрались, как построить матрицу плана X. Что касается вектора y, иногда я могу писать сверху стрелку, чтобы обозначить, что это вектор, хотя часто я буду писать просто y. Вектор "y" получается из всех - всех данных по ценам домов в моей обучающей выборке, если выстроить их одну над другой в виде вектора размерности m. В конечном итоге, имея матрицу "X" и вектор "y", можно вычислить тета как 1/(X'X) * X' * y. Хочу убедиться, что вы понимаете смысл этого уравнения и как им пользоваться. А именно, что означает 1/(X'X)? Так вот, 1/(X'X) – это матрица, обратная произведению транспонированной X и просто X. Точнее, предположим вы назначили некоторую A = X' * X. Транспонированная X - это матрица,  X' * X дает тоже матрицу, которую мы назвали A. Теперь, (X' * X)^-1 означает, что мы взяли вот эту A и обратили ее, так? Получается, скажем, матрица, обратная A.
Play video starting at :9:26 and follow transcript9:26
Вы вычисляете данное произведение, X транспонированную на X, а затем вычисляете для получившегося обратную матрицу. Еще нужно будет обсудить программирование в Octave. Мы сделаем это в последующих видео. В языке Octave или в других достаточно похожих языках, например в MATLAB, команда вычислить эту величину, вернее, вот эту - обратное от X транспонированного на X, умноженное на X транспонированное, на y - выглядит следующим образом: В Octave, X' используется, чтобы обозначить X транспонированное. Таким образом, выражение в красной рамке вычисляет произведение транспонированной матрицы X и матрицы X. pinv - это функция для нахождения обратной матрицы, поэтому вот это вычисляет транспонированную X на обратную X, умноженную на транспонированную X и умноженную на Y. Так мы вычислили эту формулу, доказательство которой я опустил. Однако, математически возможно показать (хотя я не буду сейчас этого делать), что эта формула дает вам оптимальное значения ?. То есть то самое значение, которое минимизирует затратную функцию J (?) для линейной регрессии. И еще одна важная деталь. В предыдущем видео я говорил о масштабировании факторов, которое позволяет добиться того, чтобы их значения располагались на одной шкале и имели бы одинаковые пределы. Если вы используете нормированное уравнение, то необходимость в масштабировании факторов отпадает. Фактически приемлемым является, например, случай, при котором некоторый фактор x1 принимает значения от 0 до 1, некоторый x2 - от 0 до 1000, а некоторый x3 вообще от 0 до 10^5. И, если вы используете нормированное уравнение, все в порядке - нет надобности проводить масштабирование. Если же вы применяете градиентный спуск, помнить о масштабировании очень важно. Теперь - когда следует использовать градиентный спуска, а когда - метод нормированного уравнения? Вот некоторые из недостатков и преимуществ обоих методов. Предположим, у нас имеется m обучающих примеров и n факторов. Один из недостатков градиентного спуска состоит в том, что нам нужно как-то выбрать параметр скорости обучения ?. Очень часто это приводит к тому, что приходится прогонять спуск несколько раз с разными значениями ? и только потом выбирать, какое значение работает лучше. А это - в некотором смысле дополнительная забота. Другой недостаток градиентного спуска - большое число итераций при его прогоне. И, при прочих равных, это может серьезно замедлить его работу. Хотя, как мы увидим через пару секунд, здесь есть некоторые нюансы. При работе с нормированным уравнением не возникает надобности выбирать параметр скорости обучения ?. Это, как нетрудно догадаться, делает его чрезвычайно легким в работе и реализации. Запустили и готово - оно просто работает. И не нужно никаких итераций. Не нужно строить график J(?), проверять сходимость и вообще предпринимать какие-то дополнительные шаги. Казалось бы, все говорит в пользу нормированного уравнения. Однако, рассмотрим некоторые недостатки нормированного уравнения и некоторые преимущества градиентного спуска. Градиентный спуск неплохо справляется даже в случае огромного числа факторов. Так что, даже если у вас миллионы факторов, можно использовать градиентный спуск, который в этом случае будет достаточно эффективен. Результат будет более менее вменяемый. В случае же нормированного уравнения, для того, чтобы получить параметры ?, необходимо вычислить вот это выражение. Нам нужно вычислить обратную матрицу произведения транспонированной X на X. Вот эта матрица (транспонированная X на X) имеет размер n на n, если n - число факторов. Почему так? Если вы посмотрите на размер транспонированной X, а затем на размер X, то увидите, что размер их произведения - n на n, где n - число факторов.А для большинства компьютерных реализаций алгоритма обращения матриц, затраты на обращение резко растут как куб размера обращаемой матрицы. Иными словами, порядок роста времени выполнения алгоритма обращения матрицы составляет n^3. Иногда он выполняется чуть быстрее, но мы можем считать, что время все равно очень близко к n^3. Так что, если число факторов очень велико, то вычисление
Play video starting at :13:37 and follow transcript13:37
вот этого значения может быть очень медленным, что в свою очередь сильно замедлит  метод нормального уравнения. Поэтому, если n слишком велико, я предпочитаю использовать градиентный спуск, поскольку не хочу расплачиваться n-кубическим ростом времени выполнения. Однако, если n относительно мало, нормальное уравнение дает более удобный способ найти параметры гипотезы. А что значит "n мало" и "n велико"? Ну, скажем, если n составляет порядка 100, то обращения матрицы размером 100 на 100 не составляет труда по меркам сегодняшних вычислительных ресурсов. Если n составляет 1000, я бы все еще воспользовался методом нормированного уравнения. Обращение матрицы размером 1000 на 1000, в общем, происходит достаточно быстро на современных компьютерах. Если n составляет 10 000, тут я уже задумаюсь. Обращение матрицы размером 10 000 на 10 000 будет уже подтормаживать. Я начну посматривать в сторону градиентного спуска, но не так чтобы очень активно. Обратить матрицу такого размера в принципе возможно. Но, если это займет гораздо больше времени, я скорее всего воспользуюсь градиентным спуском. Так что, если n составляет 10 в 6-й степени, то есть миллион факторов, то обращение матрицы размером миллион на миллион будет очень затратным. Если у вас такое количество факторов, я бы определенно порекомендовал градиентный спуск. Так насколько велико должно быть число факторов для того, чтобы принять решение о переходе на градиентный спуск? Конкретное число определить сложно. Лично я начинаю задумываться о применении градиентного спуска или других алгоритмов, о которых мы поговорим позже в курсе, когда число факторов начинает приближаться к 10 000. Итак, если число факторов не слишком велико, нормированное уравнение дает отличный альтернативный метод для нахождения параметров тета. Точнее, если число факторов меньше 1000, я обычно использую нормированное уравнение, а не градиентный спуск. Немного забегая вперёд, скажу, что для более сложных алгоритмов обучения... алгоритмов классификации, о которых мы будем говорить далее в нашем курсе - таких, например, как логистическая регрессия, - так вот для этих алгоритмов метод нормального уравнения не работает. Так что мы будем вынуждены вновь вернуться к градиентному спуску. Поэтому, знать, как работает градиентный спуск, очень полезно. Именно потому, что, как в случае с линейной регрессией с большим числом факторов, так и в случае некоторых других алгоритмов, которые мы рассмотрим позже, метод нормированного уравнения просто не применим, он просто не работает. Однако, при определенной модели линейной регресии нормированное уравнение может представлять более быструю
Play video starting at :16:7 and follow transcript16:07
альтернативу градиентному спуску. Так что, при должном понимании особенностей работы алгоритмов, особенностей задачи, которую вы решаете, понимании влияния числа факторов, оба алгоритма заслуживают знания и понимания.