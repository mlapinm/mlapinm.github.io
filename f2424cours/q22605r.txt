Regularization and Bias/Variance.Вы видели, как регуляризация может помочь предотвратить перегонки. Но как это влияет на смещение и дисперсии алгоритма обучения? В этом видео я хотел бы глубже рассмотреть вопрос о смещении и дисперсиях и рассказать о том, как он взаимодействует с и зависит от регуляризации вашего алгоритма обучения.
Play video starting at ::22 and follow transcript0:22
Предположим, что мы устанавливаем высокий автоматический полином, как показано здесь, но , чтобы предотвратить подгонку, нам нужно использовать регуляризацию, как показано здесь. Таким образом, у нас есть этот термин регуляризации, чтобы попытаться сохранить значения према на малых. И, как обычно, регуляризация происходит от J = 1 до m, а не j = 0 до m. Рассмотрим три случая. Первый случай очень большого значения параметра регуляризации лямбда, например, если бы лямбда была равна 10 000. Какое-то огромное значение.
Play video starting at ::54 and follow transcript0:54
В этом случае все эти параметры, theta 1, theta 2, theta 3 и поэтому on будут сильно наказаны и , поэтому большинство этих значений параметров ближе к нулю. И гипотеза будет примерно h от x, просто равна или приблизительно равна тета-нулю. Таким образом, мы имеем гипотезу о том, что более или менее выглядит так, более или меньше плоской, постоянной прямой линии. И поэтому эта гипотеза имеет высокую смещение, и она плохо подходит для этого набора данных, , поэтому горизонтальная прямая линия просто не очень хорошая модель для этого набора данных. На другой крайности, если у нас очень небольшое значение лямбда, , например, если бы лямбда были равны нулю. В этом случае, учитывая, что мы устанавливаем полинома высокого порядка, это обычная настройка перегонки. В этом случае, учитывая, что мы устанавливаем полином высокого порядка, в основном, без регуляризации или с очень минимальной регуляризацией, мы заканчиваем нашей обычной высокой дисперсией, над настройкой фитинга. Это в основном, если лямбда равна нулю, мы просто вписываемся с нашей регуляризации, так что более подходит гипотеза. И только если у нас есть некоторое промежуточное значение длиннее, которое не является ни слишком большим, ни слишком малым, мы получаем данные параметров , которые дают нам разумную пригодность к этим данным. Итак, как мы можем автоматически выбрать хорошее значение для параметра регуляризации?
Play video starting at :2:19 and follow transcript2:19
Просто повторяю, вот наша модель, и вот цель нашего алгоритма обучения. Для настройки, где мы используем регуляризацию, позвольте мне определить J train (theta), чтобы быть чем-то другим, , чтобы быть целью оптимизации, но без термина регуляризации. Ранее, в более раннем видео, когда мы не использовали регуляризацию, я определяю J поезд данных, чтобы быть таким же, как J theta, как функция причины, но когда мы используем регуляризацию, когда шесть хорошо под термином мы собираемся определить J train мой обучающий набор, чтобы быть просто моей суммой квадрата ошибки на тренировочной наборе или моя средняя ошибка в квадрате на тренировочной установке без учета этой регуляризации. И аналогичным образом я также собираюсь определить ошибку перекрестной проверки и , чтобы проверить эту ошибку, как и раньше, чтобы быть средней суммой квадратных ошибок на перекрестной проверке в тестовых наборах, так что просто суммировать мои определения J поезда J CU и J тест - это просто средний квадрат там одна половина другой квадратной записи на тренировочную валидацию тестового набора без дополнительного срока регуляризации. Итак, так мы можем автоматически выбрать регуляризацию параметр лямбда. Итак, что я обычно делаю, возможно, имеет некоторый диапазон значений лямбда, который я хочу попробовать Поэтому я мог бы рассмотреть вопрос о том, чтобы не использовать регуляризацию или вот несколько значений, которые я мог бы попробовать лямбда, учитывая лямбда = 0.01, 0.02, 0.04 и так далее. И я обычно устанавливаю их кратно двум, пока какое-то, возможно, большее значение , если бы я сделал это кратно 2, я бы в конечном итоге с 10.24. Это 10 ровно, но это достаточно близко. И от трех до четырех знаков после запятой не повлияет на ваш результат так сильно. Итак, это дает мне, возможно, 12 различных моделей. И я пытаюсь выбрать месяц, соответствующий 12 различным значениям регуляризации параметра лямбда. И, конечно, вы также можете перейти к значениям менее 0,01 или больше 10, но я только что обрезал его здесь для удобства. Учитывая проблему этих 12 моделей, то, что мы можем сделать, это следующее, мы можем взять эту первую модель с лямбда равным нулю и минимизировать мою функцию стоимости J данных, и это даст мне некоторый параметр активных данных. И похоже на более раннее видео, позвольте мне просто обозначить это как theta super скрипт один.
Play video starting at :4:49 and follow transcript4:49
И тогда я могу взять свою вторую модель с лямбда установленным на 0.01 и минимизировать мою функцию затрат теперь, используя лямбда равно 0.01, конечно. Чтобы получить несколько разных параметров вектора тета. Позвольте мне обозначить, что тета (2). И для этого я заканчиваю тета (3). Так что если часть для моей третьей модели. И так далее, пока для моей окончательной модели с лямбдой установлен в 10 или 10.24, я заканчиваю этой тетой (12). Далее, я могу говорить все эти гипотезы, все эти параметры и использовать мой набор перекрестной проверки для их проверки, поэтому я могу посмотреть на свою первую модель, мою вторую модель, , подходящих к этим различным значениям параметра регуляризации, и оценить их с помощью моего набора перекрестной проверки на основе измерить среднюю квадратную ошибку каждого из этих параметров квадратного вектора theta в моих наборах перекрестной проверки. И тогда я бы выбрал, какая из этих 12 моделей дает мне самую низкую ошибку в наборе trans validation. И предположим, ради этого примера, что я в конечном итоге выбираю theta 5, полином 5-го порядка, потому что это имеет самую низкую ошибку проверки причины. Сделав это, наконец, что бы я сделал, если бы хотел сообщить об ошибке каждого набора тестов, должен взять параметр theta 5, который я выбрал, и посмотреть, насколько хорошо он работает на моем тестовом наборе. Еще раз, вот как если бы мы подошли этот параметр, theta, к моему множеству перекрестной проверки, поэтому я откладываю отдельный тестовый набор, который я собираюсь использовать, чтобы получить лучшую оценку того, как хорошо мой вектор параметров, тета, будет обобщать ранее невидимые примеры. Таким образом, выбор модели применяется для выбора параметра регуляризации лямбда. Последнее, что я хотел бы сделать в этом видео, это получить лучшее понимание о том, как варьируются перекрестная проверка и ошибка обучения , поскольку мы меняем параметр регуляризации лямбда. И так просто напоминание право, что это была наша первоначальная стоимость на j из тета. Но для этого мы будем определять ошибку обучения без использования параметра регуляризации и ошибку перекрестной валидации без использования параметра регуляризации.
Play video starting at :7:7 and follow transcript7:07
И то, что я хотел бы сделать, это построить этот Jtrain и построить этот Jcv, означает, насколько хорошо моя гипотеза работает на тренировочном наборе и , как моя гипотеза делает, когда она пересекает наборы валидации. Как я изменяю свой параметр регуляризации лямбда.
Play video starting at :7:27 and follow transcript7:27
Так что, как мы видели ранее, если лямбда мала, мы не используем много регуляризации
Play video starting at :7:35 and follow transcript7:35
, и мы сталкиваемся с большим риском перегонки, тогда как если лямбда велика, то если бы мы были на правой части этой горизонтальной оси, то, с большим значением лямбда, мы сталкиваемся с более высоким риском , поэтому , если вы строите поезд J и J cv, вы обнаружите, что для небольших значений лямбда вы можете поместить торговый набор относительно образом, потому что вы не регуляризируете. Итак, для небольших значений лямбда термин регуляризации в основном уходит, , и вы просто минимизируете в значительной степени только серые стрелки. Поэтому, когда лямбда маленькая, вы получаете небольшое значение для Jtrain, , тогда как если лямбда большая, то у вас есть проблема с высокой смещением, и вы можете не чувствовать свою подготовку так хорошо, поэтому вы в конечном итоге получите значение там. Таким образом, Jtrain theta будет иметь тенденцию увеличиваться, когда лямбда увеличивается, , потому что большое значение лямбда соответствует высокому смещению, где вы может даже не соответствовать вашим тренировкам так хорошо, в то время как небольшое значение лямбда соответствует, , если вы действительно можете соответствовать очень высокой степени полинома к вашему данных, скажем. После ошибки проверки стоимости мы получаем такую цифру, как эта,
Play video starting at :8:51 and follow transcript8:51
, где здесь справа, если у нас есть большое значение лямбда, мы можем оказаться под подгонкой, и поэтому это режим смещения. И поэтому ошибка перекрестной проверки будет высокой. Позвольте мне просто оставить все это для этого Jcv (theta), потому что с высоким уклоном мы не будем подгоняться, мы не будем делать хорошо в наборах перекрестной проверки, тогда как здесь слева, это режим высокой дисперсии, где у нас есть два меньших значения с более длинными, то мы можем быть над подгонкой данных. И поэтому, по сравнению с данными, ошибка перекрестной проверки также будет высокой. Итак, это то, что ошибка перекрестной проверки и как торговая ошибка может выглядеть на торговой позиции, так как мы изменяем параметр регуляризации лямбда. И так еще раз, это часто будет какое-то промежуточное значение лямбды, которое является просто правильным или которое работает лучше всего С точки зрения небольшой ошибки перекрестной проверки или небольшой тест тета. И в то время как кривые, которые я нарисовал здесь, несколько карикатурны и несколько идеализирован, поэтому на реальных данных установлены кривые, которые вы получаете, могут в конечном итоге выглядеть немного более грязным и просто немного более шумным, чем это. Для некоторых наборов данных вы действительно увидите их для видов трендов и , глядя на график ошибки перекрестной проверки, вы можете либо вручную, автоматически попытаться выбрать точку, которая минимизирует ошибку перекрестной проверки, и выбрать значение лямбда, соответствующее низкому перекресту ошибка проверки. Когда я пытаюсь выбрать параметр регуляризации лямбда для алгоритма обучения, часто я нахожу, что построение такой фигуры, как эта показанная здесь, помогает мне понять лучше, что происходит, и помогает мне проверить, что я действительно выбираю хорошее значение для монитора параметров регуляризации. Итак, надеюсь, это дает вам больше понимания регуляризации и это влияние на смещение и дисперсию алгоритма обучения. К настоящему времени вы видели предвзятость и дисперсию с разных точек зрения. И то, что мы хотели бы сделать в следующем видео, это взять все идеи, через которые мы прошли , и построить на них диагностику, которая называется обучением кривых. Это инструмент, который я часто использую для диагностики, может ли алгоритм обучения страдает от проблемы смещения или дисперсии проблемы, или немного обоих.