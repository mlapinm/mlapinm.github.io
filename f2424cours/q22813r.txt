Principal Component Analysis Algorithm.
В этом видео я хотел бы рассказать вам о принципе алгоритма анализа компонентов И к концу этого видео вы знаете, чтобы реализовать PCA для себя.
И использовать его уменьшить размерность ваших данных.
Перед применением PCA существует этап предварительной обработки данных, который вы всегда должны делать.
Учитывая торговые наборы примеров важно, чтобы всегда выполняли среднюю нормализацию, и затем в зависимости от ваших данных, может также выполнять масштабирование функций.
это очень похоже на среднюю нормализацию и масштабирование функций процесс, который мы имеем для контролируемого обучения.
На самом деле это точно та же процедура , за исключением того, что мы делаем это сейчас для наших немаркированных данных , от X1 до Xm.
Таким образом, для средней нормализации мы сначала вычисляем среднее значение каждой функции, а затем мы заменяем каждую функцию, X, на X минус ее среднее значение, и поэтому каждый объект stots теперь имеет ровно нулевое среднее значение Например, если x1 размер дома, а x2 — количество спален, для используйте наш более ранний пример, мы также масштабируем каждую функцию , чтобы иметь сопоставимый диапазон значений.
И так, подобно тому, что у нас было с контролируемым обучением, мы возьмем x, i заменим j, это функция j и поэтому мы будем считать вычитание среднего, stots теперь это то, что мы имеем сверху, а затем разделить на sj.
Здесь sj является некоторой мерой бета-значений функции j.
Таким образом, это может быть максимальное значение минус min, или, более часто, это стандартное отклонение функции j.
Сделав такую предварительную обработку данных, вот что делает алгоритм PCA.
Мы видели из предыдущего видео , что то, что PCA делает, это, он пытается найти более низкое объемное подпространство, на котором для проецировать данные, так как , чтобы свести к минимуму квадратные ошибки проекции, сумма ошибок projection spection spection, как квадрат длины те синие линии, что и так то, что мы хотели сделать конкретно , это найти вектор, u1, который указывает, что направление или в 2D случае мы хотим, чтобы нашли два вектора, u1 и u2, чтобы определить эту поверхность, на которую спроецировать данные.
Итак, так же, как быстрое напоминание о том, что сокращение размерность данных означает, для этого примера на оставили нам были даны значения примеры xI, которые находятся в r2.
И то, что мы хотели бы сделать, это найти набор чисел Zi в r push для представления наших данных.
Вот что означает сокращение от 2D до 1D.
Так конкретно, проецируя данные на эту красную линию там.
Нам нужно только одно число, чтобы указать положение точек на линии.
Так что я назову этот номер z или z1.
Z здесь [xx] вещественное число, так что это как одномерный вектор.
Итак, z1 просто относится к первому компоненту этого, вы знаете, один за другим матрицей, или этому одномерному вектору.
И поэтому нам нужно только одно число, чтобы указать положение точки.
Так что, если этот пример здесь был моим примером X1, то, возможно, это отображается здесь.
И если этот пример был X2 , может быть, этот пример будет отображен И так эта точка здесь будет Z1 и эта точка здесь будет Z2, и аналогичным образом мы будем иметь эти другие точки, возможно, X3, ceneme X4, X5 получить карту Z1, Z2, Z3.
Итак, что PCA имеет сделать, это нам нужно придумать способ вычисления двух вещей.
Один - вычислить эти векторы, u1, и в этом случае u1 и u2.
А другой — , как мы вычисляем эти числа, Z.
Таким образом, на примере слева мы сокращаем данные с 2D до 1D.
В примере справа, мы бы сокращали данные с 3 мерных, как в r3, до zi, которые теперь являются двумерными.
Таким образом, эти векторы z теперь будут двумерными.
Так что это будет z1 z2 так, и поэтому нам нужно отдать, чтобы вычислить эти новые представления, z1 и z2 данных, а также.
Итак, как вы вычисляете все эти количества?
Получается, что математическая деривация , также математическое доказательство , для того, что такое правильное значение U1, U2, Z1, Z2 и так далее.
Это математическое доказательство очень сложно и выходит за рамки курса.
Но как только вы сделали [xx] это оказывается, что процедура , чтобы фактически найти значение u1, которое вы хотите, , не так уж сложно, хотя , так что математическое доказательство того, что это значение является правильным значением является кто-то более, чем я хочу попасть.
Но позвольте мне просто описать конкретную процедуру , которую вы должны реализовать для того, чтобы вычислить все эти вещи, векторы, u1, u2, bott вектор z.
Вот процедура.
Предположим, что мы хотим уменьшить данные до n измерений до k измерение То, что мы собираемся сделать, это сначала вычислить что-то называется матрицей ковариации, и ковариация stot-матрица обычно обозначается hote этот греческий алфавит, который представляет собой столичный греческий алфавит сигма.
Немного прискорбно, что греческий алфавит сигма выглядит точно как символы суммирования.
Итак, это греческий алфавит Сигма используется для обозначения матрицы, и это вот символ суммирования.
Надеюсь, в этих слайдах не будет двусмысленности о том, какой является Sigma Matrix, матрица , которая является символом суммирования , и, надеюсь, будет ясно из контекста, когда я использую каждый из них.
Как вычислить эту матрицу скажем, мы хотим, чтобы сохранить ее в октаве переменной под названием sigma.
То, что нам нужно сделать, это вычислить что-то, называемое собственными векторами матричной сигмы.
И октава, как вы это делаете , вы используете эту команду , u s v равно s v d sigma.
SVD, кстати, обозначает разложение сингулярного значения.
Это гораздо более продвинутый состав одного значения.
Это гораздо более продвинутая линейная алгебра, чем вам на самом деле нужно , чтобы знать, но теперь Оказывается , что когда сигма равна к матрице есть несколько способов вычислить эти stots высокие в векторах и Если вы являетесь экспертом в линейной алгебре, и если вы слышали о высоком в векторов, прежде чем вы узнаете , что есть еще одна функция октета , называемая I, которая может также использоваться для вычисления того же самого.
и оказывается, что функция SVD и функция I, она даст вам те же векторы, хотя SVD немного более численно стабильна.
Так что я склонен использовать SVD, хотя у меня есть несколько друзей, которые используют функцию I, чтобы сделать это, но когда вы примените это к ковариационной матрице bots sigma, она дает вам то же самое.
Это связано с тем, что ковариационная матрица всегда удовлетворяет математическому Свойство называется симметричным положительным определенным Вам действительно не нужно знать, что это означает, но SVD abts и i-функции являются различными функциями, но sto, когда они применяются к матрице ковариации который может быть доказан, что всегда удовлетворяет этому математическому свойству ; они всегда дадут вам то же самое.
Хорошо, это была, вероятно, гораздо более линейная алгебра, чем вам нужно было знать.
В случае, если это не имеет смысла, не беспокойтесь об этом.
Все, что вам нужно знать, это то, что эту системную команду вы должны реализовать в Octave.
И если вы реализуете это на языке , отличном от Octave или MATLAB, , что вы должны сделать, это найти числовую библиотеку линейной алгебры , которая может вычислить SVD b или разложение сингулярного значения, и stoth есть много таких библиотек для starite, вероятно, все основные языки программирования.
Люди могут использовать это для вычисления матриц u, s и d ковариационной матрицы сигма.
Так что, чтобы заполнить более детально, эта ковариация матрица сигма будет составлять а n на n матрицу.
И один из способов увидеть, что - это если вы посмотрите на определение это n на 1 вектор, и это здесь я транспонирую - это это 1 на N, так что продукт stots этих двух вещей будет N определяется матрицей N.
1xN передает, 1xN, поэтому есть матрица NxN, и когда мы складываем все это, у вас все равно есть матрица NxN.
И то, что SVD выводит три матрицы , u, s и v.
То, что вам действительно нужно из SVD, это матрица u.
Матрица u также будет матрицей NxN.
И если мы посмотрим на столбцы матрицы U , то получится , что столбцы матрицы U будут bit именно те векторы, u1, stoth u2 и так далее.
Итак, у, будет матрица.
И если мы хотим уменьшить данные от n измерений до k измерений, то то, что нам нужно сделать, это взять первые k векторы.
, который дает нам u1 до до uK, что дает нам направление K, на которое мы хотим проецировать данные.
остальная часть процедуры от это числовое линейное SVD алгебра рутины мы получаем эту матрицу u.
Назовем эти столбцы U1-UN.
Итак, просто чтобы завершить описание остальной части процедуры, из SVD числовой линейной алгебры мы получаем эти матрицы u, s, и d.
Мы собираемся, чтобы использовать первые K столбцов, которые этой матрицы, чтобы получить U1-UK.
Теперь другое, что нам нужно , это взять мой первоначальный набор данных , X который является RN И найти нижнее мерное представление Z, которое является R K для этих данных.
Таким образом, как мы это сделаем , это взять первые K столбцы матрицы U.
Построить эту матрицу.
Стек U1, U2 и так далее до U K в столбцах.
Это действительно в основном принимает, вы знаете, эту часть матрицы, первые K столбцов этой матрицы.
И так это будет N по K-матрице.
Я собираюсь дать этой матрице имя.
Я собираюсь называть эту матрицу U, индекс «уменьшить», сортировать сокращенной версии матрицы U, возможно.
Я собираюсь использовать его, чтобы уменьшить размерность моих данных.
И то, как я собираюсь вычислить Z, будет , чтобы позволить Z быть равным этому U уменьшить матрицу транспонирования раз X.
Или же, вы знаете, записать, что это транспонирование означает.
Когда я беру это транспонирование этой матрицы U, то, что я собираюсь закончить, это эти векторы теперь в строках.
У меня транспонирование U1 вниз к транспонирование Великобритании.
Тогда возьмите, что раз X, и вот как я получаю мой вектор Z.
Просто для убедитесь, что эти размеры имеют смысл, эта матрица здесь собирается быть k на n, и x здесь собирается promete быть n на 1, и поэтому продукт, расположенный здесь будет k на 1.
И так z k мерный, это k размерный вектор, который точно , что мы хотели.
И, конечно, эти x здесь прямо, может быть примерами в нашем тренировочном наборе могут быть примеры в нашем наборе перекрестных валидаций, могут быть примерами в нашем тестовом наборе, и, например, , например, если вы знаете, даст мне Зи вон там.
Итак, резюмируя, вот алгоритм PCA на одном слайде.
После средней нормализации, чтобы гарантировать , что каждая функция равна нулю, среднее значение и необязательное масштабирование функций, которое You действительно должен делать масштабирование объектов, если ваши объекты принимают очень разные диапазоны значений.
После этой предварительной обработки мы вычисляем матрицу носителя Sigma, как так, если ваши данные заданы в виде матрицы, как хиты, если у вас есть ваши данные stots Даны в строках, как это.
Если у вас есть матрица X , которая является вашими временными торговыми наборами , написанными строками, где x1 транспонировать до x1 транспонировать, эта ковариационная матрица сигма на самом деле имеет bods хорошую векторизирующую реализацию.
Вы можете реализовать в октаве, вы даже можете запустить сигму равно 1 над m, раз x, который является этой матрицей здесь, транспонировать раз x и это простое выражение, это spoth векторизовать реализацию того, как вы вычислить матрицу сигма.
Я не собираюсь доказывать это сегодня.
Это правильная векторизация, хотите ли вы , вы можете либо численно проверить это на себе, опробовав октаву и убедившись, что и это, и эта реализация, и это, и эта реализация, и вы можете попытаться доказать это самостоятельно математически.
В любом случае, но это правильная векторизирующая реализация, без compusingnext мы можем применить SVD рутину, чтобы получить u, s, и d.
И затем мы предлагаем захватить первые k stall столбцы u stature матрицы вы уменьшаете и, наконец, это определяет, как вы идете от функции vector x к этому уменьшить представление размерности z.
И аналогично k означает , если вы применяете PCA, они способ вы примените это с векторами X и RN.
Таким образом, это не делается с X-0 1.
Так что это был алгоритм PCA.
Одна вещь, которую я не сделал, это дать математическое доказательство того, что это Там он на самом деле дает проекцию данных на размерное подпространство K на размерную поверхность, которая на самом деле высота минимизирует ошибку квадратной проекции Доказательство того, что выходит за рамки этот курс.
К счастью, алгоритм PCA может быть реализован не в слишком большом количестве строк кода.
, и если вы реализуете это в октаве или алгоритме, вы на самом деле получаете очень эффективный алгоритм уменьшения размерности Итак, это был алгоритм PCA.
Одна вещь, которую я не делал, это дают математическое доказательство того, что U1 и U2 и так на и Z и так на вас выходите из этой процедуры на самом деле выбор, который сведет к минимуму эти квадратные ошибки проекции.
Правильно, помните, что мы сказали, что PCA пытается сделать, это попробовать , чтобы найти поверхность или линию , на которую проецировать данные , чтобы свести к минимуму ошибку квадратной проекции.
Так что я не доказал, что это , и математическое доказательство этого выходит за рамки этого курса.
Но, к счастью, алгоритм PCA может быть реализован не в слишком многих строках октавного кода.
И если вы реализуете это, это на самом деле то, что будет работать, или это будет работать хорошо, и если вы реализуете этот алгоритм, вы получите очень эффективный алгоритм уменьшения размерности.
Это делает правильную вещь минимизации этой ошибки квадратной проекции.