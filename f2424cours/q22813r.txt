Principal Component Analysis Algorithm.В этом видео я хотел бы рассказать вам о принципе алгоритма анализа компонентов
Play video starting at ::5 and follow transcript0:05
И к концу этого видео вы знаете, чтобы реализовать PCA для себя. И использовать его уменьшить размерность ваших данных. Перед применением PCA существует этап предварительной обработки данных, который вы всегда должны делать. Учитывая торговые наборы примеров важно, чтобы всегда выполняли среднюю нормализацию,
Play video starting at ::25 and follow transcript0:25
и затем в зависимости от ваших данных, может также выполнять масштабирование функций.
Play video starting at ::29 and follow transcript0:29
это очень похоже на среднюю нормализацию и масштабирование функций процесс, который мы имеем для контролируемого обучения. На самом деле это точно та же процедура , за исключением того, что мы делаем это сейчас для наших немаркированных данных
Play video starting at ::42 and follow transcript0:42
, от X1 до Xm. Таким образом, для средней нормализации мы сначала вычисляем среднее значение каждой функции, а затем мы заменяем каждую функцию, X, на X минус ее среднее значение, и поэтому каждый объект stots теперь имеет ровно нулевое среднее значение Например, если x1 размер дома, а x2 — количество спален, для используйте наш более ранний пример, мы также масштабируем каждую функцию , чтобы иметь сопоставимый диапазон значений. И так, подобно тому, что у нас было с контролируемым обучением, мы возьмем x, i заменим j, это функция j
Play video starting at :1:23 and follow transcript1:23
и поэтому мы будем считать вычитание среднего, stots теперь это то, что мы имеем сверху, а затем разделить на sj. Здесь sj является некоторой мерой бета-значений функции j. Таким образом, это может быть максимальное значение минус min, или, более часто, это стандартное отклонение функции j. Сделав такую предварительную обработку данных, вот что делает алгоритм PCA.
Play video starting at :1:40 and follow transcript1:40
Мы видели из предыдущего видео , что то, что PCA делает, это, он пытается найти более низкое объемное подпространство, на котором для проецировать данные, так как , чтобы свести к минимуму квадратные ошибки проекции, сумма ошибок projection spection spection, как квадрат длины те синие линии, что и так то, что мы хотели сделать конкретно , это найти вектор, u1, который указывает, что направление или в 2D случае мы хотим, чтобы нашли два вектора, u1 и
Play video starting at :2:10 and follow transcript2:10
u2, чтобы определить эту поверхность, на которую спроецировать данные.
Play video starting at :2:16 and follow transcript2:16
Итак, так же, как быстрое напоминание о том, что сокращение размерность данных означает, для этого примера на оставили нам были даны значения примеры xI, которые находятся в r2. И то, что мы хотели бы сделать, это найти набор чисел Zi в r push для представления наших данных.
Play video starting at :2:36 and follow transcript2:36
Вот что означает сокращение от 2D до 1D.
Play video starting at :2:39 and follow transcript2:39
Так конкретно, проецируя данные
Play video starting at :2:42 and follow transcript2:42
на эту красную линию там. Нам нужно только одно число, чтобы указать положение точек на линии. Так что я назову этот номер
Play video starting at :2:50 and follow transcript2:50
z или z1. Z здесь [xx] вещественное число, так что это как одномерный вектор. Итак, z1 просто относится к первому компоненту этого, вы знаете, один за другим матрицей, или этому одномерному вектору.
Play video starting at :3:1 and follow transcript3:01
И поэтому нам нужно только одно число, чтобы указать положение точки. Так что, если этот пример здесь был моим примером
Play video starting at :3:10 and follow transcript3:10
X1, то, возможно, это отображается здесь. И если этот пример был X2 , может быть, этот пример будет отображен И так эта точка здесь будет Z1 и эта точка здесь будет Z2, и аналогичным образом мы будем иметь эти другие точки, возможно, X3, ceneme X4, X5 получить карту Z1, Z2, Z3.
Play video starting at :3:34 and follow transcript3:34
Итак, что PCA имеет сделать, это нам нужно придумать способ вычисления двух вещей. Один - вычислить эти векторы,
Play video starting at :3:41 and follow transcript3:41
u1, и в этом случае u1 и u2. А другой — , как мы вычисляем эти числа,
Play video starting at :3:49 and follow transcript3:49
Z. Таким образом, на примере слева мы сокращаем данные с 2D до 1D.
Play video starting at :3:55 and follow transcript3:55
В примере справа, мы бы сокращали данные с 3 мерных, как в r3, до zi, которые теперь являются двумерными. Таким образом, эти векторы z теперь будут двумерными. Так что это будет z1 z2 так, и поэтому нам нужно отдать, чтобы вычислить эти новые представления, z1 и z2 данных, а также. Итак, как вы вычисляете все эти количества? Получается, что математическая деривация , также математическое доказательство , для того, что такое правильное значение U1, U2, Z1, Z2 и так далее. Это математическое доказательство очень сложно и выходит за рамки курса. Но как только вы сделали [xx] это оказывается, что процедура , чтобы фактически найти значение u1, которое вы хотите, , не так уж сложно, хотя , так что математическое доказательство того, что это значение является правильным значением является кто-то более, чем я хочу попасть. Но позвольте мне просто описать конкретную процедуру , которую вы должны реализовать для того, чтобы вычислить все эти вещи, векторы, u1, u2, bott вектор z. Вот процедура.
Play video starting at :5:2 and follow transcript5:02
Предположим, что мы хотим уменьшить данные до n измерений до k измерение То, что мы собираемся сделать, это сначала
Play video starting at :5:6 and follow transcript5:06
вычислить что-то называется матрицей ковариации, и ковариация stot-матрица обычно обозначается hote этот греческий алфавит, который представляет собой столичный греческий алфавит сигма.
Play video starting at :5:18 and follow transcript5:18
Немного прискорбно, что греческий алфавит сигма выглядит точно как символы суммирования. Итак, это греческий алфавит Сигма используется для обозначения матрицы, и это вот символ суммирования. Надеюсь, в этих слайдах не будет двусмысленности о том, какой является Sigma Matrix, матрица , которая является символом суммирования , и, надеюсь, будет ясно из контекста, когда я использую каждый из них. Как вычислить эту матрицу скажем, мы хотим, чтобы сохранить ее в октаве переменной под названием sigma. То, что нам нужно сделать, это вычислить что-то, называемое собственными векторами матричной сигмы.
Play video starting at :5:57 and follow transcript5:57
И октава, как вы это делаете , вы используете эту команду , u s v равно s v d sigma.
Play video starting at :6:3 and follow transcript6:03
SVD, кстати, обозначает разложение сингулярного значения.
Play video starting at :6:8 and follow transcript6:08
Это гораздо более продвинутый состав одного значения.
Play video starting at :6:14 and follow transcript6:14
Это гораздо более продвинутая линейная алгебра, чем вам на самом деле нужно , чтобы знать, но теперь Оказывается , что когда сигма равна к матрице есть несколько способов вычислить эти stots высокие в векторах и Если вы являетесь экспертом в линейной алгебре, и если вы слышали о высоком в векторов, прежде чем вы узнаете , что есть еще одна функция октета , называемая I, которая может также использоваться для вычисления того же самого. и оказывается, что функция SVD и функция I, она даст вам те же векторы, хотя SVD немного более численно стабильна. Так что я склонен использовать SVD, хотя у меня есть несколько друзей, которые используют функцию I, чтобы сделать это, но когда вы примените это к ковариационной матрице bots sigma, она дает вам то же самое. Это связано с тем, что ковариационная матрица всегда удовлетворяет математическому Свойство называется симметричным положительным определенным Вам действительно не нужно знать, что это означает, но SVD abts и i-функции являются различными функциями, но sto, когда они применяются к матрице ковариации который может быть доказан, что всегда удовлетворяет этому математическому свойству
Play video starting at :7:13 and follow transcript7:13
; они всегда дадут вам то же самое.
Play video starting at :7:16 and follow transcript7:16
Хорошо, это была, вероятно, гораздо более линейная алгебра, чем вам нужно было знать. В случае, если это не имеет смысла, не беспокойтесь об этом. Все, что вам нужно знать, это то, что эту системную команду вы должны реализовать в Octave. И если вы реализуете это на языке , отличном от Octave или MATLAB, , что вы должны сделать, это найти числовую библиотеку линейной алгебры , которая может вычислить SVD b или разложение сингулярного значения, и stoth есть много таких библиотек для starite, вероятно, все основные языки программирования. Люди могут использовать это для вычисления матриц u, s и d ковариационной матрицы сигма. Так что, чтобы заполнить более детально, эта ковариация матрица сигма будет составлять а n на n матрицу. И один из способов увидеть, что - это если вы посмотрите на определение
Play video starting at :8:5 and follow transcript8:05
это n на 1 вектор, и это здесь я транспонирую - это это 1 на N, так что продукт stots этих двух вещей будет N определяется матрицей N.
Play video starting at :8:19 and follow transcript8:19
1xN передает, 1xN, поэтому есть матрица NxN, и когда мы складываем все это, у вас все равно есть матрица NxN.
Play video starting at :8:27 and follow transcript8:27
И то, что SVD выводит три матрицы , u, s и v. То, что вам действительно нужно из SVD, это матрица u.
Play video starting at :8:36 and follow transcript8:36
Матрица u также будет матрицей NxN.
Play video starting at :8:41 and follow transcript8:41
И если мы посмотрим на столбцы матрицы U , то получится , что столбцы
Play video starting at :8:48 and follow transcript8:48
матрицы U будут bit именно те векторы, u1, stoth u2 и так далее.
Play video starting at :8:57 and follow transcript8:57
Итак, у, будет матрица.
Play video starting at :9: and follow transcript9:00
И если мы хотим уменьшить данные от n измерений до k измерений, то то, что нам нужно сделать, это взять первые k векторы.
Play video starting at :9:9 and follow transcript9:09
, который дает нам u1 до до uK, что дает нам направление K, на которое мы хотим проецировать данные. остальная часть процедуры от это числовое линейное SVD алгебра рутины мы получаем эту матрицу u. Назовем эти столбцы U1-UN.
Play video starting at :9:30 and follow transcript9:30
Итак, просто чтобы завершить описание остальной части процедуры, из SVD числовой линейной алгебры мы получаем эти матрицы u, s, и d. Мы собираемся, чтобы использовать первые K столбцов, которые этой матрицы, чтобы получить U1-UK.
Play video starting at :9:48 and follow transcript9:48
Теперь другое, что нам нужно , это взять мой первоначальный набор данных , X который является RN И найти нижнее мерное представление Z, которое является R K для этих данных. Таким образом, как мы это сделаем , это взять первые K столбцы матрицы U.
Play video starting at :10:8 and follow transcript10:08
Построить эту матрицу.
Play video starting at :10:11 and follow transcript10:11
Стек U1, U2 и
Play video starting at :10:14 and follow transcript10:14
так далее до U K в столбцах. Это действительно в основном принимает, вы знаете, эту часть матрицы, первые K столбцов этой матрицы.
Play video starting at :10:23 and follow transcript10:23
И так это будет N по K-матрице. Я собираюсь дать этой матрице имя. Я собираюсь называть эту матрицу U, индекс «уменьшить», сортировать сокращенной версии матрицы U, возможно. Я собираюсь использовать его, чтобы уменьшить размерность моих данных.
Play video starting at :10:43 and follow transcript10:43
И то, как я собираюсь вычислить Z, будет , чтобы позволить Z быть равным этому U уменьшить матрицу транспонирования раз X. Или же, вы знаете, записать, что это транспонирование означает. Когда я беру это транспонирование этой матрицы U, то, что я собираюсь закончить, это эти векторы теперь в строках. У меня транспонирование U1 вниз к транспонирование Великобритании.
Play video starting at :11:7 and follow transcript11:07
Тогда возьмите, что раз X, и вот как я получаю мой вектор Z. Просто для убедитесь, что эти размеры имеют смысл,
Play video starting at :11:15 and follow transcript11:15
эта матрица здесь собирается быть k на n, и x здесь собирается promete быть n на 1, и поэтому продукт, расположенный здесь будет k на 1. И так z k мерный, это k размерный вектор, который точно , что мы хотели. И, конечно, эти x здесь прямо, может быть примерами в нашем тренировочном наборе могут быть примеры в нашем наборе перекрестных валидаций, могут быть примерами в нашем тестовом наборе, и, например, , например, если вы знаете, даст мне Зи вон там. Итак, резюмируя, вот алгоритм PCA на одном слайде.
Play video starting at :11:56 and follow transcript11:56
После средней нормализации, чтобы гарантировать , что каждая функция равна нулю, среднее значение и необязательное масштабирование функций, которое You действительно должен делать масштабирование объектов, если ваши объекты принимают очень разные диапазоны значений. После этой предварительной обработки мы вычисляем матрицу носителя Sigma, как так, если ваши данные заданы в виде матрицы, как хиты, если у вас есть ваши данные stots Даны в строках, как это. Если у вас есть матрица X , которая является вашими временными торговыми наборами , написанными строками, где x1 транспонировать до x1 транспонировать,
Play video starting at :12:31 and follow transcript12:31
эта ковариационная матрица сигма на самом деле имеет bods хорошую векторизирующую реализацию.
Play video starting at :12:37 and follow transcript12:37
Вы можете реализовать в октаве, вы даже можете запустить сигму равно 1 над m, раз x, который является этой матрицей здесь, транспонировать раз x и это простое выражение, это spoth векторизовать реализацию того, как вы вычислить матрицу сигма.
Play video starting at :12:58 and follow transcript12:58
Я не собираюсь доказывать это сегодня. Это правильная векторизация, хотите ли вы , вы можете либо численно проверить это на себе, опробовав октаву и убедившись, что и это, и эта реализация, и это, и эта реализация, и вы можете попытаться доказать это самостоятельно математически.
Play video starting at :13:11 and follow transcript13:11
В любом случае, но это правильная векторизирующая реализация, без compusingnext
Play video starting at :13:16 and follow transcript13:16
мы можем применить SVD рутину, чтобы получить u, s, и d. И затем мы предлагаем захватить первые k stall столбцы u stature матрицы вы уменьшаете и, наконец, это определяет, как вы идете от функции vector x к этому уменьшить представление размерности z. И аналогично k означает , если вы применяете PCA, они способ вы примените это с векторами X и RN. Таким образом, это не делается с X-0 1. Так что это был алгоритм PCA.
Play video starting at :13:50 and follow transcript13:50
Одна вещь, которую я не сделал, это дать математическое доказательство того, что это Там он на самом деле дает проекцию данных на размерное подпространство K на размерную поверхность, которая на самом деле высота минимизирует ошибку квадратной проекции Доказательство того, что выходит за рамки этот курс. К счастью, алгоритм PCA может быть реализован не в слишком большом количестве строк кода. , и если вы реализуете это в октаве или алгоритме, вы на самом деле получаете очень эффективный алгоритм уменьшения размерности
Play video starting at :14:22 and follow transcript14:22
Итак, это был алгоритм PCA.
Play video starting at :14:25 and follow transcript14:25
Одна вещь, которую я не делал, это дают математическое доказательство того, что U1 и U2 и так на и Z и так на вас выходите из этой процедуры на самом деле выбор, который сведет к минимуму эти квадратные ошибки проекции. Правильно, помните, что мы сказали, что PCA пытается сделать, это попробовать , чтобы найти поверхность или линию , на которую проецировать данные , чтобы свести к минимуму ошибку квадратной проекции. Так что я не доказал, что это , и математическое доказательство этого выходит за рамки этого курса. Но, к счастью, алгоритм PCA может быть реализован не в слишком многих строках октавного кода. И если вы реализуете это, это на самом деле то, что будет работать, или это будет работать хорошо, и если вы реализуете этот алгоритм, вы получите очень эффективный алгоритм уменьшения размерности. Это делает правильную вещь минимизации этой ошибки квадратной проекции.