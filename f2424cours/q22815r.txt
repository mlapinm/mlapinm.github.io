Choosing the Number of Principal Components.В алгоритме PCA мы берем N размерные объекты и уменьшаем их до некоторого K-представления пространственного объекта.
Play video starting at ::7 and follow transcript0:07
Это число K является параметром алгоритма PCA.
Play video starting at ::11 and follow transcript0:11
Это число K также называется количество основных компонентов или количество основных компонентов, которые мы сохранили.
Play video starting at ::18 and follow transcript0:18
И в этом видео я хотел бы дать вам некоторые рекомендации, рассказать вам о том, как люди склонны думать о том, как выбрать этот параметр K для PCA.
Play video starting at ::28 and follow transcript0:28
Для того, чтобы выбрать k, , то есть количество главных компонентов, вот несколько полезных понятий.
Play video starting at ::36 and follow transcript0:36
То, что PCA пытается сделать , это пытается свести к минимуму
Play video starting at ::40 and follow transcript0:40
среднюю ошибку проекции в квадрате. Таким образом, он пытается свести к минимуму это количество, которое я записываю, , которое является разницей между исходными данными X и прогнозируемой версией , X-Approx-i, которая была определена в последнем видео, поэтому он пытается свести к минимуму квадратное расстояние между x и его проекция на эту нижнюю размерную поверхность.
Play video starting at :1:1 and follow transcript1:01
Итак, это средняя квадратная ошибка проекции.
Play video starting at :1:3 and follow transcript1:03
Также позвольте мне определить общую вариацию в данных как среднюю длину в квадрате из этих примеров Xi , так что общая вариация в данных stots - это среднее количество моих тренировочных наборов по всей длине каждого из моих обучающих примеров. А это говорит: «В среднем, как далеко мои обучающие примеры из вектора, от того, чтобы быть всеми нулями?» Как далеко, как далеко в среднем мои обучающие примеры от происхождения? Когда мы пытаемся выбрать k,
Play video starting at :1:35 and follow transcript1:35
довольно распространенное правило большого пальца для выбора k состоит в том, чтобы выбрать меньшие значения, чтобы соотношение между ними меньше 0,01. Другими словами, довольно распространенный способ подумать о том, как мы выбираем k , мы хотим среднеквадратичную ошибку проекции. Это среднее расстояние между x и его проекциями
Play video starting at :1:57 and follow transcript1:57
, деленное на общую вариацию данных. Это то, насколько различаются данные.
Play video starting at :2:2 and follow transcript2:02
Мы хотим, чтобы это соотношение было меньше, скажем, 0,01. Или быть менее 1%, что является еще одним способом думать об этом.
Play video starting at :2:10 and follow transcript2:10
И то, как большинство людей думают о выборе K, скорее , чем выбрать K непосредственно, как большинство людей говорят о , это как то, что это номер bit, будь то 0.01, или какое-то другое число. И если он равен 0,01, другой способ сказать это, чтобы использовать язык PCA, заключается в том, что 99% дисперсии сохраняется.
Play video starting at :2:32 and follow transcript2:32
Я не очень хочу, не беспокойтесь о том, что эта фраза действительно технически, но эта фраза "99% дисперсии сохраняется» просто означает, что это количество слева меньше 0,01. И так, если вы
Play video starting at :2:44 and follow transcript2:44
используете PCA, и если вы хотите рассказать кому-то, знаете, , сколько основных компонентов у вас есть , было бы, более распространено, чтобы сказать хорошо, я stots выбрал k, так что 99% дисперсии было сохранено. И это своего рода полезная вещь знать, это означает, что вы знаете, средняя квадратная ошибка проекции , деленная на общую вариацию , которая была не более 1%. Это своего рода проницательный вещь, о которой нужно думать, тогда как если вы скажите кому-то, что «Ну, я должен был 100 компонентов » или «k был равен объему 100 в тысяче размерных данных о stots», это немного трудно для людей, чтобы интерпретировать это. Таким образом, это число 0.01 является тем, что люди часто используют. Другие общие значения 0,05,
Play video starting at :3:26 and follow transcript3:26
и так это будет 5%, и если вы это сделаете, то вы идете и говорите, что 95% дисперсии это habt сохраняется и, вы знаете, что stots другие числа может быть 90% дисперсии является hote сохраняется, может быть, так же низко, как 85%. Так что 90% соответствовало бы сказать
Play video starting at :3:44 and follow transcript3:44
0.10, вроде 10%. И поэтому диапазон значений от, вы знаете, 90, 95, 99, может быть, до 85% от содержащиеся переменные будут довольно типичным диапазоном значений. Может быть, от 95 до 99 действительно самый общий диапазон ценностей, которые люди используют. Для многих наборов данных вы будете удивлены , чтобы сохранить 99% дисперсии, вы можете часто уменьшить размерность данных значительно и сохранить большую часть дисперсии. Потому что для большей части реальной жизни данные говорят, что многие функции просто сильно коррелированы, и поэтому оказывается возможным сжать данные bot-много и все еще сохранить вы stot-знаете 99% дисперсии care или 95% дисперсии. Итак, как вы реализуете это? Ну, вот один алгоритм, который вы можете использовать. Вы можете начать, если хочу выбрать значение k, мы можем начать с k равно 1. А потом мы бежим через PCA. Вы знаете, поэтому мы вычисляем, вы уменьшаем, вычисляем z1, z2, до zm. Вычислить все эти x1 ок. и так далее до xm ок. , а затем мы проверяем, сохраняется ли 99% дисперсии.
Play video starting at :4:47 and follow transcript4:47
Тогда мы хороши, и мы используем k равно 1. Но если это не то, что мы будем делать, мы в следующий раз попробуем K равно 2. И тогда мы снова пробежим всю эту процедуру и проверить, вы знаете, это выражение удовлетворено. Это меньше, чем 0,01. А если нет, то мы сделаем это снова. Давайте попробуем k равно 3, затем попробуем k равно 4, и так далее, пока, возможно, мы получим до k равно 17, и мы находим 99% от того, что данные были сохранены, а затем ты используем k равно 17, верно? Это один из способов выбрать наименьшее значение k, так что и 99% дисперсии сохраняется.
Play video starting at :5:22 and follow transcript5:22
Но, как вы можете себе представить, эта процедура кажется ужасно неэффективной
Play video starting at :5:26 and follow transcript5:26
мы пытаемся k равно одному, k равно двум, мы делаем все эти вычисления.
Play video starting at :5:29 and follow transcript5:29
К счастью, когда вы реализуете PCA на самом деле, в этот шаг, он на самом деле дает нам количество, которое делает его гораздо проще вычислить эти вещи, а также. В частности, когда вы вызываете SVD, чтобы получить эти матрицы u, s, и d, , когда вы вызываете usvd на матрице ковариации sigma, он также дает нам эту матрицу sts S и то, что будет представлять собой квадратную матрицу N на N матрицу на самом деле, диагональ. Итак, диагональные записи s один один, s два два, s три вниз s n n n собираются в быть единственными ненулевыми элементами both этой матрицы, и все от нуля диагонали будет равным нулю. Хорошо? Итак, те большие O, которые я рисую, под этим я подразумеваю, что все по диагонали этой матрицы, все эти записи будут нулями.
Play video starting at :6:22 and follow transcript6:22
Итак, что можно показать , и я не докажу это здесь, и оказывается, , что для заданного значения k, это количество где-то здесь можно вычислить гораздо проще. И это количество может быть вычислено как один минус сумма из i равно 1 через K из Sii, деленное на сумму от I равно 1 деление через N из Sii.
Play video starting at :6:53 and follow transcript6:53
Так что просто сказать, что это слова, или просто взять еще один представление о том, как объяснить это, если K равен 3 скажем, скажем.
Play video starting at :7: and follow transcript7:00
То, что мы собираемся сделать, чтобы вычислить числитель, это сумма от одного — я равен 1 до 3 из Sii, так что просто вычислить сумму этих первых трех элементов.
Play video starting at :7:9 and follow transcript7:09
Так вот числитель.
Play video starting at :7:10 and follow transcript7:10
А затем для знаменателя, ну это сумма всех этих диагональных записей.
Play video starting at :7:16 and follow transcript7:16
И один минус соотношение , что дает мне это количество здесь, что я кружил синим. Итак, то, что мы можем сделать , это просто проверить, если это меньше или равно 0,01. Или эквивалентно, мы можем проверить , если сумма от i равна 1 до k, s-i-i , разделенная на сумму от i равно 1 до n, s-i , если это больше или st равно 4,99, если вы хотите быть уверенным, что 99% дисперсии сохраняется.
Play video starting at :7:44 and follow transcript7:44
И поэтому то, что вы можете сделать , это просто медленно увеличить k, установить k равно одному, установить k равно два, установить k равно трем и так на, и просто проверить это количество, чтобы увидеть, что такое stots наименьшее значение k, которое гарантирует, что 99% дисперсии сохраняется.
Play video starting at :8: and follow transcript8:00
И если вы сделаете это , то вам нужно вызвать функцию SVD только один раз. Потому что это дает вам матрицу S, и как только вы имеете матрицу S, вы можете , а затем просто продолжать делать этот расчет, увеличивая значение K в числителе stots, и поэтому вам не нужно продолжать вызывать SVD над hero и снова, чтобы проверить различные значения K. Таким образом, эта процедура намного эффективнее , и это может позволить вам выбрать значение K, не требуя от запускать PCA с нуля и снова. Вы просто запускаете SVD один раз, это дает вам все эти диагональные числа, все эти числа S11, S22 вплоть до SNN, и затем вы можете просто знаете, изменить K в этом выражении, чтобы найти наименьшее значение K, так что sts, что 99% дисперсии сохраняется. Подводя итог, способ , который я часто использую, способ, который я часто выбираю K , когда я использую PCA для сжатия , я бы назвал SVD один раз в матрице ковариации, а затем я бы использовал эту формулу и photage выбирал наименьшее значение K, для которого это выражение удовлетворено.
Play video starting at :9:1 and follow transcript9:01
И кстати, даже если бы вы выбрали какое-то другое значение из K, даже если бы вы были , чтобы выбрать значение K вручную, вы знаете, может быть, у вас есть тысяча размерных данных, а я просто хочу выбрать K равно сто. Затем, если вы хотите объяснить другим, что вы только что сделали, хороший способ объяснить производительность вашей реализации PCA для , на самом деле взять это количество и вычислить, что это такое, и это будет рассказывать вам, какой процент дисперсии сохраняется. И если вы сообщаете это число, то вы знаете, люди, которые знакомы с PCA, и люди могут использовать это, чтобы получить хорошее представление о том, насколько хорошо ваше стомерное представление представляет собой stws, приближающее ваш исходный набор данных, потому что есть 99% дисперсии сохраняется. Это действительно мера вашей квадратной ошибки строительства, что соотношение составляет 0,01, просто дает людям хорошее интуитивное чувство о том, находит ли ваша реализация PCA хорошее приближение вашего исходного набора данных.
Play video starting at :9:58 and follow transcript9:58
Итак, надеюсь, что это дает вам эффективную процедуру для выбора число К. Для выбора какое измерение, чтобы уменьшить ваши данные о , и если вы применяете PCA к очень stoth высокомерные наборы данных, вы знаете, чтобы как stote тысячу размерных данных, очень часто, потому что наборы данных, как правило, имеют очень коррелированные функции , это просто свойство большинства наборов данных, которые вы видите,
Play video starting at :10:18 and follow transcript10:18
вы часто обнаруживаете, что PCA сможет сохранить девяносто девять заб% дисперсии или скажем, sts девяносто пять девяносто девять, некоторые занимают высокую долю дисперсии, даже при сжатии данных очень большим фактором.