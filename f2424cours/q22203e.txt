Gradient Descent in Practice I - Feature Scaling.In this video and in the video after this one, I wanna tell you about some of the practical tricks for making gradient descent work well. In this video, I want to tell you about an idea called feature skill.
Play video starting at ::11 and follow transcript0:11
Here's the idea. If you have a problem where you have multiple features, if you make sure that the features are on a similar scale, by which I mean make sure that the different features take on similar ranges of values,
Play video starting at ::24 and follow transcript0:24
then gradient descents can converge more quickly.
Play video starting at ::27 and follow transcript0:27
Concretely let's say you have a problem with two features where X1 is the size of house and takes on values between say zero to two thousand and two is the number of bedrooms, and maybe that takes on values between one and five. If you plot the contours of the cos function J of theta,
Play video starting at ::44 and follow transcript0:44
then the contours may look like this, where, let's see, J of theta is a function of parameters theta zero, theta one and theta two. I'm going to ignore theta zero, so let's about theta 0 and pretend as a function of only theta 1 and theta 2, but if x1 can take on them, you know, much larger range of values and x2 It turns out that the contours of the cause function J of theta
Play video starting at :1:9 and follow transcript1:09
can take on this very very skewed elliptical shape, except that with the so 2000 to 5 ratio, it can be even more secure. So, this is very, very tall and skinny ellipses, or these very tall skinny ovals, can form the contours of the cause function J of theta.
Play video starting at :1:29 and follow transcript1:29
And if you run gradient descents on this cos-function, your gradients may end up taking a long time and can oscillate back and forth and take a long time before it can finally find its way to the global minimum.
Play video starting at :1:47 and follow transcript1:47
In fact, you can imagine if these contours are exaggerated even more when you draw incredibly skinny, tall skinny contours,
Play video starting at :1:56 and follow transcript1:56
and it can be even more extreme than, then, gradient descent just have a much harder time taking it's way, meandering around, it can take a long time to find this way to the global minimum.
Play video starting at :2:12 and follow transcript2:12
In these settings, a useful thing to do is to scale the features.
Play video starting at :2:17 and follow transcript2:17
Concretely if you instead define the feature X one to be the size of the house divided by two thousand, and define X two to be maybe the number of bedrooms divided by five, then the count well as of the cost function J can become
Play video starting at :2:32 and follow transcript2:32
much more, much less skewed so the contours may look more like circles.
Play video starting at :2:38 and follow transcript2:38
And if you run gradient descent on a cost function like this, then gradient descent,
Play video starting at :2:44 and follow transcript2:44
you can show mathematically, you can find a much more direct path to the global minimum rather than taking a much more convoluted path where you're sort of trying to follow a much more complicated trajectory to get to the global minimum.
Play video starting at :2:57 and follow transcript2:57
So, by scaling the features so that there are, the consumer ranges of values. In this example, we end up with both features, X one and X two, between zero and one.
Play video starting at :3:9 and follow transcript3:09
You can wind up with an implementation of gradient descent. They can convert much faster.
Play video starting at :3:18 and follow transcript3:18
More generally, when we're performing feature scaling, what we often want to do is get every feature into approximately a -1 to +1 range and concretely, your feature x0 is always equal to 1. So, that's already in that range,
Play video starting at :3:34 and follow transcript3:34
but you may end up dividing other features by different numbers to get them to this range. The numbers -1 and +1 aren't too important. So, if you have a feature,
Play video starting at :3:44 and follow transcript3:44
x1 that winds up being between zero and three, that's not a problem. If you end up having a different feature that winds being between -2 and + 0.5, again, this is close enough to minus one and plus one that, you know, that's fine, and that's fine.
Play video starting at :4: and follow transcript4:00
It's only if you have a different feature, say X 3 that is between, that
Play video starting at :4:5 and follow transcript4:05
ranges from -100 tp +100 , then, this is a very different values than minus 1 and plus 1. So, this might be a less well-skilled feature and similarly, if your features take on a very, very small range of values so if X 4 takes on values between minus 0.0001 and positive 0.0001, then
Play video starting at :4:29 and follow transcript4:29
again this takes on a much smaller range of values than the minus one to plus one range. And again I would consider this feature poorly scaled.
Play video starting at :4:37 and follow transcript4:37
So you want the range of values, you know, can be bigger than plus or smaller than plus one, but just not much bigger, like plus 100 here, or too much smaller like 0.00 one over there. Different people have different rules of thumb. But the one that I use is that if a feature takes on the range of values from say minus three the plus 3 how you should think that should be just fine, but maybe it takes on much larger values than plus 3 or minus 3 unless not to worry and if it takes on values from say minus one-third to one-third.
Play video starting at :5:10 and follow transcript5:10
You know, I think that's fine too or 0 to one-third or minus one-third to 0. I guess that's typical range of value sector 0 okay. But it will take on a much tinier range of values like x4 here than gain on mine not to worry. So, the take-home message is don't worry if your features are not exactly on the same scale or exactly in the same range of values. But so long as they're all close enough to this gradient descent it should work okay. In addition to dividing by so that the maximum value when performing feature scaling sometimes people will also do what's called mean normalization. And what I mean by that is that you want to take a feature Xi and replace it with Xi minus new i
Play video starting at :5:52 and follow transcript5:52
to make your features have approximately 0 mean.
Play video starting at :5:56 and follow transcript5:56
And obviously we want to apply this to the future x zero, because the future x zero is always equal to one, so it cannot have an average value of zero.
Play video starting at :6:6 and follow transcript6:06
But it concretely for other features if the range of sizes of the house takes on values between 0 to 2000 and if you know, the average size of a house is equal to 1000 then you might
Play video starting at :6:21 and follow transcript6:21
use this formula.
Play video starting at :6:23 and follow transcript6:23
Size, set the feature X1 to the size minus the average value divided by 2000 and similarly, on average if your houses have one to five bedrooms and if
Play video starting at :6:39 and follow transcript6:39
on average a house has two bedrooms then you might use this formula to mean normalize your second feature x2.
Play video starting at :6:49 and follow transcript6:49
In both of these cases, you therefore wind up with features x1 and x2. They can take on values roughly between minus .5 and positive .5. Exactly not true - X2 can actually be slightly larger than .5 but, close enough. And the more general rule is that you might take a feature X1 and replace
Play video starting at :7:8 and follow transcript7:08
it with X1 minus mu1 over S1 where to define these terms mu1 is the average value of x1
Play video starting at :7:19 and follow transcript7:19
in the training sets
Play video starting at :7:22 and follow transcript7:22
and S1 is the range of values of that feature and by range, I mean let's say the maximum value minus the minimum value or for those of you that understand the deviation of the variable is setting S1 to be the standard deviation of the variable would be fine, too. But taking, you know, this max minus min would be fine.
Play video starting at :7:44 and follow transcript7:44
And similarly for the second feature, x2, you replace x2 with this sort of
Play video starting at :7:51 and follow transcript7:51
subtract the mean of the feature and divide it by the range of values meaning the max minus min. And this sort of formula will get your features, you know, maybe not exactly, but maybe roughly into these sorts of ranges, and by the way, for those of you that are being super careful technically if we're taking the range as max minus min this five here will actually become a four. So if max is 5 minus 1 then the range of their own values is actually equal to 4, but all of these are approximate and any value that gets the features into anything close to these sorts of ranges will do fine. And the feature scaling doesn't have to be too exact, in order to get gradient descent to run quite a lot faster.
Play video starting at :8:34 and follow transcript8:34
So, now you know about feature scaling and if you apply this simple trick, it and make gradient descent run much faster and converge in a lot fewer other iterations.
Play video starting at :8:44 and follow transcript8:44
That was feature scaling. In the next video, I'll tell you about another trick to make gradient descent work well in practice.