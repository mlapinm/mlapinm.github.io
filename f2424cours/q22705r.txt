Kernels II.В последнем видео мы начали , чтобы рассказать о идее ядра и о том, как ее можно использовать для определения новых функций для машины опорных векторов. В этом видео я хотел бы бросить в некоторые недостающие детали и, также сказать несколько слов о , как использовать эти идеи на практике. Например, как они относятся к , например, к компромиссу дисперсии смещения в машинах опорных векторов.
Play video starting at ::22 and follow transcript0:22
В последнем видео я говорил о процессе выбора нескольких ориентиров. Вы знаете, l1, l2, l3 и что позволили нам определить функцию подобия , также называемую ядром или в этом примере , если у вас есть bet эта функция подобия это гауссовое ядро.
Play video starting at ::38 and follow transcript0:38
И это позволило нам построить эту форму функции гипотезы.
Play video starting at ::43 and follow transcript0:43
Но откуда мы получаем эти ориентиры? Откуда мы получаем l1, l2, l3? И кажется также, что для сложных задач обучения , может быть, мы хотим, чтобы больше ориентиров, чем только три из них, которые мы могли бы выбрать вручную.
Play video starting at ::55 and follow transcript0:55
Таким образом, на практике это , как ориентиры выбираются что является тем, что учитывая проблему машинного обучения . У нас есть набор данных из некоторых положительных и отрицательных примеров. Итак, это идея здесь , которая заключается в том, что мы возьмем примеры и для каждого примера обучения, который у нас есть, мы просто назовем это. Мы просто собираемся , чтобы поставить ориентиры точно те же места, что и примеры обучения.
Play video starting at :1:18 and follow transcript1:18
Так что, если у меня есть один пример обучения , если это x1, хорошо, тогда я собираюсь , чтобы выбрать, это мой первый ориентир , чтобы быть в той же точке, что и мой первый пример обучения.
Play video starting at :1:29 and follow transcript1:29
И если у меня есть другое обучение пример x2. Ну, мы собираемся установить второй ориентир
Play video starting at :1:35 and follow transcript1:35
, чтобы быть местом моего второго учебного примера.
Play video starting at :1:38 and follow transcript1:38
На рисунке справа, я использовал красные и синие точки так же, как иллюстрация, цвет этой фигуры, цвет точки на фигуре справа не значителен.
Play video starting at :1:47 and follow transcript1:47
Но то, что я собираюсь закончить с использованием этого метода, это я собираюсь закончить с m ориентиры l1, l2
Play video starting at :1:54 and follow transcript1:54
до l (m), если у меня есть m обучающих примеров с sto один ориентир на местоположение моего каждого местоположения из моих обучающих примеров. И это приятно, потому что он говорит, что мои функции в основном собираются , чтобы измерить, насколько близко пример к одному из вещей, которые я видел в моем тренировочном наборе. Итак, просто чтобы написать этот план немного более конкретно, учитывая m примеры обучения, Я собираюсь , чтобы выбрать расположение моих ориентиров, чтобы быть точно вблизи мест моего m учебных примеров.
Play video starting at :2:25 and follow transcript2:25
Когда вы даете пример x, и в этом примере x может быть что-то в учебном наборе, это может быть что-то в перекрестной проверке наборе, или это может быть что-то в тестовом наборе. В примере x мы будем вычислять, знаете, эти функции как f1, f2 и так далее. Где l1 фактически равен x1 и так далее. И они затем дают мне вектор функций. Итак, позвольте мне написать f в качестве вектора объектов. Я собираюсь взять эти f1, f2 и так далее, и просто сгруппировать их в векторный объект.
Play video starting at :2:56 and follow transcript2:56
Отнесите их в fm.
Play video starting at :2:59 and follow transcript2:59
И, знаешь, просто по соглашению. Если мы хотим, мы можем добавить дополнительную функцию f0, которая всегда равна 1. Таким образом, это играет роль, похожую на то, что мы имели ранее. Для x0, который был нашим перехватчиком.
Play video starting at :3:13 and follow transcript3:13
Например, если у нас есть обучающий пример x (i), y (i),
Play video starting at :3:18 and follow transcript3:18
функции, которые мы будем вычислять для , этот обучающий пример будет носить следующим образом: учитывая x (i), мы должны затем сопоставить его, знаете, f1 (i).
Play video starting at :3:27 and follow transcript3:27
Что и есть сходство. Я собираюсь аббревиатуру как SIM вместо того, чтобы записывать все слово
Play video starting at :3:35 and follow transcript3:35
сходство, верно?
Play video starting at :3:37 and follow transcript3:37
И f2 (i) равно сходству между x (i) и l2, и так далее, вниз до fm (i) равно
Play video starting at :3:49 and follow transcript3:49
сходство между x (i) и l (m).
Play video starting at :3:55 and follow transcript3:55
И где-то посередине. Где-то в этом списке, вы знаете, на i-й компонент, я буду на самом деле иметь одну функцию компонент, который является f индекс i (i), который является both сходством между x и l (i).
Play video starting at :4:15 and follow transcript4:15
Где l (i) равно x (i), и поэтому вы знаете, fi (i) просто собирается быть сходством между x и самим собой.
Play video starting at :4:23 and follow transcript4:23
И если вы используете гауссовое ядро, это на самом деле e к минус 0 над 2 сигма в квадрате и так, это будет равно 1, и это нормально. Таким образом, одна из моих особенностей для этого обучающего примера будет равна 1.
Play video starting at :4:34 and follow transcript4:34
А затем похож на то, что у меня есть выше. Я могу взять все эти m объекты и сгруппировать их в векторный объект. Поэтому вместо того, чтобы представлять мой пример, используя, знаете, x (i), что это то, что R (n) плюс R (n) одномерный вектор.
Play video starting at :4:48 and follow transcript4:48
В зависимости от того, можете ли вы установить условия, либо R (n) или R (n) плюс 1. Теперь мы можем вместо этого представить мой пример обучения , используя эту функцию вектор f. Я собираюсь написать этот f надстрочный i. Который будет принимать все данные из этих вещей и укладывать их в вектор. Итак, f1 (i) вниз до fm (i) и если вы хотите и хорошо, обычно мы также добавим этот f0 (i), где f0 (i) равно 1. И поэтому этот вектор здесь дает мне мой новый вектор функции, с помощью которого , чтобы представить мой пример обучения. Итак, учитывая эти ядра и функции сходства, вот как мы используем простой векторный аппарат. Если у вас уже есть обучающий набор параметров theta, то если вы задали значение x и хотите сделать предсказание.
Play video starting at :5:41 and follow transcript5:41
То, что мы делаем, это вычисляем функции f, которые теперь R (m) плюс 1 размерный вектор-объект.
Play video starting at :5:49 and follow transcript5:49
И у нас есть m здесь, потому что у нас есть m обучающие примеры и, таким образом, m ориентиры и то, что мы делаем, мы предсказываем 1, если theta транспонировать f больше или равно 0. Вправо. Итак, если тета транспонировать f, конечно же, это просто равно theta 0, f0 плюс тета 1, f1 плюс точка точка, плюс тета m f (m). И поэтому мой вектор параметров тета также теперь будет m плюс 1 мерный вектор. И у нас есть m здесь, потому что где количество ориентиров равно к размеру тренировочного набора. Таким образом, m был размер обучающего набора и теперь, вектор-параметр тета будет m плюс один мерный.
Play video starting at :6:32 and follow transcript6:32
Вот как вы делаете прогноз , если у вас уже есть настройка для теты параметра. Как получить тету параметра? Хорошо, вы делаете это, используя алгоритм обучения SVM, и, в частности, , что вы делаете, вы решите эту проблему минимизации. Вы минимизировали тета параметра C, умноженное на эту функцию стоимости, которую мы имели раньше. Только сейчас вместо того, чтобы искать там вместо того, чтобы делать предсказания , используя theta transpose x (i), используя наши оригинальные функции , x (i). Вместо этого мы взяли функции x (i) и заменили их новыми функциями
Play video starting at :7:7 and follow transcript7:07
, так что мы используем theta transpose f (i), чтобы сделать предсказание на примерах i'f обучения, и мы видим, что, вы знаете, theta transpose f (i), чтобы сделать предсказание на примерах i'f обучения, и мы видим, что вы получаете параметры для вашей машины вектора поддержки.
Play video starting at :7:23 and follow transcript7:23
И одна последняя деталь , потому что эта задача оптимизации у нас действительно есть n равно m функции. Это здесь. Количество функций, которые у нас есть.
Play video starting at :7:37 and follow transcript7:37
Действительно, эффективное количество функций, которые у нас есть, является измерением f. Так что n на самом деле будет равным к m. Так что, если вы хотите, вы можете определить это как сумму, stoth это действительно сумма от j равна 1 до henm. И затем один из способов подумать о ней, вы можете думать об этом как n, будучи равным m, потому что если f не является новой функцией, то у нас есть m плюс 1 функции , а плюс 1 исходит из перехватчика.
Play video starting at :8:5 and follow transcript8:05
И здесь, мы все еще делаем сумму от j равно 1 до n, потому что, как и наши предыдущие видеоролики по регуляризации, мы до сих пор не регуляризируем параметр bett zero, который является stots, почему это сумма для fetarity j равняется 1 через m, а j равно нулю, хотя вы m. это - алгоритм машинного обучения вектора поддержки. Это один из видов, математических деталь, что я должен упомянуть, который является , что в том, как реализуется векторная машина поддержки , этот последний термин boot на самом деле делается немного по-другому. Так что вам действительно не нужно знать об этой последней детали в , чтобы использовать опорные векторные машины, и на самом деле уравнения, которые записаны здесь, должны дать вам должное все интуиции, которые должны понадобиться. Но в том, как реализуется векторная машина поддержки , вы знаете, этот термин, сумма j theta j в квадрате справа?
Play video starting at :8:53 and follow transcript8:53
Другой способ написать это может быть записано как theta transpose theta, если мы игнорируем параметр theta 0. Таким образом, theta 1 вниз до тета м. Игнорируя тета 0.
Play video starting at :9:11 and follow transcript9:11
Тогда эта сумма j theta j в квадрате, что этот также может быть записана theta транспонировать theta.
Play video starting at :9:19 and follow transcript9:19
И то, что большинство реализаций вектора машины на самом деле заменить эту theta транспонировать theta, вместо этого, theta транспонировать раз некоторую матрицу внутри, что зависит от используемого вами ядра, раз theta. И поэтому это дает нам немного другую метрику расстояния. Мы будем использовать немного другую меру вместо того, чтобы минимизировать ровно
Play video starting at :9:41 and follow transcript9:41
норму тета-квадрат означает , что минимизирует что-то немного похожее на нее. Это похоже на перемасштабированную версию векторного параметра theta, который зависит от ядра. Но это своего рода математическая деталь. Это позволяет программному обеспечению опорного вектора работать намного эффективнее.
Play video starting at :9:58 and follow transcript9:58
И причина, по которой машина вектора поддержки делает это, заключается в этой модификации. Это позволяет ему масштабировать до гораздо больших тренировочных наборов. Потому что, например, если у вас есть учебный набор с 10 000 обучающих примеров.
Play video starting at :10:12 and follow transcript10:12
Тогда, знаете, как мы определяем ориентиры , мы получаем 10 000 ориентиров.
Play video starting at :10:16 and follow transcript10:16
Таким образом, тета становится 10 000 мерных. И, может быть, это работает, но когда m становится действительно, действительно большим , то решение для всех этих параметров, вы знаете, если m был 50 000 или 100 000, то решение для stoth все эти параметры может стать дорогостоящим для поддержки векторной оптимизации программного обеспечения, таким образом решает проблему минимизации, которую я нарисовал здесь. Так как математическая деталь , о которой вам снова не нужно знать.
Play video starting at :10:41 and follow transcript10:41
Это на самом деле изменяет, что последний термин немного к оптимизировать что-то немного иное, чем , просто минимизируя норму в квадрате тета тета, теты. Но если вы хотите, вы можете свободно думать об этом как о n реализационной детали , которая изменяет цель бит , но делается в первую очередь по соображениям вычислительной эффективности, stoth, поэтому обычно вам не нужно беспокоиться об этом.
Play video starting at :11:7 and follow transcript11:07
И, кстати, если ваш задается вопросом, почему мы не применяем идею ядра к другим алгоритмам , а также логистической регрессии , оказывается, что, если вы хотите, вы можете фактически применить идею о том, что вы можете определить источник функций, используя ориентиры и так для логистической регрессии. Но вычислительные трюки, которые применяют для машин опорных векторов, не хорошо обобщают другие алгоритмы, такие как логистическая регрессия. И так, использование ядер с логистической регрессией идет слишком очень медленно, в то время как из-за вычислительных трюков, таких как, что воплощен и как он изменяет both это и детали того, как stoth поддержка векторного программного обеспечения является placete реализован, поддержка векторных машин и ядра, как правило, особенно хорошо сочетаются. Принимая во внимание, что логистическая регрессия и ядра, вы знаете, вы можете это сделать, но это будет работать очень медленно. И он не сможет воспользоваться передовыми методами оптимизации , которые люди выяснили для конкретного случая запуска векторной машины поддержки с ядром. Но все это относится только к тому, как вы фактически реализуете программное обеспечение для минимизации функции затрат. Я скажу больше об этом в следующем видео, но вы действительно не нужно знать о , как написать программное обеспечение для минимизировать эту функцию затрат, потому что вы можете найти очень хорошее программное обеспечение для этого.
Play video starting at :12:18 and follow transcript12:18
И так же, как, вы знаете, я бы не рекомендовал написать код, чтобы инвертировать матрицу или вычислить квадратный корень , я на самом деле не рекомендую писать программное обеспечение для того, чтобы уменьшить эту функцию затрат самостоятельно, а вместо этого использовать пакеты программного обеспечения, которые люди и поэтому эти программные пакеты уже воплощают эти приемы численной оптимизации,
Play video starting at :12:39 and follow transcript12:39
, так что вам не нужно беспокоиться о них. Но еще одна вещь, о которой стоит знать, это когда вы применяете опорный вектор машину, как вы сами сами выбираете параметры опорного вектора?
Play video starting at :12:51 and follow transcript12:51
И последнее, что я хочу сделать в этом видео, это сказать маленькое слово о смещении и отклонениях при использовании опорной векторной машины. При использовании SVM, одна из вещей, которые вам нужно выбрать, это параметр C, который был в цели оптимизации, и вы помните, что C сыграл роль bodb, аналогичную 1 над stots лямбда, где лямбда была регуляризации, который мы имели для логистической регрессии.
Play video starting at :13:15 and follow transcript13:15
Итак, если у вас есть большое значение C, это соответствует к тому, что мы имеем обратно в логистической регрессии, небольшого значения лямбда-значения не использовать много регуляризации. И если вы это сделаете, у вас , как правило, есть гипотеза с более низким уклоном и более высокой дисперсией.
Play video starting at :13:30 and follow transcript13:30
В то время как если вы используете меньшее значение C, то это соответствует, когда мы используем логистическую регрессию с большим значением lambda, и это соответствует hb гипотезе с более высоким stob-смещением и более низкой дисперсией. Итак, гипотеза с большим C имеет более высокую дисперсию и более склонна к переоборудованию, в то время как гипотеза с небольшой C имеет более высокую степень смещения и, таким образом, более склонна к недооснащению.
Play video starting at :13:56 and follow transcript13:56
Таким образом, этот параметр C является одним из параметров, которые нам нужно выбрать. Другой является параметром
Play video starting at :14:2 and follow transcript14:02
сигма в квадрате, который появился в гауссовом ядре.
Play video starting at :14:5 and follow transcript14:05
Так что если гауссовое ядро сигма квадрат велик, то в функции подобия, который это вы знаете Е к минус х минус ориентир
Play video starting at :14:16 and follow transcript14:16
варьируется в квадрате над 2 сигма в квадрате.
Play video starting at :14:20 and follow transcript14:20
В этом одном из примеров; Если у меня есть только одна функция, x1, если у меня есть ориентир там на , что место, если сигма квадрат большой, то, вы знаете, bit гауссовое ядро, как правило, фолт отпадает относительно медленно, и так это будет моя функция f-( i), и так это будет более плавной функцией, которая изменяется более плавно, и поэтому это будет дать вам гипотезу с более высоким уклоном и более низкой дисперсией, потому что гауссово ядро, что плавно падает, вы склонны получить гипотезу о том, что sts меняется медленно, или плавно изменяется, как вы меняете вход x. В то время как, напротив, если сигма в квадрате была мала, и если это мой ориентир , учитывая мою функцию 1 x1, вы знаете, мое гауссовое ядро, моя функция подобия, будет меняться более резко. И в обоих случаях я бы выбрал из 1, и поэтому, если сигма в квадрате мала, то мои функции меняются менее гладко. Так что, если это просто более высокие склоны или более высокие производные здесь. И используя это, вы заканчиваете подходящими гипотезами более низкого смещения , и вы можете иметь более высокую дисперсию.
Play video starting at :15:23 and follow transcript15:23
И если вы посмотрите на упражнение по точкам на этой неделе , вы на самом деле получите , чтобы поиграть с некоторыми из этих идей и увидеть эти эффекты самостоятельно.
Play video starting at :15:31 and follow transcript15:31
Итак, это была машина вектора поддержки с алгоритмом ядра. И, надеюсь, это обсуждение смещения и дисперсии даст вам представление о том, как вы можете ожидать, что этот алгоритм будет вести себя также.