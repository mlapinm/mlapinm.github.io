Trading Off Precision and Recall.В последнем видео мы говорили о точности и вспоминали как оценочную метрику для задач классификации с искаженными константами. Для многих приложений мы хотим каким-то образом контролировать компромисс между точностью и отзывом. Позвольте мне рассказать вам, как это сделать, а также показать вам еще более эффективные способы использовать точность и вспомнить в качестве оценочной метрики для алгоритмов обучения. В качестве напоминания, вот определения точности и отзыва из предыдущего видео. Давайте продолжим наш пример классификации рака, где y равен 1, если у пациента рак, а у равен 0 в противном случае. И предположим, что мы обучены классификатору логистической регрессии, который выводит вероятность между 0 и 1. Итак, как обычно, мы собираемся предсказать 1, y равно 1, , если h (x) больше или равно 0,5. И предсказать 0, если гипотеза выводит значение меньше 0,5. И этот классификатор может дать нам некоторую ценность для точности и некоторую ценность для отзыва. Но теперь, предположим, что мы хотим предсказать, что у пациента рак только в том случае, если мы очень уверены, что они действительно делают. Потому что, если ты пойдешь к пациенту и скажешь им, что у них рак, это даст им огромный шок. То, что мы даем, это серьезно плохие новости, и они могут в конечном итоге пройти через довольно болезненный процесс лечения и так далее. И поэтому, может быть, мы хотим сказать кому-то, что мы думаем, что у них рак, только если они очень уверены. Один из способов сделать это - изменить алгоритм, так что , что вместо установки этого порога на 0,5 мы могли бы вместо этого сказать , что мы будем предсказывать, что y равен 1, только если h (x) больше или равно 0,7. Это похоже на то, что мы скажем кому-то, что у них рак, только если мы думаем, что у них есть вероятность, что у них рак больше или равна, 70%. И, если вы это сделаете, тогда вы предсказываете, что у кого-то рак есть только тогда, когда вы более уверены и , поэтому вы получите классификатор, который имеет более высокую точность. Потому что все пациенты, к которым вы собираетесь и говорите: мы думаем, что у вас рак, хотя эти пациенты теперь те, у которых вы довольно уверены, что на самом деле рак. Таким образом, более высокая доля пациентов, у которых, по вашему мнению, есть рак , на самом деле окажется раком, потому что делать эти предсказания , только если мы уверены в себе. Но, напротив, этот классификатор будет иметь меньшее количество отзыва, потому что теперь мы собираемся делать прогнозы, мы собираемся предсказать y = 1 на меньшем количестве пациентов. Теперь, может даже взять это дальше. Вместо того, чтобы устанавливать порог в 0,7, мы можем установить его на 0,9. Теперь мы прогнозируем y=1, только если мы более 90% уверены, что у пациента рак. Таким образом, большая часть этих пациентов будет иметь рак. И таким образом, это будет более высокая точность классификатора будет иметь меньший отзыв, потому что мы хотим правильно определить, что у этих пациентов рак. Теперь рассмотрим другой пример. Предположим, что мы хотим избежать слишком большого количества фактических случаев рака, поэтому мы хотим избежать ложных негативов. В частности, если у пациента есть рак, но мы не можем сказать им, что у них рак, то это может быть очень плохо. Потому что если мы скажем пациенту, что у них нет рака, , то они не пойдут на лечение. И если выяснится, что у них рак, но мы не можем сказать им, что у них рак, хорошо, они могут вообще не лечиться. И это было бы очень плохим результатом, потому что они умирают, потому что мы сказали им, что у них нет рака. Они не получают лечения, но оказывается, у них на самом деле рак. Итак, предположим, что, когда мы сомневаемся, мы хотим предсказать, что y=1. Итак, когда мы сомневаемся, мы хотим предсказать, что у них рак, так что , по крайней мере, они заглянут дальше в него, и их можно вылечить, если у них будет рак. В этом случае вместо того, чтобы устанавливать более высокий порог вероятности, мы могли бы вместо этого взять это значение и установить его в меньшее значение. Так что, может быть, 0,3, так? И, таким образом, мы говорим, что, если мы думаем, что вероятность заболевания раком больше , чем 30%, нам лучше быть более консервативными и сказать им, что у них может быть рак, чтобы они могли обратиться за лечением, если это необходимо. И в этом случае то, что мы будем иметь, будет более высокий классификатор отзыва , потому что мы будем правильно помечать более высокую долю всех пациентов, у которых на самом деле рак. Но мы закончим с меньшей точностью потому что более высокая доля пациентов, которые, как мы сказали, имеют рак, высокая доля из них окажется не иметь рака в конце концов. И, кстати, как сайдер, когда я говорю об этом другим студентам, мне уже говорили раньше, это довольно удивительно, некоторые из моих учеников говорят, что я могу рассказать историю в обоих направлениях. Почему мы можем хотеть иметь более высокую точность или более высокий отзыв, и история на самом деле, похоже, работает в обоих направлениях. Но я надеюсь, что детали алгоритма верны, и более общий принцип зависит от того, где вы хотите, ли вы хотите более высокой точности - более низкой отзыва или более высокой - меньшей точности. Вы можете в конечном итоге предсказать y=1, когда h (x) больше некоторого порога. И так в целом, для большинства классификаторов есть , чтобы быть балансом между точностью и отзывом, и , поскольку вы изменяете значение этого порога, к которому мы присоединяемся здесь, вы можете построить какую-то кривую, которая торгует точностью и отзывом. Если значение здесь, это будет соответствовать очень высокому значению порога, возможно, порог равен 0,99. То есть, предсказать y=1 только если мы более чем 99% уверены, по крайней мере 99% вероятность этого. Так что это будет высокая точность, относительно низкий отзыв. Где в качестве точки ниже, будет соответствовать значению порога, который намного ниже, может быть равен 0,01, что означает, когда у вас есть сомнения, предсказать y=1, и если вы это сделаете, вы получите гораздо более низкую точность, более высокий классификатор отзыва. И когда вы изменяете порог, если вы хотите, чтобы вы действительно могли отслеживать кривую для вашего классификатора , чтобы увидеть диапазон различных значений, которые вы можете получить для точного отзыва. И, кстати, кривая прецизионного отзыва может выглядеть как много разных форм. Иногда это будет выглядеть так, иногда это будет выглядеть так. Теперь существует множество различных возможных форм для кривой точной отзыва, в зависимости от деталей классификатора. Итак, это поднимает еще один интересный вопрос, который: есть ли способ выбрать этот порог автоматически? Или в более общем плане, если у нас есть несколько разных алгоритмов или несколько разных идей для алгоритмов, как мы сравниваем разные числа отзыва точности? Конкретно, предположим, что у нас есть три разных алгоритма обучения. На самом деле, может быть, это три разных алгоритма обучения, может быть, это один и тот же алгоритм, но просто с разными значениями для порога. Как мы решаем, какой из этих алгоритмов лучше всего? Одна из вещей, о которой мы говорили ранее, — это важность одного показателя оценки реального числа И это идея иметь номер, который просто говорит вам, насколько хорошо работает ваш классификатор Но, переключившись на метрику точного отзыва, мы фактически потеряли это. Теперь у нас есть два реальных числа. И поэтому мы часто, мы сталкиваемся с такими ситуациями, как если бы мы пытаемся сравнить Алгоритм 1 и Алгоритм 2, мы в конечном итоге задаем себе вопрос: точность 0,5 и — это отзыв 0,4, лучше или хуже, чем точность 0,7 и отзыв 0,1? И, если каждый раз, когда вы пробуете новый алгоритм, вам в конечном итоге приходится сидеть вокруг и думать, ну, может быть, 0.5/0.4 лучше, чем 0.7/0.1, или, может быть, нет, я не знаю. Если вам в конечном итоге приходится сидеть и думать и принимать эти решения, это действительно замедляет ваш процесс принятия решений на , какие изменения полезны для включения в ваш алгоритм. В то время как, напротив, если у нас есть одна метрика оценки реального числа , как число, которое просто говорит нам, что алгоритм 1 или алгоритм 2 лучше, , то это помогает нам гораздо быстрее решить, с каким алгоритмом идти. Это помогает нам гораздо быстрее оценивать различные изменения , которые мы можем рассматривать для алгоритма. Итак, как мы можем получить одну метрику оценки реального числа? Одна естественная вещь, которую вы можете попробовать, это посмотреть на среднюю точность и вспомнить. Итак, используя P и R для обозначения точности и отзыва, то, что вы можете сделать, это просто вычислить среднее и посмотреть, какой классификатор имеет самое высокое среднее значение. Но это оказывается не таким хорошим решением, потому что похоже на пример , который мы имели ранее, оказывается, что если у нас есть классификатор, который предсказывает y=1 все время, то если вы это сделаете, вы можете получить очень высокий отзыв, , но вы в конечном итоге с очень низким значением точности. И наоборот, если у вас есть классификатор, который предсказывает y равен нулю, почти все время, то есть он предсказывает y=1 очень экономно, это соответствует установка очень высокого порога, используя нотацию предыдущего y. Тогда вы можете в конечном итоге с очень высокой точностью с очень низким отзывом. Итак, две крайности либо очень высокого порога, либо очень низкого порога, ни один из них не даст особо хорошего классификатора. И мы признаем это, видя, что мы в конечном итоге с очень низкой точностью или очень низким отзывом. И если вы просто возьмете среднее значение (P+R) /2 из этого примера, среднее на самом деле самое высокое для Алгоритма 3, даже если вы можете получить эту производительность , прогнозируя y=1 все время, и это просто не очень хороший классификатор, верно? Вы все время предсказываете y=1, просто обычный полезный классификатор, но все, что он делает, это выводит y=1. И поэтому Алгоритм 1 или Алгоритм 2 будут более полезны, чем Алгоритм 3. Но в этом примере Алгоритм 3 имеет более высокое среднее значение точности отзыва, чем Алгоритмы 1 и 2. Таким образом, мы обычно считаем эту среднюю точность и вспомнить как не очень хороший способ оценить наш алгоритм обучения. В отличие от этого, существует другой способ сочетания точности и отзыва. Это называется F Score, и он использует эту формулу. И так в этом примере, вот F Scores. И поэтому мы бы сказали из этих F оценок, похоже, что Алгоритм 1 имеет самый высокий показатель F, Алгоритм 2 имеет второй самый высокий, а Алгоритм 3 имеет самый низкий. И так, если мы пойдем по F Score, мы выберем, вероятно, Алгоритм 1 над остальными. Оценка F, которая также называется F1 Score, обычно записывается F1 Score, что у меня есть здесь, но часто люди просто говорят F Score, используется любой термин. немного похоже на среднее значение точности и отзыва, но это дает меньшее значение точности и отзыва, в зависимости от того, что это имеет, он дает ему больший вес. Итак, вы видите в числителе здесь , что F Score принимает произведение точности и отзыва. И поэтому, если точность равна 0 или отзыв равна 0, оценка F будет равна 0. Таким образом, в этом смысле он сочетает точность и отзыв, но для F Score, чтобы быть большим, как точность, так и отзыв должны быть довольно большими. Должен сказать, что существует много различных возможных формул для точности расчесывания и отзыва. Эта формула F Score действительно может быть, только один из гораздо большего количества возможностей, но исторически или традиционно это то, что люди в машинном обучении, похоже, используют. И термин F Score, это на самом деле ничего не значит, поэтому не беспокойтесь о том, почему он называется F Score или F1 Score. Но это обычно дает вам эффект, который вы хотите, потому что если либо точность равна нулю или отзыв равен нулю, это дает вам очень низкий F Score, и поэтому, чтобы иметь высокий F Score, вам нужна точность или отзыв, чтобы быть одним. И конкретно, если p=0 или R = 0, , то это дает вам, что F Score = 0. В то время как идеальный F Score, так что если точность равна одному , а отзыв равен 1, это даст вам F Score, , который равен 1 раз 1 за 2 раза 2, поэтому F Score будет равен 1, если у вас есть идеальная точность и идеальный отзыв. И промежуточные значения между 0 и 1, это обычно дает разумное ранжирование разных классификаторов. Итак, в этом видео мы говорили о понятии торговли между точностью и отзывом, и , как мы можем изменить порог, который мы используем, чтобы решить, следует ли предсказать y=1 или y=0. Таким образом, это порог, который говорит: нужно ли нам быть не менее 70% уверенными или 90% уверенными, или что-то еще, прежде чем мы предсказываем y=1. И изменяя порог, вы можете контролировать соотношение между точностью и отзывом. Мы также говорили о F Score, которая требует точности и отзыва, и опять же дает вам одну метрику оценки реального числа. И, конечно, если ваша цель состоит в том, чтобы автоматически установить этот порог, чтобы решить , что на самом деле y=1 и y=0, один из довольно разумных способов сделать это будет также попробовать ряд различных значений порогов. Таким образом, вы пробуете диапазон значений порогов и оцениваете эти различные пороги на вашем наборе перекрестной проверки, а затем выбираете любое значение порога , дает вам самый высокий показатель F на вашей кроссвалидации [INAUDIBLE]. И это довольно разумный способ автоматического выбора порога для вашего классификатора.