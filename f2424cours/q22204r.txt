Gradient Descent in Practice II - Learning Rate.
В этом видео я хочу поделиться практическими приемами по работе с методом градиентного спуска.
В основном в этом видео мы будем говорить про скорость обучения - альфа.
А именно, вот правило обновлений для градиентного спуска.
В этом видео я расскажу вам про такую вещь, как отладку, и дам несколько советов относительно того, как заставить градиентный спуск работать корректно.
Кроме того, я расскажу о том, как выбрать скорость обучения и о том, как это обычно делаю я.
Вот что я обычно делаю для того, чтобы убедиться, что градиентный спуск работает корректно.
Задача градиентного спуска - найти такое значение вектора тета, которое, в идеале минимизирует значение J(тета).
Поэтому я часто рисую график изменения функции стоимости J по мере работы градиентного спуска.
Ось X на графике - это номер итерации градиентного спуска, поэтому по мере того, как работает алгоритм, вы в идеале должны получить график какого-то такого вида.
Заметьте, что ось X - это количество итераций.
Раньше мы смотрели на графики J(тета), где ось X, горизонтальная ось, соответствовала вектору параметров тета, но не в этом случае.
Если конкретно, то это точка соответствует сотой итерации градиентного спуска.
И после ста итераций я получаю какое-то значение тета, некоторое значение вектора тета, и вычисляю функцию J(тета) для этого значения тета на сотой итерации, 
и высота этой точки - это значение J для того тета, 
которое я получил после ста итераций градиентного спуска.
А вот эта точка соответствует значению J для тета, полученного на 200-й итерации градиентного спуска.
Так что на этом графике показано значение вашей функции стоимости после некоторой итерации градиентного спуска.
И если градиентный спуск работает правильно, но J(тета) должно уменьшаться. После каждой итерации.
Чем еще может быть полезен этот график: если вы посмотрите на конкретную линию, которую я нарисовал, то увидите, что когда вы добрались до 300-й итерации, то на вот этом отрезке, между 300-й и 400-й итерацией, кажется, что J(тета) уже не сильно уменьшается.
Так что к тому времени, когда вы дошли до 400 итераций, похоже, что кривая вышла на плато.
Поэтому вот здесь, на 400 итерациях, судя по всему, градиентный спуск уже более или менее сошелся, потому что ваша функция затрат практически не уменьшается.
Так что посмотреть на этот график может быть полезно для оценки того, сошелся ли градиентный спуск.
Кстати говоря, количество итераций, которое нужно градиентному спуску, чтобы сойтись, в реальной жизни может быть абсолютно разным.
В одной задаче градиентный спуск может сойтись после 30 итераций, в другой - после 3000 итераций.
А для другого алгоритма обучения может потребоваться три миллиона итераций.
Оказывается, что заранее очень сложно предсказать, сколько итераций потребуется градиентному спуску для сходимости, 
и бывает полезно построить такой вот график. 
График функции стоимости от количества итераций.
Обычно, глядя на такие графики, я пытаюсь понять, сошелся ли градиентный спуск.
Также можно придумать автоматический критерий сходимости: а именно, добавить в алгоритм оценку того, сошелся ли градиентный спуск.
Вот, к примеру, типичный вариант автоматического критерия: процесс явно сходится, если функция стоимости J(тета) уменьшается меньше чем на некоторое маленькое значение эпсилон, скажем, 10^-3, за одну итерацию.
Но для меня подобрать этот порог обычно довольно сложно.
Поэтому для проверки сходимости градиентного спуска я чаще все-таки смотрю на такие графики, как в левой части слайда, а не полагаюсь на автоматические критерии сходимости.
Еще, глядя на такой график, можно заранее увидеть, что градиентный спуск, возможно, работает некорректно.
---Конкретно, если вы построите J (тета) как функцию количества итераций.
Затем, если вы видите такую ​​фигуру, где J(theta) фактически увеличивается, то это дает вам явный признак того, что градиентный спуск не работает.
И такая тета обычно означает, что вы должны использовать скорость обучения альфа.
Если J(theta) на самом деле увеличивается, наиболее распространенная причина этого заключается в том, что вы пытаетесь минимизировать функцию, которая может выглядеть так.
Но если ваша скорость обучения слишком велика, то, если вы начнете с этого, градиентный спуск может выйти за пределы минимума и отправить вас туда.
И если скорость обучения слишком велика, вы можете снова промахнуться, и это отправит вас туда, и так далее.
Итак, вы действительно хотели, чтобы это началось здесь и медленно пошло вниз, верно?
Но если скорость обучения слишком велика, то вместо этого градиентный спуск может продолжать превышать минимум.
Так что вы на самом деле становитесь все хуже и хуже вместо того, чтобы достигать более высоких значений функции стоимости J (тета).
Таким образом, вы получаете такой график, и если вы видите такой график, исправление обычно состоит в том, чтобы использовать меньшее значение альфы.
О, и, конечно же, убедитесь, что в вашем коде нет ошибок.
Но обычно слишком большое значение альфы может быть распространенной проблемой.
Точно так же иногда вы также можете видеть, как J(тета) делает что-то подобное, она может опускаться на некоторое время, затем подниматься, затем опускаться на некоторое время, затем подниматься, опускаться на некоторое время, подниматься и так далее.
И исправление для чего-то подобного также состоит в том, чтобы использовать меньшее значение альфы.
Я не собираюсь доказывать это здесь, но при других предположениях о функции стоимости J, которые справедливы для линейной регрессии, математики показали, что если ваша скорость обучения альфа достаточно мала, то J(тета) должна уменьшаться на каждом шагу. итерация.
Поэтому, если этого не происходит, вероятно, это означает, что альфа-канал слишком велик, вам следует установить его меньше.
Но, конечно, вы также не хотите, чтобы ваша скорость обучения была слишком маленькой, потому что, если вы это сделаете, градиентный спуск может медленно сходиться.
И если бы альфа был слишком маленьким, вы могли бы начать, скажем, здесь, и в конечном итоге сделать крошечные детские шаги.
И просто сделать много итераций, прежде чем вы, наконец, доберетесь до минимума, и поэтому, если альфа слишком мала, градиентный спуск может очень медленно прогрессировать и медленно сходиться.
Подводя итог, если скорость обучения слишком мала, у вас может возникнуть проблема с медленной сходимостью, а если скорость обучения слишком велика, J (тета) может не уменьшаться на каждой итерации и может даже не сходиться.
В некоторых случаях, если скорость обучения слишком велика, также возможна медленная сходимость.
Но более распространенная проблема, которую вы видите, заключается в том, что J (тета) может не уменьшаться на каждой итерации.
И для того, чтобы отлаживать все эти вещи, часто отображая зависимость J(theta) от количества итераций, вы можете понять, что происходит.
Конкретно, что я на самом деле делаю, когда запускаю градиентный спуск, так это пробую диапазон значений.
Так что просто попробуйте запустить градиентный спуск с диапазоном значений альфы, например, 0.001 и 0.01.
Таким образом, это коэффициент десяти различий.
И для этих различных значений альфы просто график J (тета) как функция количества итераций, а затем выберите значение альфы, которое, кажется, вызывает быстрое уменьшение J (тета).
На самом деле то, что я делаю, это не эти десять шагов.
Таким образом, это десятичный масштабный коэффициент для каждого шага вверх.
Что я на самом деле делаю, так это пробую этот диапазон значений.
И так далее, где это 0.001.
Затем я увеличу скорость обучения в три раза, чтобы получить 0.003.
И затем этот шаг вверх, это еще одно примерно трехкратное увеличение от 0.003 на 0.01.
Итак, это, грубо говоря, попытка градиентного спуска с каждым значением, которое я пытаюсь сделать примерно в 3 раза больше, чем предыдущее значение.
Итак, что я буду делать, так это пробовать диапазон значений, пока не найду одно значение, которое слишком мало, и убедиться, что я нашел одно значение, которое слишком велико.
А затем я попытаюсь выбрать максимально возможное значение или просто что-то немного меньшее, чем самое большое разумное значение, которое я нашел.
И когда я это делаю, обычно это просто дает мне хорошую скорость обучения для моей проблемы.
И если вы сделаете это тоже, возможно, вы сможете выбрать хорошую скорость обучения для своей реализации градиентного спуска.