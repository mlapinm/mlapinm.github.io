var l22124r = [
"Gradient Descent For Linear Regression.",
"В предыдущих видеороликах мы говорили об алгоритме градиентного спуска, а также о модели линейной регрессии и функции стоимости квадрата ошибки.",
"В этом видео мы собираемся объединить градиентный спуск с нашей функцией стоимости, и это даст нам алгоритм линейной регрессии или прямой линии к нашим данным.",
"Так это то, что мы работали в предыдущих видео.",
"Этот алгоритм градиентного спуска, с которым вы должны быть знакомы, и вот модель линейной регрессии с нашей линейной гипотезой и нашей функцией стоимости квадрата ошибки.",
"Что мы собираемся сделать, так это применить градиентный спуск, чтобы минимизировать нашу функцию стоимости квадрата ошибки.",
"Теперь, чтобы применить градиентный спуск, чтобы, вы знаете, написать этот фрагмент кода, ключевым термином, который нам нужен, является вот этот производный термин.",
"Итак, вам нужно выяснить, что представляет собой этот член частной производной, и добавить определение функции причины j.",
"Оказывается вот что.",
"Сумма от y равна 1, хотя m. Из этого члена функции стоимости квадрата ошибки.",
"И все, что я здесь сделал, это просто, вы знаете, вставил туда определение функции стоимости.",
"И еще немного упрощая, это оказывается равным этому.",
"Сигма i равна единице через m числа тета-ноль плюс тета-единица x i минус Yi в квадрате.",
"И все, что я там сделал, это взял определение своей гипотезы и вставил его туда.",
"И оказывается, нам нужно выяснить, какова эта частная производная для двух случаев, когда J равно 0 и J равно 1.",
"Итак, мы хотим выяснить, какова эта частная производная как для случая тета 0, так и для случая тета 1.",
"И я просто напишу ответы.",
"Оказывается, этот первый член упрощает до 1/M сумму за мой шаг обучения только X (i) - Y (i), и для этого термина частная производная давайте напишем тета 1, получается, я получаю это срок.",
"Минус Y(i), умноженный на X(i).",
"Итак, вычисляем эти частные производные, так что мы исходим из этого уравнения.",
"Правильно переходим от этого уравнения к любому из уравнений внизу.",
"Вычисление этих частных производных требует некоторого многомерного исчисления.",
"Если вы разбираетесь в математическом анализе, не стесняйтесь работать с выводами самостоятельно и убедитесь, что если вы возьмете производные, вы действительно получите ответы, которые получил я.",
"Но если вы менее знакомы с исчислением, не беспокойтесь об этом, и вполне нормально просто взять эти уравнения, которые были разработаны, и вам не нужно знать исчисление или что-то в этом роде, чтобы выполнить домашнее задание, так что давайте реализовать градиентный спуск и вернуться к работе.",
"Итак, вооружившись этими определениями или вооружившись тем, что мы разработали как производные, которые на самом деле являются просто наклоном функции стоимости j, мы теперь можем подключить их обратно к нашему алгоритму градиентного спуска.",
"Итак, вот градиентный спуск для линейной регрессии, который будет повторяться до сходимости, тета 0 и тета 1 обновляются, как вы знаете, минус альфа, умноженная на производный член.",
"Так вот этот термин.",
"Итак, вот наш алгоритм линейной регрессии.",
"Это первый срок здесь.",
"Этот член, конечно, всего лишь частная производная по тета-нулю, которую мы вычислили на предыдущем слайде.",
"И этот второй член здесь, этот член — просто частная производная по отношению к тета 1, которую мы вычислили в предыдущей строке.",
"И просто в качестве быстрого напоминания, вы должны при реализации градиентного спуска.",
"На самом деле есть одна деталь, которую вы должны реализовать, чтобы одновременно обновлять тета 0 и тета 1.",
"Так. Давайте посмотрим, как работает градиентный спуск.",
"Одна из проблем, с которыми мы столкнулись при градиентном спуске, заключается в том, что он может быть восприимчив к локальным оптимумам.",
"Итак, когда я впервые объяснил градиентный спуск, я показал вам эту картину, на которой он движется вниз по поверхности, и мы увидели, как в зависимости от того, где вы его инициализируете, вы можете оказаться в разных локальных оптимумах.",
"Вы окажетесь либо здесь, либо здесь.",
"Но оказывается, что функция стоимости для линейной регрессии всегда будет иметь форму дуги, как эта.",
"Технический термин для этого состоит в том, что это называется выпуклой функцией.",
"И я не буду давать формальное определение того, что такое выпуклая функция, C, O, N, V, E, X.",
"Но неформально выпуклая функция означает чашеобразную функцию, поэтому эта функция не имеет локальных оптимумов, кроме одного глобального оптимума.",
"И делает ли градиентный спуск для этого типа функции стоимости, которую вы получаете всякий раз, когда используете линейную регрессию, он всегда будет сходиться к глобальному оптимуму.",
"Потому что нет другого локального оптимума, глобального оптимума.",
"Итак, теперь давайте посмотрим на этот алгоритм в действии.",
"Как обычно, вот графики функции гипотезы и моей функции стоимости j.",
"Итак, допустим, я инициализировал свои параметры этим значением.",
"Скажем, обычно вы инициализируете свои параметры нулем, нулем. Тета ноль и тета равно нулю.",
"Но для демонстрации, в этой физической инфронтации я инициализировал тета-ноль на 900 и тета-единицу примерно на -0,1, хорошо.",
"Итак, это соответствует h(x)=-900-0.",
"1x, [пересечение должно быть +900] это линия, здесь на стоимости",
"Теперь, если мы сделаем один шаг в градиентном спуске, мы закончим тем, что перейдем от этой точки сюда, вниз и влево, к этой второй точке вон там.",
"И вы заметили, что моя линия немного изменилась, и когда я сделаю еще один шаг градиентного спуска, моя линия слева изменится.",
"Верно?",
"И я также перешел к новой точке моей функции затрат.",
"И по мере того, как я делаю дальнейшие шаги по градиентному спуску, я снижаю стоимость.",
"Так что мои параметры и тому подобное следуют этой траектории.",
"А если посмотреть налево, то это соответствует гипотезам.",
"Кажется, это все лучше и лучше соответствует данным, пока, в конце концов, я не пришел к глобальному минимуму, и этот глобальный минимум соответствует этой гипотезе, которая дает мне хорошее соответствие данным.",
"Итак, это градиентный спуск, и мы только что запустили его и получили хорошее соответствие моему набору данных о ценах на жилье.",
"И теперь вы можете использовать его, чтобы предсказать, знаете ли, если у вашего друга есть дом площадью 1250 квадратных футов, вы теперь можете прочитать его стоимость и сказать ему, что я не знаю, может быть, они могли бы получить 250 000 долларов за свой дом.",
"Наконец, чтобы дать этому другое имя, оказывается, что алгоритм, который мы только что рассмотрели, иногда называют пакетным градиентным спуском.",
"И оказывается, что в машинном обучении я не знаю, я чувствую, что мы, люди, обучающиеся машинному обучению, не всегда были хороши в том, чтобы давать имена алгоритмам.",
"Но термин пакетный градиентный спуск относится к тому факту, что на каждом этапе градиентного спуска мы рассматриваем все обучающие примеры.",
"Итак, при градиентном спуске при вычислении производных мы вычисляем суммы [НЕРАЗБОРЧИВО].",
"Таким образом, на каждом шаге градиентного спуска мы в конечном итоге вычисляем что-то вроде этого, которое суммирует наши m обучающих примеров, и поэтому термин пакетный градиентный спуск относится к тому факту, что мы смотрим на всю партию обучающих примеров.",
"И опять же, это действительно не очень удачное имя, но именно так его называют люди, занимающиеся машинным обучением.",
"И оказывается, что иногда существуют другие версии градиентного спуска, которые не являются пакетными версиями, а вместо этого являются таковыми.",
"Не смотрите на весь тренировочный набор, а просматривайте небольшие подмножества тренировочных наборов за раз.",
"И мы поговорим об этих версиях позже в этом курсе.",
"Но пока, используя алгоритм, о котором мы только что узнали, или пакетный градиентный спуск, вы теперь знаете, как реализовать градиентный спуск для линейной регрессии.",
"Итак, линейная регрессия с градиентным спуском.",
"Если вы уже видели продвинутую линейную алгебру, возможно, кто-то из вас посещал занятия по продвинутой линейной алгебре.",
"Возможно, вы знаете, что существует решение для численного решения минимума функции стоимости j без необходимости использования итеративного алгоритма, такого как градиентный спуск.",
"Позже в этом курсе мы также поговорим об этом методе, который просто находит минимум функции стоимости j без необходимости этих нескольких шагов градиентного спуска.",
"Этот другой метод называется методом нормальных уравнений.",
"Но если вы слышали об этом методе, оказывается, что градиентный спуск лучше масштабируется для больших наборов данных, чем метод обычного уравнения.",
"И теперь, когда мы знаем о градиентном спуске, мы сможем использовать его во множестве различных контекстов, а также во множестве различных задач машинного обучения.",
"Так что поздравляю с изучением вашего первого алгоритма машинного обучения.",
"Позже у нас будут упражнения, в которых мы попросим вас реализовать градиентный спуск и, надеюсь, вы сами увидите эти алгоритмы.",
"Но перед этим я сначала хочу рассказать вам в следующем наборе видео.",
"Первый, кто расскажет вам об обобщении алгоритма градиентного спуска, которое сделает его намного более мощным.",
"И, думаю, я расскажу вам об этом в следующем видео.",
]