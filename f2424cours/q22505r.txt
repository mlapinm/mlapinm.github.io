Gradient Checking.
В последних нескольких видео мы говорили о том, как сделать прямое распространение и обратное распространение в нейронной сети для вычисления производных.
Но обратная подборка как алгоритм имеет много деталей и может быть сложно реализуема.
И одна неприятная особенность в том, что есть большой шанс ошибиться в реализации обратного распространения.
Так что если вы запустите его с градиентным спуском или каким-то другим алгоритмом оптимизации, на самом деле он может выглядеть так, как будто он работает.
И ваша функция стоимости, J от тета может в конечном итоге уменьшаться на каждой итерации градиентного спуска.
Но это может оказаться истиной даже, хотя могут быть некоторые ошибки в вашей реализации обратно распространения.
Это выглядит так, будто J от тета уменьшается, но вы можете получить нейронную сеть, которая имеет больше ошибок, чем при реализации без учета ошибок.
И вы можете даже не знать, что была маленькая ошибка, который давала вам худшую производительность.
Итак, что мы можем с этим сделать?
Есть идея под названием проверка градиента которая устраняет почти все эти проблемы.
Сегодня каждый раз, когда я внедряю обратное распространение или похожий алгоритм градиентного спуска на нейронной сети или любая другая достаточно сложная модель, я всегда реализую проверку градиента.
И если вы сделаете это, это поможет вам быть уверенным в том, ваша реализация прямого и обратного распространения или другого, правильна на 100%.
И из всего, что я видел это в значительной степени устраняет все проблемы, связанные с с какой-то ошибочной реализацией обратного распространения.
И в предыдущих видео я просил вас поверить, что формулы, которые я дал для для вычисления дельты, производной и так далее, я попросил вас поверить что они действительно вычисляют градиенты функции стоимости.
Но как только вы реализуете проверку числового градиента, это тема этого видео, вы сможете полностью убедиться, что код, который вы пишете действительно вычисляет производную функции J.
Итак, в чем идея, рассмотрим следующий пример.
Предположим, что у меня есть функция J от тета, и у меня есть некоторое значение тета и для этого примера предположим, что тета - это просто действительное число.
И скажем, что я хочу оценить производную этой функции в этой точке и, следовательно, производная равна наклону этой касательной.
Вот как я собираюсь численно приблизить производную, или скорее это процедура для численной аппроксимации производной.
Я собираюсь вычислить тета плюс эпсилон, поэтому теперь мы переместим его вправо.
И я собираюсь вычислить тета минус эпсилон и я собираюсь посмотреть на эти две точки, и соединить их прямой линией.
И я собираюсь соединить эти две точки прямой линией, и я буду использовать наклон этой маленькой красной линии как мое приближение к производной.
То есть истинная производная - это наклон этой синей линии.
Мне кажется, это было бы довольно хорошим приближением.
Математически наклон этой красной линии равен этой вертикальной высоте поделенной на эту горизонтальную ширину.
Таким образом, эта точка сверху является J (Тета плюс Эпсилон).
Эта точка здесь J (тета минус Эпсилон), так эта вертикальная разница составляет J (тета плюс эпсилон) минус J тета минус эпсилон и это горизонтальное расстояние составляет всего 2 эпсилон.
Так что мое приближение получится, что производная J (тета) при этом значении тета, что это примерно J (тета плюс эпсилон) минус J (тета минус эпсилон) делить на 2 эпсилон.
Обычно я использую довольно небольшое значение для эпсилон, ожидается, что эпсилон может быть порядка 10 минус 4.
Обычно для эпсилона существует большой диапазон различных значений, которые работают просто отлично.
И на самом деле, если вы позволите эпсилону стать очень маленьким, то математически это выражение математически становится производной.
Это становится точным наклоном функции в этой точке.
Мы не хотим использовать эпсилон слишком маленький, потому что вы можете столкнуться с проблемами вычислений.
Поэтому я обычно использую эпсилон около десяти в минус четвертой.
Возможно некоторые из вас видели альтернативную формулу которая соответствует производной, которая является этой формулой.
Эта справа называется односторонняя разница, в то время как формула слева это называется двусторонняя разница.
Двусторонняя разница дает нам более точную оценку, поэтому я обычно использую её, а не эту одностороннюю оценку разницы.
Когда вы реализуете в OCTAVE, вы делаете следующее: вы реализуете вызов для вычисления gradApprox, которая будет нашим приближением производной, как раз здесь эта формула, J (тета плюс эпсилон) минус J (тета минус эпсилон), деленный на 2 раза эпсилон.
И это даст вам численную оценку градиента в этой точке.
И в этом примере это выглядит как довольно хорошая оценка.
На предыдущем слайде, мы рассмотрели случай, когда тета была свернутым числом.
Теперь давайте рассмотрим более общий случай, когда тета является векторным параметром скажем тета это Rn.
И это может быть развернутая версия параметров нашей нейронной сети.
Таким образом, тета - это вектор, который имеет n элементов, от тета 1 до тета n.
Мы можем тогда использовать аналогичную идею аппроксимации всех частей частной производной.
Конкретно частная производная функции стоимости по первому параметру, тета один, который можно получить, взяв J и увеличив тета один.
Итак, у вас есть J (тета one плюс эпсилон) и так далее.
минус эта J (тета один минус эпсилон) и разделить его на два эпсилон.
Частная производная по отношению ко второму параметру тета два, снова это за исключением того, что взял отсюда J (тета 2 увеличил на эпсилон), и здесь вы уменьшаете тета 2 на эпсилон и т.
д.
по каждой производной.
Отношение тета n даст вам увеличение и уменьшение тета и на эпсилон тут.
Эти уравнения дают вам возможность численно аппроксимировать частную производную от J по любому из ваших параметров тета (i).
В итоге вы реализуете следующее.
Мы реализуем следующее в OCTAVE для численного вычисления производных.
Вы говорим, что для i = 1: n, где n - размерность нашего параметра вектора тета.
И я обычно делаю это с развернутой версией параметра.
Так что тэта - это просто длинный список всех моих параметров в моей нейронной сети, Я собираюсь установить thetaPlus = theta, затем увеличивать тэта плюс элемента (i) на эпсилон.
И поэтому в основном thetaPlus равно тета, за исключением thetaPlus (i), который теперь увеличивается на эпсилон.
Тета плюс равен, [Тета 1; Тета 2; .
.
.
.
Затем к .
.
.
Тета i добавили эпсилон, и затем мы спустимся к Тета n].
Вот что такое thetaPlus.
Эти две строки устанавливают тэта минус к чему-то похожему, только вместо Тета i плюс эпсилон становится Тета i минус эпсилон.
И, наконец, вы реализуете этот gradApprox (i) и это даст вам ваше приближение к частной производной Тета i в отношении J(тета).
И способ, которым мы воспользуемся в нашей реализации нейронной сети, мы реализовали этот for цикл, чтобы вычислить верхнюю частную производную функции стоимости для каждого параметра в этой сети, и тогда мы можем взять градиент, который мы получили от backprop.
Таким образом, DVec ,будет производной, которую мы получили от backprop.
так что backprop, обратное распространение был относительно эффективный способ вычисления производной или частной производной.
из функции стоимости с учетом всех наших параметров.
И то, что я обычно делаю, беру мою численно вычисленную производную это GradApprox, который мы только что получили отсюда.
И убеждаюсь, что это равно или примерно равно вплоть до маленьких значений числового округления, и они довольно близко.
DVec, который я получил от backprop.
И если эти два способа вычисления производных дают мне один и тот же ответ, дают мне похожие ответы, с точностью до нескольких знаков после запятой, тогда я более уверен, что моя реализация backprop верна.
И когда я подключаю эти векторы DVec к градиентному спуску или какому-то продвинутому алгоритму оптимизации, тогда я могу быть гораздо более уверен, что я правильно вычисляю производные, и поэтому надеюсь, мой код будет работать правильно, и я проделал хорошую работу по оптимизации J (тета).
Наконец, я хочу собрать все вместе и рассказать вам, как реализовать эту числовую проверку градиента.
Вот, что я обычно делаю.
Первое, что я делаю, это реализую обратное распространение для вычисления DVec.
Есть процедура, о которой мы говорили в предыдущем видео для вычисления DVec, который может быть нашей развернутой версией этих матриц.
Итак, что я делаю, это реализовываю числовой градиент проверки вычисления gradApprox.
Это то, что я описал ранее в этом видео и на предыдущем слайде.
Затем следует убедиться, что DVec и GradApprox дают одинаковые значения, скажем, до несколько знаков после запятой.
И, наконец, и это важный шаг, прежде чем начать использовать свой код для обучения, для серьезного обучения вашей сети, важно отключить проверку градиента и больше не вычислять эту вещь gradApprox, используя формулы числовой производной, о которых мы говорили ранее в этом видео.
И причина этого кроется в коде проверки числового кода, то, о чём мы говорили в этом видео, это очень дорого в вычислительном отношении, это очень медленный способ попытаться приблизить производную.
В отличие от алгоритма обратного распространения, о котором мы говорили ранее, это то, о чем мы говорили ранее для вычислений D1, D2, D3 для Dvec.
Backprop - намного более эффективный способ вычислений для производных.
Итак, когда вы убедились, что ваша реализация обратного распространения работает правильно, вы должны отключить проверку градиента и просто прекратить использовать это.
Хочу повторить, вы должны быть уверены, что отключили код проверки градиента перед запуском вашего алгоритма для много итерационного градиентного спуска или для много итерационных продвинутых алгоритмов оптимизации, чтобы тренировать свой классификатор.
Конкретно, если бы вы запустили проверку числового градиента на каждой итерации градиентного спуска.
Или, если вы были во внутреннем цикле вашей функции costFunction, тогда ваш код будет очень медленным.
Поскольку код проверки числового градиента намного медленнее чем алгоритм обратного распространения, это метод где как вы помните, мы вычисляли дельту (4), дельту (3), дельту (2) и так далее.
Это был алгоритм обратного распространения.
Это гораздо более быстрый способ вычисления производных, чем проверка градиента.
Поэтому как только вы проверите реализацию обратного распространения убедитесь, что вы ВЫКЛЮЧИЛИ код проверки градиента для тренировки своего алгоритма, иначе код может работать очень медленно.
Итак, вот как вы берете градиенты численно, и это как вы можете проверить реализация обратного распространения верна.
Всякий раз, когда я реализую обратное распространение или аналогичный алгоритм распознавания градиента для сложных режимов, я всегда использую проверку градиента и это действительно помогает мне убедиться, что мой код правильный.