Diagnosing Bias vs.
Variance.
Если вы запускаете алгоритм обучения, и это не делает до тех пор, пока вы надеетесь, почти все время, это будет потому, что у вас либо проблема с высоким уклоном, либо проблема с высокой дисперсией, другими словами, либо проблема с недооснащением, либо проблема переоснащения.
В этом случае очень важно выяснить, какая из эти две проблемы являются смещением или дисперсией или немного обоих, что у вас есть на самом деле.
Потому что знание того, что из этих двух вещей происходит, даст очень сильный индикатор для ли полезные и перспективные способы попытаться улучшить свой алгоритм.
В этом видео я хотел бы глубже вникать в эту проблему смещения и дисперсии и лучше понять их, как это было выяснить, как посмотреть в алгоритме обучения и оценить или диагностировать, может ли у нас проблема смещения или дисперсия проблема, так как это будет иметь решающее значение для определения out , как улучшить производительность алгоритма обучения, который вы будете реализовывать.
Итак, вы уже видели эту цифру несколько раз, когда если вы подойдете две простые гипотезы, такие как прямая линия, которая подкладывает данные, если вы вписываете две сложные гипотезы, то, что может соответствовать обучающий набор идеально, но перегружать данные, и это может быть гипотезой некоторых промежуточный уровень сложностей некоторых, возможно, степени два полинома или не слишком низкий и не слишком высокий уровень , это как правильно и дает вам лучшую ошибку обобщения над этими опциями.
Теперь, когда мы вооружены понятием обучения цепи и валидации в тестовых наборах, мы можем понять понятия смещения и дисперсии немного лучше.
Конкретно давайте сделаем так, чтобы наша ошибка обучения и ошибка перекрестной проверки были определены как в предыдущих видео.
Просто скажем квадратная ошибка, средняя квадратная ошибка, как измеряется на тренировочных наборах или как измеряется на перекрестном наборе валидации.
Теперь давайте нарисуем следующий рисунок.
На горизонтальной оси я собираюсь построить степень полинома.
Так что, когда я иду вправо, я буду подгонять полиномы высшего и высшего порядка.
Итак, где слева от этой цифры, где, возможно, d равно одному, мы будем подгонять очень простые функции, тогда как мы здесь справа от горизонтальной оси, у меня гораздо большие значения ds, гораздо более высокой степени полинома.
Итак, здесь это будет соответствовать подгонке гораздо более сложных функций к вашему тренировочному набору.
Давайте посмотрим на ошибку обучения и ошибку перекрестной проверки и нарисуем их на этом рисунке.
Начнем с ошибки обучения.
Как мы увеличиваем степень полинома, мы будем иметь возможность соответствовать нашему тренировочный набор лучше и лучше, и поэтому, если d равен одному, то есть высокая ошибка обучения, если у нас очень высокая степень полинома, наша обучательная ошибка будет очень низкой, может быть даже 0, потому что будет очень хорошо соответствовать тренировочный набор.
Таким образом, по мере увеличения степени полинома мы обычно обнаруживаем, что обучающая ошибка уменьшается.
Так что я собираюсь написать J индекс поезд theta там, , потому что наша ошибка обучения имеет тенденцию уменьшаться с степень полинома, который мы подходим к данным.
Далее, давайте посмотрим на ошибку перекрестной валидации или на то пошло, , если мы посмотрим на ошибку тестового набора, мы получим довольно похожий результат, как если бы мы построили ошибку перекрестной валидации.
Итак, мы знаем, что если d равен единицу, мы подстраиваем очень простую функцию, и поэтому мы можем быть недооснащением набора обучения, и поэтому это будет очень высокая ошибка перекрестной проверки.
Если мы подойдем к полиному промежуточной степени, у нас было d равно двум в нашем примере на предыдущем слайде, у нас будет гораздо более низкая ошибка перекрестной валидации, потому что мы находим гораздо более подходящее для данных.
И наоборот, если d были слишком высокими.
Поэтому, если d взял на себя, скажем, значение четырех, , то мы снова переподгоняем, и поэтому мы получаем высокое значение для ошибки перекрестной проверки.
Итак, если вы должны были изменить это плавно и построить кривую, вы можете закончить с такой кривой, где это JCV theta.
Опять же, если вы планируете J тест theta, вы получите что-то очень похожее.
Таким образом, этот вид сюжета также помогает нам лучше понять понятия смещения и дисперсии.
Конкретно предположим, что вы применили алгоритм обучения, и он работает не так хорошо, как вы надеетесь, , поэтому, если ваша ошибка перекрестной проверки или ошибка вашего тестового набора высока, , как мы можем выяснить, страдает ли алгоритм обучения от высокой смещения или страдает от высокой дисперсия?
Таким образом, установка ошибки перекрестной валидации высокой соответствует либо этому режиму, либо этому режиму.
Таким образом, этот режим слева соответствует проблеме высокой смещения.
То есть, если вы устанавливаете слишком низкий полином порядка, такой как a d, равен одному, когда нам действительно нужен полином более высокого порядка, чтобы соответствовать данным, тогда как, напротив, этот режим соответствует задаче высокой дисперсии.
То есть, если d степень полинома была слишком большой для набора данных, который мы имеем, и эта цифра дает нам подсказку, как различать эти два случая.
Конкретно, для случая с высоким уклоном , то есть случай недооснащения, то, что мы находим, что и ошибка перекрестной проверки, и ошибка обучения будут высокими.
Итак, если ваш алгоритм страдает от проблемы смещения, ошибка набора обучения будет высокой, и вы можете обнаружить, что ошибка перекрестной проверки также будет высокой.
Это может быть близко, может быть чуть выше, чем ошибка обучения.
Итак, если вы видите эту комбинацию, это признак того, что ваш алгоритм может страдать от высокой смещения.
Напротив, если ваш алгоритм страдает от высокой дисперсии, , то если вы посмотрите здесь, мы заметим, что J train, , что ошибка обучения, будет низкой.
То есть, вы очень хорошо подстраиваете учебный набор, , тогда как ваша ошибка перекрестной проверки, предполагающая, что это, скажем, квадратная ошибка, которую мы пытаемся минимизировать, скажем, , тогда как в отличие от вашей ошибки на множестве перекрестной проверки или вашей кросс-функции или перекрестной проверки будет гораздо больше, чем ошибка набора тренировок.
Итак, это в два раза больше знака.
Это символ карты для гораздо большего, чем обозначается двумя знаками больше.
Итак, если вы видите эту комбинацию значений, , то это подсказка, что ваш алгоритм обучения может страдать от высокой дисперсии и может быть переусердным.
Ключ, который отличает эти два случая, если у вас есть проблема высокой смещения, ваша ошибка набора обучения также будет высокой является ваша гипотеза просто не соответствует тренировочный набор хорошо.
Если у вас проблема с высокой дисперсией, ошибка набора обучения обычно будет низкой, , что намного ниже, чем ошибка перекрестной проверки.
Надеюсь, это дает вам несколько лучшее понимание двух проблем смещения и дисперсии.
У меня еще есть гораздо больше, чтобы сказать о предвзятости и дисперсии в следующих нескольких видео, , но мы увидим позже, что, диагностируя, может ли алгоритм обучения страдать от высокой смещения или высокой дисперсии, я покажу вам еще более подробную информацию о том, как это сделать в последующих видео.
Но мы увидим, что, выяснив, может ли алгоритм обучения быть , страдающий высокой смещением или высокой дисперсией или комбинацией обоих, , что даст нам гораздо лучшее руководство для того, что может быть многообещающим вещи, чтобы попытаться, чтобы улучшить производительность алгоритма обучения.