Model Representation II.
В последнем видео мы дали математическое определение того, как представлять или как вычислять гипотезы, используемые нейронной сетью.
В этом видео мне нравится показать вам, как на самом деле выполнять эти вычисления эффективно, и , который показывает вам реализацию векторного подъема.
И во-вторых, и что более важно, я хочу, чтобы начал давать вам интуицию о , почему эти нейронные сетевые представления могут быть хорошей идеей и как они могут помочь нам выучить сложные нелинейные гипотезы.
Рассмотрим эту нейросеть.
Ранее мы говорили, что последовательность шагов, которые нам нужно для того, чтобы вычислить вывод гипотез это эти уравнения, заданные на, слева, где мы вычисляем stoth значения активации из трех скрытых применений, а затем мы используем те, чтобы вычислить окончательный вывод наших гипотез h из x.
Теперь я собираюсь определить несколько дополнительных терминов.
Итак, этот термин, который я подчеркиваю здесь , я собираюсь определить, что будет z надстрочный индекс 2 1.
Так что у нас есть, что a (2) 1, который этот термин равен g от z до 1.
И кстати , эти надстрочные 2, вы знаете, что это означает, что z2 и это a2 , надстрочный знак 2 в скобках означает, что эти sts являются значениями, связанными со слоем stay 2, то есть со скрытым net-слоем в нейронной сети.
Теперь этот термин здесь Я собираюсь точно определить как z (2) 2.
И, наконец, этот последний термин здесь, который я подчеркиваю, позвольте мне определить это как z (2) 3.
Так что аналогично у нас есть (2) 3 равно g из z (2) 3.
Таким образом, эти значения z - это просто линейная комбинация, взвешенная линейная комбинация входных значений x0, x1, x2, x3, которые идут в определенный нейрон.
Теперь, если вы посмотрите на этот блок чисел, вы можете заметить, что этот блок чисел соответствует подозрительно похожим на операцию матричного вектора, умножение вектора матрицы, умножение вектора h1 раз вектора h.
векторизировать это вычисление нейронной сети.
Конкретно, давайте определим вектор-объект x как обычно как вектор x0, x1, x2, x3, где x0 , как обычно, всегда равен категории 1, и это определяет sto z2 как векторный результат этих z-значений, вы знаете, из z (2) 1 z (2) 2, z (2) 3.
И обратите внимание, что z2 это трехмерный вектор.
Теперь мы можем векторизировать вычисление a (2) 1, a (2) 2, a (2) 3 следующим образом.
Мы можем написать это в два шага.
Мы можем вычислить z2 как theta 1 раз x и что даст нам этот вектор z2; и затем a2 это g z2 и просто , чтобы быть ясным z2 здесь, Это stots является трехмерным вектором и startmation a2 также трехмерным вектором, и, таким образом, это des активации g.
сигмоидная функция поэлементно для каждого элементов z2.
И , кстати, чтобы сделать нашу нотацию немного более согласующейся с , что мы будем делать позже, в этом входном слое у нас есть входы x, но мы должны также сказать, что это sts, как в активациях первых слоев.
Итак, если я определил a1 to быть равным x.
Итак, a1 является вектором, я могу теперь взять этот x здесь и заменить это на z2 равно theta1 bots раз a1, просто определив sts a1 быть активациями в моем входном слое.
Теперь, с тем, что я написал до сих пор, я получил себе значения для a1, a2, a3, и на самом деле , я должен также поместить надписи bood там.
Но мне нужно еще одно значение , которое я также хочу, чтобы это (0) 2 , и это соответствует единице смещения в скрытом слое , который идет на выход там.
Конечно, здесь был блок смещения , который, вы знаете, просто не нарисовал здесь, но чтобы позаботились об этом дополнительном модуле смещения, both то, что мы собираемся сделать, это добавить лишний надстрочный индекс a0 2, hote, равный одному, и после того, как мы сделали этот шаг, мы теперь поняли, что a2 собирается быть четырехмерным вектором , потому что мы только что добавили этот дополнительный, вы знаете, a0, который равен 1, что соответствует измерению единицы смещения в скрытом слое.
И, наконец, , чтобы вычислить фактическое значение наших гипотез, нам тогда просто нужно вычислить z3.
Итак, z3 равен этому термину здесь, что я просто подчеркиваю.
Этот внутренний термин есть z3.
И z3 заявлено 2 раза a2 и, наконец, мои гипотезы вывода h из x, которые это a3, то есть активация моего одного и единственного блока в выходного слоя.
Значит, это просто реальное число.
Вы можете написать его как a3 или как (3) 1, и это g z3.
Этот процесс вычисления h из x также называется форвард распространения и называется, потому что мы начало с активаций входо-единиц, а затем bote мы вроде forward-распространяем это на stot-скрытый слой и вычисляем активации stot-скрытого слоя и то мы вроде форварда распространяем это и вычисляем активации выходного слоя, но этот процесс вычисления активаций от входа, затем скрытый, затем выходной слой, и это также называется форвардом распространения, и то, что мы только что сделали, это stoth, мы просто работали из вектора мудрой реализации этой процедуры .
Итак, если вы реализуете его с помощью этих уравнений , которые у нас есть справа, эти даст вам эффективный способ или оба эффективного способа вычисления h от x.
почему они могут помочь нам узнать интересные нелинейные гипотезы.
Рассмотрим следующую нейросеть и скажем, я скрываю левый путь к этой картине на данный момент.
Если посмотреть на то, что осталось на этой картине.
Это выглядит очень похоже на логистическую регрессию, где мы делаем, это мы используем , что примечание, это просто логистическая регрессионная единица, и мы должны использовать это, чтобы сделать stot-предсказание h от x.
И конкретнее, что гипотезы выводя h-от x идет быть равным g, который - это моя сигмоидная функция активации раз тета 0 раз а0 равно 1 плюс тета 1 плюс тета 2 раз а2 плюс тета тета тета 3 раза а3 ли значения s a1, a2, a3 plus являются те, которые заданы этими тремя заданными единицами.
Теперь, чтобы быть на самом деле последовательным моей ранней нотации.
На самом деле, нам нужно, вы знаете, заполнить эти надстрочные 2 здесь везде , и у меня также есть эти индексы 1 там, потому что у меня есть только одна единица вывода, но если вы сосредоточитесь на синих частях нотации.
Это, вы знаете, это выглядит ужасно как стандартная логистическая регрессионная модель , за исключением того, что у меня теперь есть заглавная тета вместо теты в нижнем регистре.
И то, что это делает , это просто логистическая регрессия.
Но где объекты, подаваемые в логистическую регрессию , это значения , вычисленные скрытым слоем.
Чтобы снова сказать, что эта нейронная сеть делает, это так же, как логистическая регрессия, за исключением , что вместо использования оригинальных функций x1, x2, x3, bit использует эти новые функции a1, a2, a3.
Опять же, мы поставим надстрочные знаки там, вы знаете, чтобы соответствовать нотации.
И крутая вещь об этом, заключается в том, что функции a1, a2, a3, они сами выучили как функции входа.
Конкретно, функция отображения от слоя 1 к слою 2, , которая определяется некоторыми другим набором параметров, тета 1.
Таким образом, это как если бы нейронная сеть , вместо того, чтобы быть ограничена для подачи функций x1, x2, x3 в логистическую регрессию.
Он получает узнать свои собственные особенности, a1, a2, a3, чтобы питаться в логистическую регрессию и как вы можете себе представить в зависимости от того, какие параметры он выбирает для stoth theta 1.
Вы можете узнать некоторые довольно интересные и сложные функции, и, следовательно, вы можете получить лучшие гипотезы, чем если бы вам было ограничено использовать сырые функции x1, x2 или x3, или если b b вы будете ограничивать, чтобы сказать, выбрать многономиальные термины stots, вы знаете, cotmay x1, x2, x3 и так далее.
Но вместо этого, этот алгоритм имеет гибкость, чтобы попытаться , чтобы узнать любые функции сразу, используя эти a1, a2, a3 в , чтобы питаться в эту последнюю единицу, которая, по сути, sts логистическую регрессию здесь.
Я понял этот пример описан как несколько высокий уровень и поэтому Я не уверен, если эта интуиция нейронной сети, вы знаете, имея более сложные функции будет вполне bots иметь смысл еще, но если он еще не в следующем конкретный пример о том, как нейронная сеть может использовать это скрытое там для вычисления более сложных объектов для подачи в этот конечный выходной слой и как это может узнать более сложные гипотезы.
Итак, на случай, если то, что я говорю здесь, не совсем sense, придерживайтесь меня для следующих двух видео и , надеюсь, там, работая через эти примеры, это объяснение будет иметь немного больше смысла.
Но только точка О.
У вас могут быть нейронные сети с другими типами диаграмм, как хорошо, и то, как они связаны между собой, это называется архитектурой.
Таким образом, термин архитектура относится к , как разные нейроны связаны друг с другом.
Это пример другой архитектуры нейронной сети и еще раз вы можете получить эту интуицию о том, как второй слой, bott здесь у нас есть три единицы заголовка, которые вычисляются некоторые сложные функции, возможно, из входного слоя, а затем третий слой может принимать объекты второго слоя и вычислять еще более сложные объекты в третьем слое , так что к тому времени, когда вы получите на выходной слой, слой 4, , вы можете иметь еще больше bd сложных функций того, что stoth вы можете вычислить в слое 3 и поэтому получить очень интересные нелинейные гипотезы.
Кстати, в такой сети , как этот, слой один, это называется входным слоем.
Уровень 4 по-прежнему является нашим выходным слоем, и эта сеть имеет два скрытых слоя.
Таким образом, все, что не является входным слоем или выходным слоем , называется скрытым слоем.
Итак, надеюсь, из этого видео вы получили ощущение , как шаг распространения корма вперед в нейронной сети работает, где вы начинаете с активаций bot-входного слоя и вперед stot-распространяете это на stemet-первый скрытый слой, затем второй скрытый слой, а затем, наконец, выходной слой.
И вы также видели, как мы можем векторизировать эти вычисления.
В следующем я понял , что некоторые интуиции в этом видео о том, как, знаете, другие определенные слои вычисляются сложные особенности ранних слоев.
Я понял, что некоторые из этой интуиции могут быть еще немного абстрактными и вроде как высокого уровня.
И вот, что я хотел бы сделать в двух видео , это работа через подробный пример , как нейронная сеть может быть использована для вычисления нелинейных функций bbd-входа и st-надежды, что даст вам хороший смысл видов сложных нелинейных гипотез, которые мы можем выйти из Нейронных Сети.