Deciding What to Do Next Revisited.
Мы говорили о том, как оценивать алгоритмы обучения , говорили о выборе модели, много говорили о смещении и дисперсии.
Итак, как это помогает нам понять , что потенциально плодотворно, потенциально не плодотворно вещи, чтобы попытаться сделать, чтобы улучшить производительность алгоритма обучения.
Вернемся к нашему оригинальному примеру и пойдем к результату.
Итак, вот наш предыдущий пример о том, что, возможно, соответствует регуляризованной линейной регрессии и обнаруживает, что она не работает так хорошо, как мы надеемся.
Мы сказали, что у нас есть это меню вариантов.
Итак, есть ли способ выяснить, какие из них могут быть плодотворными?
Первое, что все это было получить больше обучающих примеров.
Для чего это хорошо, это помогает исправить высокую дисперсию.
И конкретно, если у вас вместо проблема с высоким уклоном и не имеют проблемы дисперсии, то мы видели в предыдущем видео , что получение больше обучающих примеров, в то время как, возможно, просто не собирается много помочь вообще.
Итак, первый вариант полезен только если вы, скажем, построили кривые обучения и выясните, что у вас есть хотя бы немного дисперсии, что означает, что ошибка перекрестной валидации, вы знаете, sts немного больше, чем ошибка вашего тренировочного набора.
Как насчет того, чтобы попробовать меньший набор функций?
Ну, пытаясь меньший набор функций , это снова что-то, что фиксирует высокую дисперсию.
И другими словами, если вы выясните , глядя на кривые обучения или что-то еще, что вы использовали, , которые имеют высокую проблему смещения ; затем для доброты sakes, не тратьте время, пытаясь тщательно выбрать размер меньшего набора функций для использования.
Потому что, если у вас проблема с высоким уклоном, использование меньшего количества функций не поможет.
В то время как, напротив, если вы посмотрите на кривые обучения или что-то еще вы выясните, что у вас проблема с высокой дисперсией, то, действительно пытается выбрать размер меньшего набора функций, stoth, которые действительно могут быть очень хорошим использованием вашего времени.
Как насчет попытки получить дополнительные функции , добавления функций, как правило, не всегда, но обычно мы думаем об этом как о решении для исправления проблем с высоким уклоном.
Так что, если вы добавляете дополнительные функции , это обычно потому, что ваша текущая гипотеза слишком просто , и поэтому мы хотим, чтобы попытались получить дополнительные функции для того, чтобы наша гипотеза могла лучше соответствовать обучающему набору.
И аналогичным образом, добавление полиномиальных объектов; это еще один способ добавления объектов, и поэтому есть еще один способ попробовать исправить проблему высокого смещения.
И если конкретно, если ваши кривые обучения показывают вам , что у вас все еще есть проблема с высокой дисперсией, то, вы знаете, опять же, это , возможно, менее хорошее использование вашего времени.
И, наконец, уменьшается и увеличивается лямбда.
Это быстро и легко попробовать, Я думаю, что они менее вероятно будут пустой тратой, знаете, многих месяцев вашей жизни.
Но уменьшая лямбда, вы уже знаете, что фиксирует высокий уклон.
В случае, если это не ясно вам, вы знаете, я призываю вас приостановить видео и подумать над этим, что убедит себя, что уменьшение лямбда помогает исправить высокий уклон, в то время как увеличение объема lambda фиксирует высокую дисперсию.
И если вы не уверены, почему это так, сделайте паузу видео и убедитесь, что вы можете убедить себя, что это так.
Или взгляните на кривые , которые мы строили в конце предыдущего видео, и попытайтесь убедиться, что вы понимаете, почему это так.
Наконец, давайте возьмем все , которые мы узнали, и соотносим его обратно к нейросетям и так, вот некоторые практические советы о том, как я обычно выбираю архитектуру или шаблон связи stots нейронных сетей, которые я использую.
Итак, если вы устанавливаете нейросеть, один из вариантов будет соответствовать, скажем, довольно небольшой нейронной сети с вы знаете, относительно мало скрытых единиц, может быть просто обогнать одну скрытую единицу.
Если вы устанавливаете нейронную сеть, один из вариантов будет соответствовать относительно небольшой нейронной сети с, скажем, относительно небольшим, может быть, только одним скрытым слоем и, возможно, только относительно небольшим количеством sts скрытых единиц.
Таким образом, подобная сеть может иметь относительно несколько параметров и быть более склонна к недооснащению.
Основным преимуществом этих небольших нейронных сетей является то, что вычисления будут дешевле.
Альтернативой было бы подгонка, возможно, относительно большой нейронной сети с большим количеством скрытых единиц — есть много скрытых в одном — или с более скрытыми слоями.
Таким образом, эти нейронные сети, как правило, имеют больше параметров и, следовательно, более склонны к переоборудованию.
Один недостаток, часто не основной, но что-то, о чем думать , заключается в том, что если у вас есть большое количество нейронов в вашей сети, то это может быть более вычислительно дорогим.
Хотя в пределах разумного, это часто, надеюсь, не огромная проблема.
Основная потенциальная проблема этих гораздо больших нейронных сетей заключается в том, что они могут быть более склонны к переоборудованию , и оказывается, если вы применяете нейронные сети очень часто используя большую нейросеть часто это на самом деле больше, тем лучше, но если это переоборудование, вы можете затем используйте регуляризацию для адреса переоборудования, обычно используя большую нейросеть, используя регуляризацию для адреса, что часто более эффективно, чем использование небольшой нейронной сети.
И основным возможным недостатком является то, что он может быть более дорогостоящим.
И, наконец, одним из других решений является, скажем, количество скрытых слоев, которые вы хотите иметь, верно?
Итак, вы хотите один скрытый слой или вы хотите три скрытых слоя, как мы показали здесь, или вы хотите два скрытых слоя?
И обычно, как я думаю, что я сказал в предыдущем видео, использование одного скрытого слоя является разумным по умолчанию, но , если вы хотите выбрать количество номеров скрытых слоев, одна вещь, которую вы можете попробовать, это попробуйте тренировать нейронные сети с одним скрытым слоем или двумя скрытыми слоями или тремя скрытыми слоями и посмотреть, какая из этих нейронных сетей лучше всего работает на наборах перекрестной валидации.
Вы берете свои три нейронные сети с одним, двумя и тремя скрытыми слоями и вычисляете ошибку перекрестной валидации в Jcv и все их оценки и используйте это, чтобы stots выбрать, какой из этих средств вы считаете лучшей нейронной сетью.
Итак, это все для смещения и дисперсии и способов , как кривые обучения, которые пытались диагностировать эти проблемы.
Насколько вы думаете , для одного может быть правдивым или не правдивым, чтобы попытаться улучшить производительность алгоритма обучения.
Если вы поняли содержание последних нескольких видео, и если вы применяете их, вы на самом деле быть гораздо более эффективным уже и получение алгоритмов обучения для работы над проблемами и даже большой долей, stots может быть большинство практиков есть машинного обучения здесь в Силиконовая Долина сегодня делает эти вещи в качестве своей полной занятости.
Так что я надеюсь, что эти части советов по опыту в диагностике помогут вам значительно эффективно и мощно применять обучение и оценка заставить их работать очень хорошо.