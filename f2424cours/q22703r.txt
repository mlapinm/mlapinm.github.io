Mathematics Behind Large Margin Classification.В этом видео я хотел бы рассказать вам немного о математике за большой маржинальной классификацией. Это видео является необязательным, поэтому, пожалуйста, не стесняйтесь его пропустить. Это также может дать вам лучшую интуицию о том, как проблема оптимизации для машины поддерживает vex, как это приводит к большим классификаторам маржи. Для того, чтобы начать, позвольте мне сначала напомнить вам о пару свойств того, как выглядят векторные внутренние продукты. Допустим, у меня есть два вектора U и V, которые выглядят так. Таким образом, оба двухмерных вектора. Тогда давайте посмотрим, как выглядит U транспонирование V. И U транспонирование V это также называется внутренними продуктами между векторами U и V. Используйте двухмерный вектор, поэтому я могу на графике его на этом рисунке. Итак, допустим это вектор U. И , что я имею в виду, это , если на горизонтальной оси, что значение принимает любое значение, и на вертикальной оси stoth высота, что является то, что U2 henmet является вторым компонентом stheld вектора U. Теперь, одно количество будет хорошо , чтобы иметь это норма вектора U. Таким образом, эти являются, вы знаете, двойные полосы на слева и справа, что обозначает норму или длину b U. Так что это просто означает; на самом деле высота эвклидова длина вектора carty U. И этот ролик теорема Пифагора просто равно U1 в квадрате плюс U2 квадратный корень, верно? И это длина вектора U. Это реальное число. Просто скажите, что вы знаете, какова длина этого, какова длина этого вектора. Какова длина этой стрелки , которую я только что нарисовал, является нормальным видом? Теперь вернемся и посмотрим на вектор V, потому что мы хотим вычислить внутренний продукт. Таким образом, V будет каким-то другим вектором с, знаете, какое-то значение V1, V2. И так, вектор V будет выглядеть так, к V, как это. Теперь вернемся к и посмотрим, как вычислить внутренний продукт между U и V. Вот как вы можете это сделать. Позвольте мне взять вектор V и проецировать его вниз на вектор U. Поэтому я собираюсь взять ортогональную проекцию или проекцию 90 градусов, и проецировать его вниз на U вот так. И то, что я собираюсь сделать измерить длину этой красной линии, которую я только что нарисовал здесь. Итак, я назову длину , что красная линия P. Итак, P это длина или есть величина проекции вектора V на вектор bet U. Позвольте мне просто записать это. Итак, P - это длина проекции вектора V на вектор U. И это возможно, чтобы показать, что единичный продукт bit U транспонировать V, что stoth это будет равным stormal p, умноженное на норму, или длину heth вектора U. Таким образом, это hucs является одним из способов вычисления внутренний продукт. И если вы действительно сделаете , геометрия выясните, что такое P и выясните, что такое норма U. Это должно дать вам тот же путь , тот же ответ, что и другой способ вычислительной единицы продукта. Вправо. То есть, если вы берете U транспонирование V, то U транспонирует этот U1 U2, его один на две матрицы, 1 раз V. И поэтому это должно на самом деле дать вам sts U1, V1 плюс U2, V2. И так теорема линейной алгебры , что эти две формулы дают вам один и тот же ответ. И, кстати, U транспонирование V также равно V транспонирование U. Поэтому, если вы должны были сделать тот же процесс в обратном направлении, вместо того, чтобы проецировать V на U, вы могли бы проецировать sto U на V. Тогда, знаете, сделать то же самое, но со строками U и V обратными. И вы бы на самом деле, вы должны на самом деле получить то же число, какое бы это ни было. И просто чтобы прояснить, что происходит в этом уравнении, норма U является реальным числом , а P — реальным числом. И поэтому U транспонирование V является регулярное умножение как два вещественных числа длина P умножает нормальное представление. Только одна последняя деталь, которая если посмотреть на норму P, P фактически подписано так справа. И это может быть как положительным, так и отрицательным. Позвольте мне сказать, что я имею в виду , если U является вектором, который выглядит как это, а V - вектор, который выглядит так. Так что, если угол между U и V больше девяносто градусов. Затем, если я проецирую V на U, то, что я получаю это проекция, которая выглядит как это и так, что длина P. И в этом случае у меня все равно будет stoth, который U транспонирует V равно P, умноженное на норму U. За исключением этот пример P будет отрицательным. Итак, вы знаете, во внутренних продуктах, если угол между U и V меньше , чем девяносто градусов, то P является положительной длиной для этой красной линии , тогда как если угол этого угла both здесь больше, чем 90 градусов, то P stooter здесь будет отрицательным от супер линия этого маленького сегмента линии прямо там. Таким образом, внутреннее произведение между двумя векторами также может быть отрицательным , если угол между ними больше 90 градусов. Так вот как работают векторные внутренние продукты . Мы собираемся использовать эти свойства vector внутреннего продукта, чтобы попытаться понять цель оптимизации векторной машины поддержки . Здесь является целью оптимизации для векторной машины , которую мы разработали ранее. Просто для цель этого слайда я собираюсь сделать одно упрощение или один раз, чтобы сделать цель легкой для анализа и то, что я собираюсь сделать, это игнорировать indeceptrums. Итак, мы просто проигнорируем theta 0 и установим, что равно 0. Чтобы упростить построение, я также собираюсь установить N количество объектов, равное 2. Итак, у нас есть только 2 функции, X1 и X2. Теперь давайте посмотрим на объективную функцию. Цель оптимизации для SVM. То, что у нас есть только две функции. Когда N равно 2. Это можно написать, половину тета один квадрат плюс тета два квадрата. Потому что у нас есть только два параметра, тета один и thetaa два. То, что я собираюсь сделать, это немного переписать. Я собираюсь написать это как одна половина тета один в квадрате плюс тета два квадрата и квадратный корень в квадрате. И причина, по которой я могу это сделать, , заключается в том, что для любого числа, вы знаете, W, правильно, квадратные корни W и затем квадратные, это просто равно W. Таким образом, квадратные корни и квадраты должны дать вам одно и то же. Вы можете заметить, что этот термин внутри, это равен норме или длине вектора тета и то, что я имею в виду, что stots, если мы выписываем вектор-тета, как это, как будто вы знаете theta один, theta два. Тогда этот термин, который я только что подчеркнул красным, это точно длина, или норма, вектора тета. Мы называем определение нормы вектора, которая у нас есть на предыдущей строке. И на самом деле это фактически равно длине вектора тета, пишете ли вы это как тета нуль, тета 1, тета 2. То есть, если тета нуль равен нулю, как я предполагаю здесь. Или просто длина theta 1, theta 2; но для эта строка я буду игнорировать theta 0. Так что позвольте мне просто, вы знаете, рассматривать theta как это, позвольте мне просто написать theta, нормальный theta, как это theta 1, theta 2 только, но math bet работает в любом случае, sts ли мы включаем theta zero здесь или нет. Так что это не будет иметь значения для остальной части нашего вывода. И вот, наконец, это означает , что моя цель оптимизации равна половине нормы в квадрате. Таким образом, вся поддерживающая векторная машина делает в оптимизации цель заключается в минимизации нормы в квадрате длины параметра вектора тета. Теперь то, что я хотел бы сделать , это посмотреть на эти термины, theta транспонировать X и лучше понять, что они делают. Итак, учитывая параметр вектора тета и заданный и пример x, что это равно? И на предыдущем слайде, мы выяснили, как выглядит U транспонирование V, с разными векторами U и V. И поэтому мы должны взять эти определения, вы знаете, с theta и X (i) играя роль stots U и V. и давайте посмотрим, как выглядит эта картина. Итак, предположим, я заговор. Допустим, я смотрю на только один пример обучения. Предположим, что у меня есть положительный пример, рисунок был там, и скажем, что мой пример X (i), что , что действительно означает, что построено на горизонтальной оси, по некоторому значению X (i) 1, а по вертикальной оси placet X (i) 2. Вот как я рисую свои обучающие примеры. И хотя мы не были на самом деле думать об этом как о векторе, что это на самом деле, это вектор из происхождения от 0, 0 до того, как обстоит место этого обучающего примера. А теперь допустим, что у нас есть вектор параметров и Я собираюсь построить , что как вектор, а также. То, что я имею в виду, если я рисую theta 1 здесь и theta 2 там так что внутренний продукт theta транспонировать X (i). Используя наш более ранний метод, способ, который мы вычисляем, что мы возьмем мой пример и проецируем его на мой векторный параметр theta. И тогда я собираюсь посмотреть по длине этого сегмента , который я раскрашиваю, в красный цвет. И я собираюсь называть надстрочный индекс P I , чтобы обозначить, что это проекция i-го учебного примера на векторный параметр тета. И так что у нас есть , что тета транспонирование X (i) равно тому, что у нас есть на предыдущем слайде, это будет равно b p умножить длину stots нормы вектора theta. И это, конечно же, также равно тета 1 x1 плюс тета 2 x2. Таким образом, каждый из них, вы знаете, одинаково действительный способ вычисления внутреннего продукта между тетой и X (i). Хорошо. Так где это нас оставит? Это означает, что этот ограничивает, что theta транспонирование X (i) будет больше или равно одному или меньше минус один. Это означает, что он может заменить использование ограничений , что P (i) умножить X больше или равно единице. Поскольку тета транспонирование X (i) равно , равному P (i), умноженное на норму теты. Так что напишу это в нашу цель оптимизации. Это то, что мы получаем , где у меня есть, вместо тета транспонировать X (i), я теперь это P (i) умножить норму теты. И просто, чтобы напомнить вам, мы тоже работали раньше, что эта цель оптимизации может быть написана как в полтора раза норму теты в квадрате. Итак, теперь давайте рассмотрим пример обучения, который мы имеем внизу и на данный момент, продолжая использовать упрощение, которое тета тета 0 равно 0. Посмотрим, какую границу решения выберет машина опорных векторов. Вот один из вариантов, скажем машина вектора поддержки должна была выбрать эту границу решения. Это не очень хороший выбор, потому что он имеет очень небольшие поля. Эта граница решения очень близка к примерам обучения. Посмотрим, почему машина вектора поддержки не сделает этого. Для этого выбора параметров можно показать, что вектор параметра тета на самом деле составляет на 90 градусов до границы решения. И так, эта зеленая граница решения соответствует вектору параметров тета, который указывает в этом направлении. И кстати, упрощение того, что тета 0 равно 0, что просто означает, что граница решения должна пройти через начало происхождения, (0,0) вон там. Итак, теперь давайте посмотрим, что это подразумевает для цели оптимизации. Предположим, что этот пример здесь. Допустим, это мой первый пример, вы знаете, X1. Если мы посмотрим на проекцию этого примера на мои параметры theta. Это проекция. И так, что маленький сегмент красной линии. Это равно P1. И это будет довольно маленьким, верно. И аналогичным образом, если этот пример здесь, если это произойдет , чтобы быть X2, это мой второй пример. Затем, если я взгляну на проекцию этого примера на тету. Ты знаешь. Тогда позвольте мне нарисовать этот пурпурным цветом. Этот маленький пурпурный сегмент линии, это будет P2. Это проекция второго примера на мой, на направление моего векторного параметра тета, который идет вот так. И так, этот маленький сегмент линии проекции становится довольно маленьким. P2 фактически будет отрицательным числом, так что P2 является в противоположном направлении. Этот вектор имеет больше , чем 90 градусов угол с моим параметром vector theta, он будет меньше 0. И вот что мы находим, это то, что эти термины P (i) будут довольно маленькими числами. Итак, если мы посмотрим на цель оптимизации и посмотрим, ну, для положительных примеров нам нужно P (i) раз норма теты быть больше, чем любой из них. Но если P (i) над здесь, если P1 здесь довольно мал, это означает, что , что нам нужно, чтобы норма тета тета была довольно большой, верно? Если P1 theta мала , и мы хотим, чтобы P1 вы знаете раз во всех theta , чтобы быть больше, чем один, ну единственный способ для этого, чтобы быть истинным для прибыли, что sto эти два числа, чтобы быть большим, если P1 мал, как мы говорим, мы хотим, чтобы норма theta была большой. И аналогичным образом для нашего негативного примера нам нужен P2 умноженное на норму тета, чтобы быть меньше или равно минус единице. И мы видели в этом примере уже, что P2 идет довольно маленькое отрицательное число, и поэтому единственный способ для , что также произойдет, - это, чтобы норма theta была stots большой, но то, что мы делаем в оптимизации цель проекта, мы должны попытаться найти параметров, где норма тета мала, и поэтому вы знаете, так что это не кажется таким хорошим направлением для вектора параметров и тета. В отличие от этого, просто посмотрите на другую границу решения. Здесь, скажем, этот SVM выбирает эту границу решения. Теперь будет очень по-другому. Если это граница решения, здесь соответствующее направление для тета. Итак, с направлением границы вы знаете, что вертикальная линия, которая соответствует к этому можно, чтобы показать, используя линейную алгебру, что both способ получить, что зеленое решение hots границы имеет вектор theta быть starmet на 90 градусов к нему, wetange и теперь, если вы посмотрите на проекции ваших данных на вектор x, скажем, его до этот пример - мой пример x1. Поэтому, когда я проецирую это на x, или на theta, я нахожу, что это P1. Эта длина есть P1. Другой пример, этот пример , и я делаю ту же проекцию и то, что я нахожу, что эта длина здесь - это та же самая оценка P2, которая будет меньше 0. И вы заметили, что теперь P1 и P2, эти длины проекций собираются в быть гораздо больше, и поэтому , если мы все еще должны обеспечить bets эти ограничения, что P1 из stots норма theta является фазовым потоком номер один, потому что P1 намного больше сейчас. Нормаль может быть меньше. И так, это означает , что, выбрав решение границу, показанную на правой , а не слева, SVM может сделать норму bett параметров theta stots гораздо меньше. Итак, если мы можем сделать норму theta меньше и , поэтому сделайте квадрат норму тета меньше, что является , почему SVM выбрал бы эту гипотезу справа. Вот как SVM порождает к этому большому эффекту сертификации маржи. В основном, если вы посмотрите на эту зеленую линию, если вы посмотрите на эту зеленую гипотезу , мы хотим, чтобы проекции моих положительных и отрицательных примеров на тету были большими, и, по его мнению, единственный способ, чтобы он был истинным, это если окружать зеленую линию. Есть такой большой запас, есть этот большой разрыв, который отделяет положительные и отрицательные примеры, это действительно величина этого разрыва. Величина этой маржи — это именно значения P1, P2, P3 и так далее. И поэтому, сделав маржу большой, этими tyros P1, P2, P3 и так далее, это SVM может в конечном итоге с меньшим значением для нормы botm theta, что это то, что он пытается сделать в цели. И вот почему эта машина заканчивается с увеличающимися классификаторами поля , потому что itss пытается максимизировать норму этих P1, которая является расстоянием от примеров обучения до границы решения. Наконец, мы сделали всю эту деривацию , используя это упрощение, что параметр theta 0 должен быть равен 0. Эффект от этого, как я кратко упомянул, заключается в том, что если тета 0 равно 0, что это означает, что является то, что мы занимаем границы решения, которые проходят через stot-истоки границ решения проходят через theta происхождения, как это, если вы можете разрешить theta ноль быть не 0 то, что это означает, что вы развлекаете пределы решения, которые не пересекают происхождение, как тот, который я только что нарисовал. И я не собираюсь делать полный вывод, что. Получается , что это же самое большая маржа доказательство работает в почти точно так же. И есть обобщение этого аргумента , который мы только что прошли через их давным-давно, что показывает , что даже когда theta 0 не 0, то, что SVM пытается сделать, когда у вас есть цель оптимизации. Который снова соответствует случаю , когда C очень велик. Но можно, чтобы показать, что, вы знаете, когда тета не равно 0, эта векторная машина поддержки все еще находит действительно пытается найти большой разделитель поля, который между положительными и отрицательными примерами. Так что объясняет, как эта машина векторов поддержки является большим классификатором полей. В следующем видео мы начнем говорить о том, как взять некоторые из этих идей SVM и начать применять их для построения сложных bit нелинейных классификаторов.