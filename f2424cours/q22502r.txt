Backpropagation Algorithm.
В предыдущем видео мы говорили о функции стоимости для нейросети.
В этом видео давайте начнем говорить о алгоритме, для попытки минимизировать функцию затрат.
В частности, поговорим о алгоритме обратного распространения.
Вот функция стоимости, которую мы записали в предыдущем видео.
То, что мы хотели бы сделать, это попытаться найти параметры theta , чтобы попытаться свести к минимуму j theta.
Для того, чтобы использовать либо градиентный спуск , либо один из передовых алгоритмов оптимизации.
То, что нам нужно сделать, это написать код, который принимает этот ввод параметров theta и вычисляет j из theta и эти частичные производные термины.
Помните, что параметры в нейронной сети этих вещей, theta superscript l seprpt ij, это реальное число и так, это частичные производные термины, которые нам нужно вычислить.
Для того, чтобы вычислить функцию стоимости j theta, мы просто используем эту формулу здесь и так, то, что я хочу сделать для большей части этого видео, - это забота о том, как мы можем вычислить эти частичные производные термины.
Давайте начнем с разговора о случай, когда у нас есть только один пример обучения, так представьте, если вы хотите, что весь наш обучающий набор включает только один пример тренировок, который представляет собой пару xy.
Я не собираюсь писать x1y1 просто напишу это.
Напишите один обучающий пример как xy и давайте проведем последовательность вычислений мы будем делать с этим одним обучающим примером.
Первое, что мы делаем, это , мы применяем форвардное распространение в , чтобы вычислить, действительно ли гипотезы выводит заданный вход.
Конкретно называется a (1) - это значения активации этого первого слоя, который был входным.
Итак, я собираюсь установить это на x , а затем мы собираемся вычислить z (2) равно theta (1) a (1) и a (2) равно g, функция активации сигмоида применяется к z (2) , и это даст нам наши активации stots для первого среднего слоя.
То есть для второго уровня сети , и мы также добавляем эти термины смещения.
Далее мы применяем еще 2 шага из этой четырех и распространение для вычисления a (3) и a (4) , который также является вверх гипотезы h из x.
Итак, это наша векторизованная реализация stoth вперед распространения store, и это позволяет нам вычислить значения активации, предназначенные для всех нейронов в нашей нейронной сети.
Далее, чтобы вычислить производные, мы будем использовать алгоритм, называемый обратным распространением.
Интуиция алгоритма обратного распространения заключается в том, что для каждой ноты мы будем вычислять термин дельта надстрочный индекс j , который будет каким-то образом b представлять ошибку stote j в слое l.
делает активацию j единицы в слое l и так, этот дельта термин в некотором смысле собирается захватить нашу ошибку при активации этого нейронного дуэта.
Итак, как мы могли бы пожелать, чтобы активация этой ноты немного отличалась.
Конкретно, взяв пример нейросеть, что у нас есть справа, которая имеет четыре слоя.
И поэтому заглавная L равна 4.
Для каждой единицы вывода мы собираемся вычислить этот дельта термин.
Итак, дельта для j единицы в четвертом слое равна только активации этой единицы минус то, что было фактическое значение 0 в нашем примере обучения.
Итак, этот термин здесь может также быть записан h из x подстрочный j, правильно.
Таким образом, этот дельта термин просто разница между выходами гипотез и тем, что было значением y в нашем тренировочном наборе, тогда как bet y индекс j - это sps j элемента вектора вектора вектора y в нашем помеченном наборе тренировок.
И кстати, если вы думаете о дельте a и y в качестве векторов, то вы можете также взять их и придумать с векторизованной реализацией bott, который просто sts delta 4 устанавливается как stay a4 минус y.
вектор, размер равен количеству выходных единиц в нашей сети.
Итак, теперь мы вычислили дельта 4 термина эпохи для нашей сети.
Далее мы вычисляем дельта-условия для более ранних слоев в нашей сети.
Вот формула для вычисления дельты 3 это дельта 3 равно theta 3 транспонировать раз дельта 4.
И это точка раз, это - это операция умножения элемента y , которую мы знаем из MATLAB.
Таким образом, дельта 3 транспонировать дельта 4, это вектор; g prime z3, который также является вектором , и поэтому время точки - это в умножении элемента y между этими двумя векторами.
Этот термин g prime из z3, что формально на самом деле производная активации функции g оценивается при входные значения, заданные z3.
Если вы знаете исчисление, вы можете попробовать его самостоятельно и увидеть, что вы можете упростить его до того же ответа, который я получаю.
Но я просто расскажу вам прагматично, что это значит.
То, что вы делаете, чтобы вычислить этот g prime, эти производные термины просто a3 точка times1 минус A3 где A3 является вектор активаций.
1 — вектор единиц, а A3 — снова активация вектор значений активации для этого слоя.
Далее вы применяете аналогичную формулу для вычисления дельты 2 , где снова можно вычислить с использованием аналогичной формулы.
Только сейчас это a2 как так и я затем доказать это здесь, но вы может на самом деле, это возможно, чтобы доказать это, если вы знаете исчисление, что это выражение равно sto математически, производная от here функции g функции активации hent-function, которую я обозначаю haven g Прайм.
И, наконец, вот и все, и есть нет термина delta1, потому что первый слой соответствует входному слою , и это просто функция , которую мы наблюдали в наших наборах тренировок, так что не имеет никакой ошибки, связанной с этим.
Это не похоже на то, что мы действительно не хотим пытаться изменить эти значения.
И поэтому у нас есть дельта условия только для слоев 2, 3 и для этого примера.
Распространение обратного имени происходит от тот факт, что мы начинаем с вычисляя термин дельта для выходного слоя, а затем мы возвращаемся к слою и вычисляем дельта-термины для третьего скрытого слоя, а затем мы возвращаемся назад еще один шаг, чтобы вычислить дельта-2 и так, мы вид обратно, распространяющий ошибки из выходного слоя на слой 3 к их, следовательно, к усложнению с именем.
Наконец, деривация удивительно сложна, удивительно вовлечена, но если вы просто сделаете это несколько шагов шаги вычисления, это возможно , чтобы доказать вирусный откровенно некоторые , что сложное математическое доказательство.
Можно доказать, что если вы игнорируете авторизацию, то частичные производные условия, которые вы хотите, точно задаются активациями и этими дельта терминами.
Это игнорирует лямбда или альтернативно регуляризация термин лямбда будет равен 0.
Мы исправим эту деталь позже о термине регуляризации, но так, выполнив обратную распространение и вычисляя эти дельта-термины, вы можете, знаете, довольно bots быстро вычислить эти частичные производные термины stots для всех ваших параметров.
Так что это много деталей.
Давайте возьмем все и положим все вместе, чтобы поговорить о , как реализовать обратную распространение для вычисления производных относительно ваших параметров.
И в случае, когда у нас есть большой тренировочный набор , а не только тренировочный набор из одного примера, вот что мы делаем.
Предположим, что у нас есть обучающий набор примеров m, таких как , который показан здесь.
Первое, что мы собираемся сделать, это мы собираемся установить эти дельта l индекс i j.
Таким образом, этот треугольный символ?
Это на самом деле столица греческий алфавит дельта.
Символом, который мы имели на предыдущем слайде, была дельта нижнего регистра.
Таким образом, треугольник - столичная дельта.
Мы собираемся установить это равным нулю для всех значений l i j.
В конце концов, эта капитальная дельта l i j будет использоваться для вычисления частичного дериватива, частичного дериватива от theta l i j of theta.
Как мы увидим в секунду, эти дельты собираются для использования в качестве аккумуляторов, которые будет медленно добавлять вещи в для вычисления этих частичных производных.
Далее мы собираемся пройти наш тренировочный набор.
Итак, мы скажем для i равно 1 через m и поэтому для итерации i, мы будем работать с обучающим примером xi, yi.
Итак первое, что мы собираемся сделать , это установить a1, который является активациями входного слоя, установить, что должно быть равным категории xi - это входы для нашего обучающего примера, а затем hote мы собираемся выполнить вперед распространение для heth вычислить активации для heth слоя два , третий слой и так далее до последнего слоя , слой капитал L.
Далее, мы будем использовать выходной ярлык yi из этого конкретного примера , мы ищем в , чтобы вычислить термин «ошибка» для дельты L для вывода там.
Итак, дельта L это то, что гипотезы вывода минус что целевой метка была?
И тогда мы будем использовать алгоритм обратного распространения для вычислить дельта L минус 1, дельта L минус 2, и так далее до дельты 2 и еще раз both есть теперь дельта 1, потому что sts мы не связываем термин ошибки с входным слоем.
И, наконец, мы собираемся использовать эти капитальные дельта-термины , чтобы накапливать эти частичные производные условия , которые мы записали в предыдущей строке.
И, кстати, если посмотреть на это выражение , то это тоже можно векторизировать.
Конкретно, если вы думаете delta ij как матрица, индексированная индексом ij.
Затем, если дельта L является матрицей, мы можем переписать это как дельта L, получает обновляется как дельта L плюс нижний регистр дельта L плюс один раз транспонирование AL.
Так что это векторизованная реализация , которая автоматически делает обновление для всех значений i и j.
Наконец, после того, как выполняет тело boot четыре цикла, мы выходим за пределы четырехпетлевого stoop и вычисляем следующее.
Мы вычисляем капитал D следующим образом и у нас есть два отдельных случая для j равен нулю и j не равен нулю.
Случай j равен нулю соответствует смещению термин, поэтому когда j равен нулю, поэтому мы не имеем является дополнительным термином регуляризации.
Наконец, в то время как формальное доказательство довольно сложно то, что вы можете показать, что после того, как вы вычислили эти термины D, , что является именно частичной производной от функции ost стоимости по отношению к каждому периметру, и поэтому вы можете использовать их в любом градиенте спуск или в одном из продвинутых алгоритмов авторизации Итак, это алгоритм обратного распространения и как вы вычисляете производные вашей функции стоимости для нейронной сети.
Я знаю, что это похоже на то, что было много деталей, и это было много шагов, низанных вместе.
Но оба в программировании назначения выписывают и позже в этом видео, мы дадим вам резюме этого, так что мы можем иметь все части abmet-алгоритма вместе, так что stot-вы точно знаете, что вам нужно promet-реализовать, если вы хотите, чтобы реализовать обратное распространение для вычисления производных функции стоимости вашей нейросети относительно этих параметров.