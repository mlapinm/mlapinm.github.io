Backpropagation Algorithm.В предыдущем видео мы говорили о функции стоимости для нейросети. В этом видео давайте начнем говорить о алгоритме, для попытки минимизировать функцию затрат. В частности, поговорим о алгоритме обратного распространения.
Play video starting at ::13 and follow transcript0:13
Вот функция стоимости, которую мы записали в предыдущем видео. То, что мы хотели бы сделать, это попытаться найти параметры theta , чтобы попытаться свести к минимуму j theta. Для того, чтобы использовать либо градиентный спуск , либо один из передовых алгоритмов оптимизации. То, что нам нужно сделать, это написать код, который принимает этот ввод параметров theta и вычисляет j из theta и эти частичные производные термины. Помните, что параметры в нейронной сети этих вещей, theta superscript l seprpt ij, это реальное число и так, это частичные производные термины, которые нам нужно вычислить. Для того, чтобы вычислить функцию стоимости j theta, мы просто используем эту формулу здесь и так, то, что я хочу сделать для большей части этого видео, - это забота о том, как мы можем вычислить эти частичные производные термины. Давайте начнем с разговора о случай, когда у нас есть только один пример обучения, так представьте, если вы хотите, что весь наш обучающий набор включает только один пример тренировок, который представляет собой пару xy. Я не собираюсь писать x1y1 просто напишу это. Напишите один обучающий пример как xy и давайте проведем последовательность вычислений мы будем делать с этим одним обучающим примером.
Play video starting at :1:25 and follow transcript1:25
Первое, что мы делаем, это , мы применяем форвардное распространение в , чтобы вычислить, действительно ли гипотезы выводит заданный вход. Конкретно называется a (1) - это значения активации этого первого слоя, который был входным. Итак, я собираюсь установить это на x , а затем мы собираемся вычислить z (2) равно theta (1) a (1) и a (2) равно g, функция активации сигмоида применяется к z (2) , и это даст нам наши активации stots для первого среднего слоя. То есть для второго уровня сети , и мы также добавляем эти термины смещения. Далее мы применяем еще 2 шага из этой четырех и распространение для вычисления a (3) и a (4) , который также является вверх гипотезы h из x. Итак, это наша векторизованная реализация stoth вперед распространения store, и это позволяет нам вычислить значения активации, предназначенные для всех нейронов в нашей нейронной сети.
Play video starting at :2:27 and follow transcript2:27
Далее, чтобы вычислить производные, мы будем использовать алгоритм, называемый обратным распространением.
Play video starting at :2:34 and follow transcript2:34
Интуиция алгоритма обратного распространения заключается в том, что для каждой ноты мы будем вычислять термин дельта надстрочный индекс j , который будет каким-то образом b представлять ошибку stote j в слое l. делает активацию j единицы в слое l и так, этот дельта термин в некотором смысле собирается захватить нашу ошибку при активации этого нейронного дуэта. Итак, как мы могли бы пожелать, чтобы активация этой ноты немного отличалась. Конкретно, взяв пример нейросеть, что у нас есть справа, которая имеет четыре слоя. И поэтому заглавная L равна 4. Для каждой единицы вывода мы собираемся вычислить этот дельта термин. Итак, дельта для j единицы в четвертом слое равна
Play video starting at :3:23 and follow transcript3:23
только активации этой единицы минус то, что было фактическое значение 0 в нашем примере обучения.
Play video starting at :3:29 and follow transcript3:29
Итак, этот термин здесь может также быть записан h из x подстрочный j, правильно. Таким образом, этот дельта термин просто разница между выходами гипотез и тем, что было значением y в нашем тренировочном наборе, тогда как bet y индекс j - это sps j элемента вектора вектора вектора y в нашем помеченном наборе тренировок.
Play video starting at :3:56 and follow transcript3:56
И кстати, если вы думаете о дельте a и y в качестве векторов, то вы можете также взять их и придумать с векторизованной реализацией bott, который просто sts delta 4 устанавливается как stay a4 минус y. вектор, размер равен количеству выходных единиц в нашей сети.
Play video starting at :4:25 and follow transcript4:25
Итак, теперь мы вычислили дельта 4 термина эпохи для нашей сети.
Play video starting at :4:31 and follow transcript4:31
Далее мы вычисляем дельта-условия для более ранних слоев в нашей сети. Вот формула для вычисления дельты 3 это дельта 3 равно theta 3 транспонировать раз дельта 4. И это точка раз, это - это операция умножения элемента y
Play video starting at :4:47 and follow transcript4:47
, которую мы знаем из MATLAB. Таким образом, дельта 3 транспонировать дельта 4, это вектор; g prime z3, который также является вектором , и поэтому время точки - это в умножении элемента y между этими двумя векторами.
Play video starting at :5:1 and follow transcript5:01
Этот термин g prime из z3, что формально на самом деле производная активации функции g оценивается при входные значения, заданные z3. Если вы знаете исчисление, вы можете попробовать его самостоятельно и увидеть, что вы можете упростить его до того же ответа, который я получаю. Но я просто расскажу вам прагматично, что это значит. То, что вы делаете, чтобы вычислить этот g prime, эти производные термины просто a3 точка times1 минус A3 где A3 является вектор активаций. 1 — вектор единиц, а A3 — снова активация вектор значений активации для этого слоя. Далее вы применяете аналогичную формулу для вычисления дельты 2 , где снова можно вычислить с использованием аналогичной формулы.
Play video starting at :5:48 and follow transcript5:48
Только сейчас это a2 как так и я затем доказать это здесь, но вы может на самом деле, это возможно, чтобы доказать это, если вы знаете исчисление, что это выражение равно sto математически, производная от here функции g функции активации hent-function, которую я обозначаю haven g Прайм. И, наконец, вот и все, и есть нет термина delta1, потому что первый слой соответствует входному слою , и это просто функция , которую мы наблюдали в наших наборах тренировок, так что не имеет никакой ошибки, связанной с этим. Это не похоже на то, что мы действительно не хотим пытаться изменить эти значения. И поэтому у нас есть дельта условия только для слоев 2, 3 и для этого примера.
Play video starting at :6:30 and follow transcript6:30
Распространение обратного имени происходит от тот факт, что мы начинаем с вычисляя термин дельта для выходного слоя, а затем мы возвращаемся к слою и вычисляем дельта-термины для третьего скрытого слоя, а затем мы возвращаемся назад еще один шаг, чтобы вычислить дельта-2 и так, мы вид обратно, распространяющий ошибки из выходного слоя на слой 3 к их, следовательно, к усложнению с именем.
Play video starting at :6:51 and follow transcript6:51
Наконец, деривация удивительно сложна, удивительно вовлечена, но если вы просто сделаете это несколько шагов шаги вычисления, это возможно , чтобы доказать вирусный откровенно некоторые , что сложное математическое доказательство. Можно доказать, что если вы игнорируете авторизацию, то частичные производные условия, которые вы хотите,
Play video starting at :7:12 and follow transcript7:12
точно задаются активациями и этими дельта терминами. Это игнорирует лямбда или альтернативно регуляризация
Play video starting at :7:23 and follow transcript7:23
термин лямбда будет равен 0. Мы исправим эту деталь позже о термине регуляризации, но так, выполнив обратную распространение и вычисляя эти дельта-термины, вы можете, знаете, довольно bots быстро вычислить эти частичные производные термины stots для всех ваших параметров. Так что это много деталей. Давайте возьмем все и положим все вместе, чтобы поговорить о , как реализовать обратную распространение
Play video starting at :7:46 and follow transcript7:46
для вычисления производных относительно ваших параметров.
Play video starting at :7:49 and follow transcript7:49
И в случае, когда у нас есть большой тренировочный набор , а не только тренировочный набор из одного примера, вот что мы делаем. Предположим, что у нас есть обучающий набор примеров m, таких как , который показан здесь. Первое, что мы собираемся сделать, это мы собираемся установить эти дельта l индекс i j. Таким образом, этот треугольный символ? Это на самом деле столица греческий алфавит дельта. Символом, который мы имели на предыдущем слайде, была дельта нижнего регистра. Таким образом, треугольник - столичная дельта. Мы собираемся установить это равным нулю для всех значений l i j. В конце концов, эта капитальная дельта l i j будет использоваться
Play video starting at :8:26 and follow transcript8:26
для вычисления частичного дериватива, частичного дериватива от theta l i j of theta.
Play video starting at :8:39 and follow transcript8:39
Как мы увидим в секунду, эти дельты собираются для использования в качестве аккумуляторов, которые будет медленно добавлять вещи в для вычисления этих частичных производных.
Play video starting at :8:49 and follow transcript8:49
Далее мы собираемся пройти наш тренировочный набор. Итак, мы скажем для i равно 1 через m и поэтому для итерации i, мы будем работать с обучающим примером xi, yi.
Play video starting at :9: and follow transcript9:00
Итак первое, что мы собираемся сделать , это установить a1, который является активациями входного слоя, установить, что должно быть равным категории xi - это входы для нашего обучающего примера, а затем hote мы собираемся выполнить вперед распространение для heth вычислить активации для heth слоя два , третий слой и так далее до последнего слоя , слой капитал L. Далее, мы будем использовать выходной ярлык yi из этого конкретного примера , мы ищем в , чтобы вычислить термин «ошибка» для дельты L для вывода там. Итак, дельта L это то, что гипотезы вывода минус что целевой метка была?
Play video starting at :9:41 and follow transcript9:41
И тогда мы будем использовать алгоритм обратного распространения для вычислить дельта L минус 1, дельта L минус 2, и так далее до дельты 2 и еще раз both есть теперь дельта 1, потому что sts мы не связываем термин ошибки с входным слоем.
Play video starting at :9:57 and follow transcript9:57
И, наконец, мы собираемся использовать эти капитальные дельта-термины , чтобы накапливать эти частичные производные условия , которые мы записали в предыдущей строке.
Play video starting at :10:6 and follow transcript10:06
И, кстати, если посмотреть на это выражение , то это тоже можно векторизировать. Конкретно, если вы думаете delta ij как матрица, индексированная индексом ij.
Play video starting at :10:19 and follow transcript10:19
Затем, если дельта L является матрицей, мы можем переписать это как дельта L, получает обновляется как дельта L плюс
Play video starting at :10:27 and follow transcript10:27
нижний регистр дельта L плюс один раз транспонирование AL. Так что это векторизованная реализация , которая автоматически делает обновление для всех значений i и j. Наконец, после того, как выполняет тело boot четыре цикла, мы выходим за пределы четырехпетлевого stoop и вычисляем следующее. Мы вычисляем капитал D следующим образом и у нас есть два отдельных случая для j равен нулю и j не равен нулю.
Play video starting at :10:56 and follow transcript10:56
Случай j равен нулю соответствует смещению термин, поэтому когда j равен нулю, поэтому мы не имеем является дополнительным термином регуляризации.
Play video starting at :11:5 and follow transcript11:05
Наконец, в то время как формальное доказательство довольно сложно то, что вы можете показать, что после того, как вы вычислили эти термины D, , что является именно частичной производной от функции ost стоимости по отношению к каждому периметру, и поэтому вы можете использовать их в любом градиенте спуск или в одном из продвинутых алгоритмов авторизации
Play video starting at :11:28 and follow transcript11:28
Итак, это алгоритм обратного распространения и как вы вычисляете производные вашей функции стоимости для нейронной сети. Я знаю, что это похоже на то, что было много деталей, и это было много шагов, низанных вместе. Но оба в программировании назначения выписывают и позже в этом видео, мы дадим вам резюме этого, так что мы можем иметь все части abmet-алгоритма вместе, так что stot-вы точно знаете, что вам нужно promet-реализовать, если вы хотите, чтобы реализовать обратное распространение для вычисления производных функции стоимости вашей нейросети относительно этих параметров.