Gradient Descent For Linear Regression.В предыдущих видео мы говорили об алгоритме градиентного спуска, о модели линейной регрессии и о функции затрат, определенной через сумму среднеквадратических отклонений. Теперь мы сведем вместе градиентный
Play video starting at ::20 and follow transcript0:20
спуск и нашу функции стоимости, что даст нам алгоритм линейной регрессии для аппроксимации данных прямой линией. Напомню, к чему мы пришли в предыдущих видео. Это хорошо знакомый нам алгоритм градиентного спуска, а это наша модель линейной регрессии: линейная гипотеза и усредненная сумма квадратов отклонений, наша функция затрат. Я собираюсь применить
Play video starting at :1:13 and follow transcript1:13
градиентный спуск к нашей функции стоимости. Чтобы применить алгоритм и написать
Play video starting at :1:27 and follow transcript1:27
программу, в первую очередь нам нужно получить эту производную. Давайте посчитаем эту частную производную... подставим функцию J... то есть коэффициент... сумма от 1 до m квадрата ошибки... Пока я просто переписал сюда определение функции затрат, упростим еще немного... сумма от 1 до m... тета нулевое плюс тета первое на x(i) минус y(i) и все это в квадрате. Теперь я просто подставил формулу функции-гипотезы. И, собственно говоря,
Play video starting at :3:14 and follow transcript3:14
нам нужно получить эти частные производные для двух случаев: для j,
Play video starting at :3:23 and follow transcript3:23
равного 0, и для j, равного 1. То есть взять ее относительно тета нулевого и тета
Play video starting at :3:39 and follow transcript3:39
первого.
Play video starting at :3:43 and follow transcript3:43
Я просто напишу, чему они равны.
Play video starting at :3:47 and follow transcript3:47
В первом случае получится 1/m умножить на сумму по
Play video starting at :3:52 and follow transcript3:52
обучающему набору... h от x(i) минус y(i). А во втором частная производная по тета первому получится равна... то же самое, умножить на x(i). Отлично.
Play video starting at :4:24 and follow transcript4:24
Расчет этих частных производных, то есть получение этих выражений из этого, требует представления об анализе функций многих переменных. Если вы знакомы с многомерным анализом, можете сами провести выкладки и убедиться, что частные производные действительно получатся такими, как у меня. А если не знакомы, ничего страшного,
Play video starting at :5:3 and follow transcript5:03
можете просто использовать выведенные мной выражения. В домашнем задании анализ вам тоже не понадобится, для реализации градиентного спуска достаточно готовых производных. Итак, получив эти выражения, эти производные,
Play video starting at :5:38 and follow transcript5:38
соответствующие уклону графика функции стоимости J, мы можем подставить их в формулы алгоритма градиентного спуска. Вот формулы шага градиентного спуска для линейной регрессии, которые мы будем применять до схождения. Новое значение для тета нулевого и тета первого получаем, вычитая из старого производную, умноженную на альфа. Вот она. Итак, это алгоритм линейной регрессии.
Play video starting at :6:41 and follow transcript6:41
Верно? равен, соответственно,
Play video starting at :6:47 and follow transcript6:47
частной производной по тета нулевому, которую мы получили на предыдущем слайде.
Play video starting at :6:57 and follow transcript6:57
А во втором — частной производной по тета первому,
Play video starting at :7:8 and follow transcript7:08
тоже полученной на предыдущем слайде.
Play video starting at :7:21 and follow transcript7:21
На всякий случай напомню одну тонкость реализации градиентного спуска: обновлять тета нулевое и тета первое вам нужно одновременно. Посмотрим, как градиентный спуск работает. Если помните, у градиентного спуска была одна проблема: он может «застрять» в локальном экстремуме. Когда я показывал вам градиентный спуск, я пользовался этим графиком, по которому мы спускались, как с холма, и оказалось, что мы можем прийти в разные локальные экстремумы в зависимости от того, откуда начали. Вы можете прийти сюда или сюда. Но, как выясняется, функция стоимости для линейной регрессии всегда будет чашеобразной, как на этом графике. Математический термин для этого — выпуклая функция. Я не буду давать строгого определения выпуклой функции, говоря простым языком, выпуклая функция и функция с чашеобразным графиком, ну, условно
Play video starting at :9:5 and follow transcript9:05
чашеобразным, — это одно и то же. У такой функции нет никаких локальных экстремумов, кроме одного глобального. Таким образом, применив градиентный спуск к выпуклой функции, а при линейной регрессии она всегда выпуклая, вы всегда окажетесь в глобальном экстремуме, потому что других локальных экстремумов нет. Теперь посмотрим на алгоритм в действии. Как обычно, здесь у меня графики функции-гипотезы и функции стоимости J. Пусть начальные значения моих параметров соответствуют этой точке.
Play video starting at :9:55 and follow transcript9:55
Обычно мы задаем в качестве начальных значений нули. Но для иллюстрации этого случая я положил тета нулевое равным примерно 900, а тета первое — примерно ?0,1.