Model Representation I.В этом видео я хочу начать рассказывать вам о том, как мы представляем нейронные сети. Другими словами, как мы представляем нашу гипотезу или , как мы представляем нашу модель при использовании нейронных сетей. Нейронные сети были разработаны как имитирующие нейроны или сети нейронов в мозге. Итак, чтобы объяснить представление гипотезы , давайте начнем с того, как выглядит один нейрон в мозге. Ваш мозг и мой джем полный нейронов, как эти, и нейроны являются клетками в мозге. И две вещи, на которые нужно обратить внимание, это первое. Нейрон имеет тело клетки, вот так, и более того, нейрон имеет ряд входных проводов, которые называются дендритами. Вы думаете о них как о входных проводах, и они получают входы из других мест. И нейрон также имеет выходной провод, называемый Axon, и этот выходной провод является тем, что он использует для отправки сигналов другим нейронам, так, чтобы отправлять сообщения другим нейронам. Таким образом, на упрощенном уровне, что такое нейрон, является вычислительной единицей, которая получает через него входные провода и выполняет некоторые вычисления, а затем говорит, что выходы через его аксон в другие узлы или в другие нейроны в мозге. Вот иллюстрация группы нейронов. То, как нейроны взаимодействуют друг с другом, с маленькими импульсами электричества, они также называются шипами, но это просто означает импульсы электричества. Итак, вот один нейрон, и что он делает, если он хочет отправить сообщение, что он делает, это посылает небольшой импульс электричества. Варис аксон на какой-то другой нейрон и здесь, этот аксон, который является этим открытым проводом, соединяется с дендритами этого второго нейрона здесь, который затем принимает это входящее сообщение, что некоторые вычисления. И они, в свою очередь, решают разослать это сообщение об этом аксоне другим нейронам, и это процесс, в котором происходит вся человеческая мысль. Это нейроны делают вычисления и передают сообщения другим нейронам в результате того, что у них есть другие входные данные. И, кстати, так работают и наши чувства, и наши мышцы. Если вы хотите переместить одну из ваших мышц так, как где еще в вашем нейроне может отправить это электричество в ваши мышцы, и это приводит к тому, что ваши мышцы сокращаются и ваши глаза, некоторые чувства, как ваш глаз, должны отправить сообщение вашему мозгу, в то время как он он чувствует хостов электричества Нейрон в вашем мозге, вот так. В нейросети, точнее, в искусственной нейронной сети, которую мы реализовали на компьютере, мы будем использовать очень простую модель , что нейрон делает мы будем моделировать нейрон как просто логистическую единицу. Итак, когда я рисую такой желтый круг, вы должны думать об этом как о ролевом анализе , который, возможно, является телом нейрона, и мы подаем нейрону несколько входов, которые являются различными дендритами или входными кулами. И нейрон делает некоторые вычисления. И выведите какое-то значение на этом выходном проводе, или в биологическом нейроне, это аксон. И всякий раз, когда я рисую диаграмму, как это, это означает, что это представляет собой вычисление h из x равно одному плюс e к отрицательной тета-транспонирование x, где, как обычно, x и тета являются нашими векторами параметров, вот так. Таким образом, это очень простая, возможно, очень упрощенная модель, вычислений, которые делает нейрон, где он получает количество входов, x1, x2, x3 и выводит некоторое значение, вычисленное так. Когда я рисую нейросеть, обычно я рисую только входные узлы x1, x2, x3. Иногда, когда это полезно, я нарисую дополнительный узел для x0. Этот x0 теперь иногда называется единицей смещения или нейроном смещения, но , потому что x0 уже равен 1, иногда я рисую это, иногда Я не буду зависеть только от того, что более нотационно удобно для этого примера. Наконец, последний бит терминологии, когда мы говорим о нейронных сетях, иногда мы скажем, что это нейрон или искусственный нейрон с сигмоидной или логистической активацией. Таким образом, эта функция активации в нейросетевой терминологии. Это просто еще один термин для этой функции для , что нелинейность g (z) = 1 над 1+e до -z. И в то время как до сих пор я называл theta параметрами модели, я в основном продолжу использовать эту терминологию. Здесь это копия параметров, но в нейронных сетях, в литературе нейронной сети иногда можно услышать, как люди говорят о весах модели, а весах означает точно то же самое, что и параметры модели. Но я буду в основном продолжать использовать терминологические параметры в этих видео, , но иногда вы можете услышать, что другие используют терминологию весов. Итак, эта маленькая диаграмма представляет собой один нейрон. Что такое нейронная сеть, это просто группа этих разных нейронов, сильная вместе. Полностью, здесь у нас есть входные единицы x1, x2, x3 и еще раз, иногда вы можете нарисовать эту дополнительную ноту x0, а иногда нет, просто поток, что здесь. И здесь у нас есть три нейрона, которые написали 81, 82, 83. Я поговорю об этих индексах позже. И еще раз мы можем, если мы хотим добавить только a0 и добавить блок смещения смеси там. Всегда есть значение 1. И затем, наконец, у нас есть третий узел и последний слой, и есть третий узел, который выводит значение, которое вычисляет гипотеза h (x). Чтобы ввести немного больше терминологии, в нейронной сети, первый слой, это также называется входным слоем, потому что здесь мы Ввод наших объектов, x1, x2, x3. Последний слой также называется выходным слоем, потому что этот слой имеет нейрон, этот, который выводит конечное значение, вычисленное гипотезой. А затем, слой 2 между ними, это называется скрытым слоем. Термин скрытый слой не очень большая терминология, но эта идея заключается в том, что, вы знаете, вы контролировали рано, , где вы можете увидеть входы и увидеть правильные выходы, где есть скрытый слой значений, которые вы не можете наблюдать в тренировочной настройке. Это не х, и это не у, и поэтому мы называем тех, кто скрыт. И они пытаются увидеть нейронные сети с более чем одним скрытым слоем, но в этом примере у нас есть один входной слой, слой 1, один скрытый слой, слой 2, и один выходной слой, слой 3. Но в основном, все, что не является входным слоем, а не является выходным слоем, называется скрытым слоем. Так что я хочу быть очень ясно о том, что делает эта нейронная сеть. Давайте пройдем через вычислительные шаги, которые и тело, представленное этой диаграммой. Чтобы объяснить эти конкретные вычисления, представленные нейронной сетью, вот немного больше нотации. Я собираюсь использовать надстрочный индекс j i для обозначения активации нейрона i или единицы i в слое j. Так что полностью это дало надстрочный индекс подгруппе 1, это активация первого блока во втором слое, в нашем скрытом слое. И под активацией я просто имею в виду значение, которое вычисляется и как вывод конкретным. Кроме того, новая сеть параметризуется этими матрицами, theta super script j Где theta j будет матрицей весов, управляющей функцией отображения одного слоя, может быть, первого слоя ко второму слою, или от второго слоя к третьему слою. Итак, вот вычисления, представленные этой диаграммой. Эта первая скрытая единица здесь имеет значение, вычисленное следующим образом, есть a21 равна сигма-функции функции активации сигма, также называется функцией активации логистики, применяется к подобному линейному сочетанию этих входов. И затем этот второй скрытый блок имеет эту активацию значение компьютера, как сигмоид этого. И аналогично для этой третьей скрытой единицы вычисляется по этой формуле. Итак, здесь у нас есть 3 тета 1, которая является матрицей параметров, регулирующих наше отображение из наших трех различных единиц, наших скрытых единиц. Тета 1 будет 3. Theta 1 будет 3x4-мерной матрицей. И в более общем плане, если в сети есть SJU единицы в j и sj + 1 единицы и sj + 1, то матрица theta j , которая управляет отображением функций оттуда sj + 1. Это должно будет упомянуть sj +1 на sj + 1 Я просто буду ясно об этой нотации право. Это индекс j + 1 и это s индекс j, и затем вся эта штука, плюс 1, вся эта штука (sj + 1), хорошо? Итак, это s индекс j + 1 by, Так что это s индекс j + 1 на sj + 1, где этот плюс один не является частью индекса. Хорошо, мы говорили о том, что три скрытые единицы делают для вычисления своих значений. Наконец, есть потеря этого финала и после этого у нас есть еще один блок, который компьютер h из x и , который равен также может быть записан как (3) 1, и это равно этому. И вы заметили, что я написал это с надстрочным индексом два здесь, потому что тета надстрочного индекса два является матрицей параметров, или матрицей весов, которая управляет функцией, которая отображает скрытые единицы, , что является слоем двух единиц к одному слою три единицы, то есть выход единица. Подводя итог, то, что мы сделали, показано, как картина вроде этого здесь определяет искусственную нейронную сеть, которая определяет функцию h , которая сопоставляет с входными значениями x, надеюсь, какое-то пространство, которое провизирует y. И эти гипотезы параметризируются параметрами , обозначающими капитал тета, так что, как мы изменяем тета, мы получаем разные гипотезы, и мы получаем разные функции. Mapping говорят от x до y. Таким образом, это дает математическое определение того, как представить гипотезу в нейронной сети. В следующих нескольких видео, что я хотел бы сделать, это дать вам больше интуиции о , что делают эти представления гипотез, а также перейти к нескольким примерам и рассказать о том, как их эффективно вычислить.