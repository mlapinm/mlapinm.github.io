The Problem of Overfitting.
К этому моменту, вы уже видели пару различных обучающих алгоритмов: линейную регрессию и логистическую регрессию.
Они работают хорошо для многих проблем, но когда вы применяете их для некоторых приложений <br />машинного обучения, они могут столкнуться с проблемой<br />которая называется переобучение, и может привести к <br />очень плохим результатам.
Я бы хотел в этом видео объяснить вам Вы что такое это переобучение и в нескольких видео после этого Мы будем говорить о технике, называемой Регуляризация, что позволит нам улучшить или уменьшить эту проблему переобучения и заставить эти алгоритмы обучения работать гораздо лучше Итак, что такое переобучение?
Давайте продолжим использовать наш работающий пример прогнозирования цен на жилье с помощью линейной регрессии, когда мы хотим предсказать Цену в зависимости от размера дома.
Одна вещь, которую мы могли бы сделать, это подобрать линейную функцию к этим данным, и если мы сделаем это, может быть мы получим такого рода прямую линию,<br /> которая удовлетворяет данным.
Но это не очень хорошая модель.
Глядя на данные, кажется довольно ясно, что когда Размер жилища увеличивается, жилищные цены достигают плато, или как-то становятся пологими при движении<br />вправо, и значит, этот алгоритм не аппроксимирует обучающие данные хорошо, и мы называем эту проблему <br />недостаточным обучением, и еще один термин для этого явления что этот алгоритм имеет<br /> высокий bias (уровень предвзятости).
И то, и другое примерно означает, что он даже не подбирает кривую,<br />подходящую к обучающим данным Термин является своего рода историческим или техническим, но идея состоит в том, что если мы подбираем прямую линию к данным, то это как если бы у алгоритма было очень сильное предубеждение, или очень сильный bias что цены<br />на жилье будут меняться линейно с их размером и несмотря на данные об обратном, Несмотря на свидетельства обратного, предубеждения все же являются предвзятыми, заставляют алгоритм подбирать прямую линию и в конце концов мы получаем<br />плохое соответствие данным Теперь в середине, мы могли бы подобрать квадратическую функцию и с этими данными мы подбираем Квадратическую функцию,<br /> может быть, мы получим такого рода кривую, и, это работает довольно хорошо.
И, с другой стороны, если бы нам нужно<br />было подобрать полином 4-го порядка, так здесь у нас есть пять параметров, от тета_0 до тета_4 и с ними мы можем подобрать<br />кривую, которая проходит через все наши пять обучающих примеров Вы можете получить кривую, которая <br />выглядит следующим образом.
Что, с одной стороны, кажется, очень хорошо подходит к обучающей выборке она проходит через все мои данные,<br />во всяком случае, Но, это все еще очень волнистая<br /> кривая, правильно?
Так она идет вверх и вниз, все время, и мы на самом деле не <br />думаем, что это такая уж хорошая модель<br />для предсказания цен на жилье.
Таким образом, эту проблему мы называем переобучение, и другой термин для этого что этот алгоритм имеет<br /> высокую дисперсию.
термин высокая дисперсия является еще одним историческим или техническим, но интуитивно, если мы подбираем полином такого высокого порядка, то гипотеза может подойти Это почти как если бы она могла подойти почти к любой функции, и пространство возможных гипотез слишком велико, или<br />слишком варьируется И у нас нет достаточно данных чтобы ограничить его, чтобы дать нам хорошую гипотезу, это <br />и называется переобучением И тому, что в середине, на самом деле нет имени, но я просто напишу "просто верно".
Где полином второй степени, квадратичная функция кажется наиболее подходящая для этих данных.
Подведём итог: проблема переобучения происходит, когда у нас есть слишком много свойств, чтобы обучить гипотезу очень хорошо соответствовать обучению.
Так, ваша функция стоимости на самом деле может быть очень близка к нулю или может быть даже ноль точно, но вы может затем закончиться кривая, как эта, пытаясь слишком сильно соответствовать обучающему набору и даже может соответствовать новым примерам, но не в состоянии предсказать цены на новых примерах хорошо, и тут термин "обобщенный" относится к тому насколько хорошо гипотеза применима даже к новым примерам.
То есть данные для домов, которые она не видела в обучающем наборе.
На этом слайде мы рассмотрели переобучение для случая линейной регрессии.
Аналогичная вещь может относиться и к логистической регрессии.
Вот логистическая регрессия пример с двумя свойствами X1 и x2.
Одна вещь, которую мы могли бы сделать, это соответствие логистической регрессии с простой гипотезой, как эта, где, как обычно, G - моя сигмовидная функция.
И если вы сделаете это, вы основитесь на гипотезе, пытаясь использовать, может быть, просто прямую для отделения положительных и отрицательных примеров.
И это не выглядит как очень хорошо вписывается в гипотезу.
Итак, еще раз, это пример недостаточного обучения или гипотезы, имеющей большой сдвиг.
Напротив, если бы вы добавили к вашим свойствам эти квадратные члены, то вы могли бы получить границу решения, которая может выглядеть как эта.
И это очень хорошо подходит для данных.
Возможно максимально хорошо, для этого обучающего набора.
И, наконец другая крайность, если бы вы использовали полином высокого порядка, если вы должны были генерировать много полиномиальных условий высокого порядка, тогда логистическая регрессия может искривляться, и может быть действительно трудно найти границу решения, которая соответствует вашему тренировочному набору или быть очень сильно изогнутой чтобы соответствовать каждому элементу обучающего набора.
И, вы знаете, если свойства X1 и X2 предлагает предсказать, допустим рак, рак - это злокачественные, доброкачественные опухоли молочной железы.
Это не так, это действительно не выглядят как очень хорошая гипотеза для предсказаний.
И так, еще раз, это случай переобучения и гипотеза имеет большое расхождение и в действительности маловероятно соответствует новым примерам.
Позже, в этом курсе, когда мы поговорим об отладке и диагностике вещей, которые могут пойти не так с алгоритмами обучения, мы дадим вам специальные инструменты для распознавания когда переобучение, а также, когда недостаточное обучение может происходить.
Но пока давайте поговорим о проблеме, если мы считаем, что происходит переобучение, что мы можем сделать, чтобы решить эту проблему?
В предыдущих примерах мы имели одномерные или двухмерные данные, мы могли бы просто построить гипотезу и посмотреть, что происходит и выбрать подходящую степень полинома.
Итак, раньше для жилья с примерами цен, мы могли бы просто построить гипотезу и вы может быть могли увидеть, что это выглядит как очень волнистая функция, которая предсказывает цены на жилье.
И мы могли бы тогда использовать график, чтобы выбрать подходящий полином степени.
Так что построение гипотезы могло бы быть одним из способов попытаться решить, какую степень полинома использовать.
Но это не всегда работает.
И на самом деле чаще мы могут возникнуть проблемы с обучением, где у нас просто есть много свойств.
Это не так просто выбрать степень полинома.
И, на самом деле, когда мы имеем так много свойств, это также становится намного сложнее построить график данных и это становится гораздо сложнее это визуализировать, чтобы решить, какие свойства оставить, какие нет.
Конкретно, если мы пытаемся предсказывать цены на жилье, иногда мы можем просто иметь много разных свойств.
И все эти функции кажутся полезными.
Но если у нас есть много свойств, и очень мало данные обучения, то, более вероятно переобучение станет проблемой.
Для того, чтобы избавиться от переобучения, есть две основных варианта, которые мы можем использовать.
Первый вариант - попробовать уменьшить количество свойств.
Конкретно, во-первых мы можем вручную просматривать список свойств, и попытаться решить, какие являются более важными свойствами, и, следовательно, какие свойства мы должны сохранить, и, какие свойства мы должны выбросить.
Позже в этом курсе, где также поговорим об алгоритмах выбора моделей.
Какие алгоритмы для автоматического решения, какие свойства сохранить и какие свойства выкинуть.
Эта идея сокращения количества свойств может работать хорошо и может уменьшить переобучение.
И когда мы будем говорить о модели выбора, мы углубимся в это.
Но недостатком является то, что выбрасывая некоторые из свойств, также выбрасываем некоторую информацию о проблеме, которую мы имеем.
Например, может быть, все эти свойства действительно полезны для прогнозирования цены жилья, так что, может быть, мы на самом деле не хотим выкидывать некоторую нашу информацию или выбрасывать некоторые из наших свойств.
Второй вариант, о котором мы будем говорить в следующих нескольких видео, это регуляризация.
При этом мы собираемся сохранить все свойства, но мы собираюсь уменьшить величину или вес параметров thera J.
И этот метод работает хорошо, мы увидим, когда у нас есть много свойств, каждое из которых добавляет немного к прогнозированию значения Y, как мы видели в примере предсказания стоимости жилья.
Где мы могли иметь много свойств, каждое из которых, довольно полезно и мы не хотели бы их выкинуть.
Это вписывается в идею регуляризации очень хорошо.
И я понимаю, что все эти детали, вероятно, еще не имеет смысла для вас.
Но в следующем видео мы начнём формулировать, как именно применять регуляризацию и, что именно означает регуляризация.
И тогда мы начнем выяснить, как это использовать, чтобы сделать алгоритмы обучения работающими хорошо и избежать переобучения.