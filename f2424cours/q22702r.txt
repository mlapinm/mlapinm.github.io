Large Margin Intuition.
Иногда люди говорят о поддержке векторных машин , как большие маржи классификаторы, в этом видео я бы хотел сказать вам, что означает « », и это также даст нам полезную картину о том, как может выглядеть гипотеза SVM stots.
Вот моя функция стоимости для опорной векторной машины , где здесь слева Я построил свою стоимость 1 функции z, которую я использовал для положительных примеров, а справа я построил свой ноль функции «Z», где у меня есть 'Z' здесь на горизонтальной оси.
Теперь давайте подумаем о том, что нужно, чтобы сделать эти функции стоимости малыми.
Если у вас есть положительный пример, так что если у равно 1, то стоимость 1 из Z равна нулю только тогда, когда Z больше или равна 1.
Другими словами, если у вас есть положительный пример, мы действительно хотим, чтобы theta transpose x была больше или равна 1 и, наоборот, если y равна нулю, посмотрите этот stots стоимость нулевой функции z, speter то это только в нашем регионе, где z меньше, чем равно 1 у нас есть стоимость равна нулю как z равна нулю, и это интересное свойство поддержки векторной машины права, которое есть , если у вас есть положительный пример, поэтому если у вас есть y равен одному, stoth то все, что нам действительно нужно heta transpose x больше, чем равно до нуля.
И это означало бы, что мы правильно классифицируем , потому что если theta транспонирование x больше нуля, наша гипотеза предсказат ноль.
И аналогично, если у вас есть отрицательный пример, то действительно все, что вы хотите, это то, что theta transpose x равно меньше нуля, и это будет убедиться, что мы получили пример правильно.
Но машина вектора поддержки хочет немного больше, чем это.
В нем говорится, вы знаете, не просто едва ли поймите пример правильно.
Так что не просто имеют его немного больше нуля.
То, что я действительно хочу, чтобы это было намного больше нуля скажем, может быть, бит больше или равно одному , и я хочу, чтобы это было намного меньше нуля.
Может быть, я хочу, чтобы он был меньше или равен -1.
И поэтому это строит дополнительный коэффициент безопасности или коэффициент запаса прочности в машину опорного вектора.
Логистическая регрессия делает что-то похожее, конечно же, , но давайте посмотрим, что происходит или давайте посмотрим, каковы последствия этого, в контексте boot машины опорного вектора.
Конкретно, то, что я хотел бы сделать дальше, это рассмотреть случай , когда мы установили эту константу C как очень большое значение, так что давайте предположим, что мы установили C в sts очень большое значение, может быть сто тысяч, какое-то огромное число.
Посмотрим, что будет делать машина вектора поддержки.
Если C очень, очень большой, то при минимизации этой цели оптимизации, мы собираемся , чтобы быть очень мотивированными, чтобы выбрать значение, так что этот первый срок будет равен нулю.
Итак, давайте попробуем понять проблему оптимизации в контекст, что бы это займет, чтобы сделать этот первый термин в объективной bit равным нулю, потому что вы знаете, может быть, мы установим C для того, чтобы иметь какую-то огромную константу, и этот cermote будет надеяться, что это должно дать нам дополнительная интуиция о том, что вроде гипотез учит машина опорных векторов.
Итак, мы уже видели, что всякий раз, когда у вас есть обучающий пример с меткой y=1, если вы хотите сделать первый термин bot-ноль, вам нужно, это найти значение theta promate, так что theta tranpose x i больше, чем или равно 1.
И аналогичным образом, всякий раз, когда у нас есть пример, с меткой нулевой, для того, чтобы , чтобы убедиться, что стоимость, стоимость ноль Z, для того, чтобы убедиться, что стоимость равна нулю, нам нужно, чтобы theta транспонирование х i s s s меньше, или он равен -1.
Итак, если мы думаем о нашей проблеме оптимизации как сейчас, действительно выбирая параметры и покажем, что этот первый термин равен нулю, то, что мы остались, это shopt следующая проблема оптимизации.
Мы собираемся свести к минимуму этот первый термин ноль, так что C умножить на ноль, потому что мы собираемся , чтобы выбрать параметры, которые равны нулю, плюс половина, а затем вы знаете, что sto второй термин и этот staby первый термин 'C' умножить на ноль, потому что давайте просто пересечем, что Я знаю, что это будет ноль.
И это будет подпадать под ограничение , что theta транспонирование x (i) больше или равно единице, если y (i) равно единице и b theta транспонирование x (i) меньше, чем spoth или равно минус один всякий раз, когда у вас есть отрицательный пример, и это оказывается, что когда вы решить эту задачу оптимизации, когда вы минимизируйте это как функцию параметров theta , вы получаете очень интересное решение hots границы.
Конкретно, если вы посмотрите на набор данных как это с положительными и отрицательными примерами, эти данные линейно отделяемы и по , что, я имею в виду, что существует, вы знаете, прямая линия, есть много разных прямых линий, both они могут разделить положительную и отрицательные примеры отлично.
Например, вот одна граница решения , которая разделяет положительные и отрицательные примеры, но каким-то образом, что не похож на очень естественный, верно?
Или, нарисовав еще хуже, вы знаете вот еще одна граница решения, которая разделяет положительные и отрицательные примеры , но едва ли.
Но ни один из них не кажется особенно хорошим выбором.
Векторные машины поддержки вместо этого выбирают эту границу решения , которую я рисую черным цветом.
И это кажется гораздо лучшей границей решения , чем любой из те, которые я нарисовал пурпурным или зеленым.
Черная линия кажется более надежным разделителем , она делает лучшую работу по отделению положительных и отрицательных примеров.
И математически, что это делает, эта черная граница решения имеет большее расстояние.
Это расстояние называется маржей, когда я составляю эти две дополнительные синие линии, мы видим , что черная граница решения имеет некоторое большее минимальное расстояние от любого из моих обучающих примеров, both в то время как пурпурный и зеленые линии примеры.
, а затем это, похоже, делает менее хорошую работу, разделяя положительные и отрицательные классы, чем моя черная линия.
И так это расстояние называется маржа векторной машины поддержки , и это дает SVM определенную надежность, потому что он пытается отделить данные с как можно большим запасом.
Таким образом, машина вектора поддержки иногда также называется большим классификатором поля , и этот на самом деле является следствием проблемы оптимизации, которую мы записали на предыдущем слайде.
Я знаю, что вам может быть интересно, как это, что проблема оптимизации, которую я записал на предыдущем слайде, как это приводит к этому большому классификатору маржи.
Я знаю, что еще не объяснил это.
И в следующем видео я собираюсь нарисовать немного интуиции о том, почему эта проблема оптимизации дает нам этот большой классификатор маржи.
Но это полезная функция имейте в виду, если вы пытаетесь понять, что такое гипотеза, которую выберет SVM.
То есть, пытаясь отделить положительные и отрицательные примеры с максимально большим запасом.
Я хочу сказать последнее, что о больших классификаторах маржи в этой интуиции, поэтому мы выписали эту большую маржу классификации настройку в случае, когда C, что концепция регуляризации, stots была очень большой, я думаю, что я установил, что сто тысяч или что-то в этом роде.
Таким образом, учитывая набор данных , как это, возможно, мы выберем эту границу решения, которая разделяет положительные и отрицательные примеры на большой марже.
Теперь SVM на самом деле довольно сложен , чем может предполагать этот большой вид поля И, в частности, если все, что вы делаете , это использовать большой классификатор поля , то ваши алгоритмы обучения могут быть чувствительны к выбросам, поэтому давайте просто добавить дополнительный положительный пример stoth, как показано на экране.
Если бы у него был один пример, то кажется, что разделять данные с большим запасом, , может быть, я закончу изучение границы решения, как это, верно?
, это пурпурная линия и , это действительно не ясно, что на основе на единственном выбросе, основанном на на одном примере, и это действительно не ясно, что это на самом деле хорошая идея, чтобы изменить свою границу решения от черного stote один на пурпурный.
Итак, если C, если параметр регуляризации C был очень большим, то это на самом деле то, что будет делать SVM, он изменит границы решения bit с черного на stowmagenta, но если бы вы были разумно малы, если бы вы использовали C, не слишком большой, то вы все еще заканчиваются этой границей черного решения.
И, конечно, если данные не были линейно разделены, так что если у вас были некоторые положительные примеры здесь, или если у вас были некоторые негативные примеры здесь, то SVM также будет делать правильно.
И так эта картинка большой классификатор маржи, который действительно, это действительно картина , которая дает лучшую интуицию только для случая, когда параметр правил bbit s s s очень велик, и просто, чтобы напомнить вам, что это соответствует cetan C играет роль, похожую на один над Lambda, где Lambda является параметром регуляризации , который мы имели ранее.
И так что это только один над Lambda очень большой или эквивалентно , если Lambda очень мал, что вы получаете такие вещи, как этой границы решения Magenta, но на практике при применении опорных векторных машин, stoth, когда C не очень большой, как это означает, что он может сделать лучшая работа, игнорируя несколько выбросов, как здесь.
И также отлично работает и делает разумные вещи , даже если ваши данные не линейно разделены.
Но когда мы говорим о смещении и дисперсии в контексте опорных векторных машин , которые сделают чуть позже, мы надеемся, что все из этих компромиссов, связанных с регуляризацией параметра , станут яснее в тот момент.
Поэтому я надеюсь, что дает некоторую интуицию о , как эта векторная машина поддержки функционирует как большой классификатор полей, который пытается отделить данные с большим запасом, технически эта фотография этого представления является истинным, только тогда, когда параметр C очень велик, который полезный способ думать о машинах опорных векторов.
Там был один недостающий шаг в это видео, который, почему это, что проблема оптимизации мы записали на этих слайдах , как это на самом деле bit приводит к большому классификатору маржи, Я stote не сделал этого в этом видео, pother в следующем видео я скетчу немного больше математики за этим , чтобы объяснить , что отдельное рассуждение о том, как проблема оптимизации мы написали приводит к большому классификатору маржи.