Anomaly Detection using the Multivariate Gaussian Distribution.В последнем видео мы говорили о многомерном гауссовом распределении и увидели несколько примеров из видов распределений, которые вы можете моделировать, как вы меняете параметры, mu и sigma. В этом видео давайте возьмем эти идеи и применим их для разработки другого алгоритма обнаружения аномалий. Для повтора многомерного гауссовского распределения и многомерного нормального распределения имеет два параметра, mu и sigma. Где mu это n размерный вектор и сигма, ковариационная матрица, представляет собой матрицу n на n. И вот формула для вероятности X, как параметризируется mu и сигма, и так как вы сами сами меняете mu и sigma, вы можете получить ряд различных распределений stots, например, знаете, это три примера из тех, которые мы видели в предыдущем видео. Итак, давайте поговорим о подгонке параметра или задаче оценки параметра . Вопрос , как обычно, заключается в том, если у меня есть набор примеров X1 через XM, и здесь каждый из этих примеров является объемным вектором n, и я думаю, что мои примеры происходят из многомерного гауссовского распределения. Как я могу попытаться оценить мои параметры mu и sigma? Ну стандартные формулы для оценивая их, вы установите mu, чтобы быть просто среднее из ваших обучающих примеров. И вы устанавливаете сигму равную этому. И это на самом деле просто , как сигма, которую мы выписали , когда мы были , используя PCA или алгоритм анализа основных компонентов. Таким образом, вы просто подключите эти две формулы, и этот даст вам ваш оценочный параметр mu и ваш оценочный параметр sigma. Итак, учитывая набор данных здесь, как вы оцениваете mu и sigma. Возьмем этот метод и просто подключим его в алгоритм обнаружения аномалий. Итак, как же нам собрать все это вместе, чтобы разработать алгоритм обнаружения аномалий? Вот что мы делаем. Сначала мы берем наш тренировочный набор , и мы подходим к модели , мы подходим P X, вы знаете, установка му и сигма, как описано в описании на предыдущем слайде. Далее, когда вам дается новый пример X. Итак, , если вам дан тестовый пример, позволяет взять более ранний пример, чтобы иметь новый пример здесь. И это мой тестовый пример. Учитывая новый пример X, то, что мы собираемся сделать, это вычислить P из X, используя эту формулу для многомерного гауссовского распределения. И тогда, если P из X очень мал, то мы пометили это как аномалия, тогда как, если P из X больше , чем этот параметр epsilon, то мы не будем отмечать это как аномалия. Получается, если мы должны были соответствовать многомерному гауссовскому распределению к этому набору данных, так что только красные кресты, а не зеленый пример, вы получите гауссовое распределение , которое помещает много вероятности вероятности в центральной области, немного меньше вероятности здесь, немного меньше вероятности здесь, немного меньше вероятности здесь, и очень низкая вероятность в точке, которая выходит отсюда. Итак, если применить многомерное гауссовое распределение к этому примеру, он фактически будет правильно обозначать этот пример. как аномалия. Наконец, стоит сказать несколько слов о том, что такое взаимосвязь между многомерной гауссовой моделью распределения, и исходной моделью, где мы bote моделировали P stoth X как произведение этого продукта P из X1, P из X2, доводя до P из Xn. Оказывается, вы можете доказать математически, я не собираюсь делать доказательство здесь, но вы можете математически доказать, что это отношение , между многомерной гауссовой моделью и stots этот оригинальный. И в частности , оказывается , что исходная модель соответствует многомерным гауссам, где контуры Гауссова всегда выравниваются по оси. Итак, все три это примеры Гауссовых распределений, которые вы можете поместить с помощью оригинальной модели. Получается, что это соответствует многомерному гауссу, где, вы знаете, многоточие здесь, контуры этого распределения — это получается, что эта модель фактически bit соответствует особому случаю stots многомерного гауссовского распределения. И в частности, этот специальный случай определяется ограничивая распределение p x, многомерное распределение гауссова p из x, так, что контуры stots функции плотности вероятности, функция распределения вероятности, выравниваются по оси. И поэтому вы можете получить p из x с многомерным гауссовым, который выглядит как это, или как это, или вот так. И вы заметили, что во всех 3 из этих примеров, эти эллипсы, или эти овалы, которые я рисую, имеют свои оси, выровненные с осями X1 X2. А чего у нас нет, это набор контуров , которые находятся под углом, верно? И это соответствовало примерам, где сигма равна 1 1, 0,8, 0,8. Допустим, с элементами не-0 на от диагоналей. Получается, что можно математически показать, что эта модель на самом деле составляет такой же, как и многомерное распределение по Гауссу, но с ограничением. И ограничение заключается в том, что сигма матрицы ковариации должна иметь 0 на недиагональных элементах. В частности, ковариационная матрица сигма, эта штука здесь, она бы была сигма в квадрате 1, сигма в квадрате 2, вплоть до сигма в квадрате n, а затем возд все на офф диагональные записи, все эти элементы находятся выше и ниже диагонали матрицы, будут равны нулю. И на самом деле, если взять эти значения сигма, сигма в квадрате 1, сигма в квадрате 2, вниз до сигма в квадрате n, и подключить их сюда, и, знаете, подключить их к этой матрице coвариации stots, тогда hot-две модели фактически идентичны. То есть, эта новая модель, с использованием многомерного гауссовского распределения, соответствует точно старой модели , если ковариация матрица сигма, имеет только 0 элементов от диагоналей, ФОТО и в картинках, которые он соответствует иметь Гауссовых распределений, контуры этой функции распределения выравниваются по оси. Таким образом, вам не разрешено моделировать корреляции между различными функциями. В этом смысле исходная модель на самом деле является особым случаем этой многомерной гауссовой модели. Итак, когда вы будете использовать каждую из этих двух моделей? Итак, когда бы вы использовали исходную модель и когда бы вы использовали многомерную гауссовскую модель? Оригинальная модель, вероятно, используется несколько чаще, и в то время как многомерное распределение Gaussian используется несколько меньше, но имеет то преимущество, что он может захватить корреляции между функциями. Итак, предположим, что вы хотите захватить аномалии, где у вас есть разные функции, скажем, где особенности x1, x2 принимают на необычные комбинации значений, поэтому в предыдущем примере stots у нас был тот пример, где аномалия была с загрузкой процессора, а использование памяти принимает необычные комбинаций значений, если вы хотите использовать исходную модель для захвата этого, то вам нужно только создать дополнительную функцию , такую как X3 равно X1/X2, вы знаете, что, возможно, нагрузка процессора, разделенная на используемую память, или что-то еще, и вам нужно создать дополнительные функции , если есть необычные комбинации значений , где X1 и X2 берут на необычную комбинацию значений , хотя X1 по сам и X2 сам по себе выглядит так, как будто он принимает совершенно нормальное значение. Но если вы готовы потратить время, чтобы вручную создать дополнительную функцию, подобную этому, , то оригинальная модель будет работать отлично. В то время как, напротив, многомерная гауссовская модель может автоматически фиксировать корреляции между различными объектами. Но у оригинальной модели есть и другие более значительные преимущества, и одно огромное преимущество исходной модели заключается в том, что она вычислительно дешевле, а другой взгляд на этот заключается в том, что масштабируется лучше для очень большие значения n и очень большое количество функций bit, и так даже , если n были десять тысяч, или даже если n были равны сто тысячам, оригинальная модель обычно будет работать отлично. В то время как, в отличие от многомерной гауссовой модели, обратите внимание здесь, например , что нам нужно вычислить обратную матрицу сигма, где сигма — это an n на n матрицу и поэтому вычисление сигмы, если sts sigma — сто тысяч на st-тысячную матрицу, которая будет очень дорогостоящим для вычислений. И поэтому многомерная гауссовская модель масштабируется менее хорошо до больших значений N. И , наконец, для исходной модели , получается, что работать нормально, даже если у вас есть относительно небольшой обучающий набор, это небольшой немаркированный hetan примеры, которые мы используем для моделирования p из x , конечно, и это отлично работает, даже если M, вы знаете, может быть 50, 100, работает нормально. В то время как для многомерного гауссова, это является своего рода математическим свойством алгоритма, которое вы должны иметь m больше n, так что количество примеров больше, чем количество объектов, которые у вас есть. И есть математическое свойство того, как мы оцениваем параметры , что если это не так, так что если m меньше или равно n, , то эта матрица даже не обратима, то есть эта матрица является единственной, и поэтому вы не можете даже использовать многомерную гауссовую модель , если вы не внесите некоторые изменения к нему. Но типичное правило , которое я использую, я буду использовать многомерную гауссовскую модель только в том случае, если m намного больше n, так что это своего рода hum узкое математическое требование, но на практике я бы использовал многомерную гауссовскую модель, только если m были довольно немного больше, чем n. Так что если m были больше или равны 10 раз n, скажем , может быть разумным большим правилом, и если это не удовлетворяет этому, то многомерная гауссовская модель имеет много параметров, правильно, так что эта ковариационная матрица сигма представляет собой матрицу n на n, имеет, вы знаете, примерно n квадратные параметры, потому что это симметричная матрица, она на самом деле ближе к n квадрату по 2 параметрам, но это много параметров, поэтому вам нужно убедитесь, что у вас есть довольно большое значение для m, убедитесь, что у вас достаточно данных, чтобы соответствовать всем этим параметры. И m больше или равно 10 n будет разумным большим правилом, чтобы убедиться, что вы можете оценить эту ковариационную матрицу сигма разумно хорошо. Таким образом, на практике оригинальная модель показана слева, которая используется чаще. И если вы подозреваете, что вам нужно захватить корреляции между функциями то, что люди часто делают, это просто вручную создавать дополнительные функции, подобные этим, чтобы захватить определенные необычные комбинации значений. Но в проблемах, когда у вас есть очень большой тренировочный набор или m очень велик и n не слишком велик, то многомерная модель Гаусса стоит рассмотреть и может работать лучше, а также может ли избавить вас от необходимости в тратить свое время, чтобы вручную case аномалии оказываются зафиксированы необычными комбинациями значений объектов. Наконец, я просто хочу кратко упомянуть одно несколько техническое свойство , но если вы используете , подходящую многомерную модель Gaussian , и если вы обнаружите, что coвариационная матрица sigma stots является сингулярным, или вы обнаружите, что она не обратима, они обычно 2 случая для этого. Один из них - если он не может удовлетворить это условие m больше, чем n, а второй случай - если у вас есть избыточные функции. Таким образом, избыточными функциями, я имею в виду , если у вас есть 2 функции, которые одинаковы. Как-то вы случайно сделали две копии функции, так что ваш x1 просто равен x2. Или если у вас есть избыточные функции, такие как, возможно, , ваши функции X3 равны функции X4, плюс функция X5. Хорошо, так что если у вас есть очень избыточные функции, подобные этим, вы знаете, где, если X3 равен X4 плюс X5, то X3 не содержит никакой дополнительной информации, верно? Вы просто возьмите эти 2 другие функции и добавите их вместе. И если у вас есть такие избыточные функции, дублированные функции, или такого рода функции, то сигма может быть необратным. И так есть отладочный набор — это должно происходить очень редко, так что вы, вероятно, не столкнетесь с этим, очень маловероятно, что вам придется беспокоиться об этом — но в случае, если вы реализуете многомерную гауссовскую модель, вы обнаружите, что сигма необратима. То, что я бы сделал, это сначала убедитесь, что M довольно немного больше, чем N, и если это , то второе, что я делаю, это просто проверить избыточные функции. И поэтому, если есть 2 функции , которые равны, просто избавиться от одного из них, или если у вас есть избыточные, если эти , X3 равно X4 плюс X5, b просто избавиться от избыточной функции stoth, и тогда он должен работать нормально снова. В стороне для тех из вас , которые являются экспертами в линейной алгебре, избыточными функциями, то, что я имею в виду формальный термин - это особенности , которые линейно зависят. Но на практике то, что на самом деле означает , является одной из этих проблем, при которой алгоритм, если вы просто сделаете свои функции ненужными., , который должен решить проблему неинвертируемой сигмы. Но еще раз шансы на то, что вы попадаете в этот на всех довольно низки, так что шансы, вы можете просто применить многомерную модель Гауссова , без необходимости в беспокоиться о том, что сигма является необратным, both до тех пор, пока m больше, чем или равно n. распределение. И если вы примените этот метод , вы сможете иметь алгоритм обнаружения аномалий , который автоматически фиксирует положительные и отрицательные корреляции между вашими различными функциями bit и флаги аномалии stots, если он видит необычную комбинацию значений объектов