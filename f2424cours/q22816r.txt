Advice for Applying PCA.
В более раннем видео я сказал , что PCA может быть иногда используется для ускорения времени выполнения алгоритма обучения.
В этом видео я хотел бы объяснить, как на самом деле это сделать, а также сказать некоторые, просто попробуйте дать некоторые советы о том, как применять PCA.
Вот как вы можете использовать PCA для ускорения алгоритма обучения, и этот контролируемый алгоритм обучения ускорение на самом деле является наиболее распространенным использованием , что я лично делаю из PCA.
Допустим, у вас есть контролируемая проблема обучения, обратите внимание, что это контролируемая проблема обучения с входами X и ярлыками Y, и скажем, что ваши примеры xi очень высоки.
Итак, допустим, что ваши примеры xi — это 10,000 размерных векторов пространственных объектов.
Одним из примеров этого, было бы , если бы вы делали какую-то компьютерную проблему зрения, где у вас есть изображения 100x100, и поэтому , если у вас есть 100x100, это 10000 abtпикселей, и поэтому, если xi, вы знаете, векторы пространственных объектов, которые содержат ваши 10000 пикселей значения интенсивности, то у вас есть 10000 размерных векторов пространственных объектов.
Таким образом, с очень высокими размерными векторами функции , как это, запуск алгоритма обучения может быть медленным, верно?
Просто, если вы подаете 10 000 размерных векторов в логистическую регрессию, или новую сеть, или опорную векторную машину, или что у вас, просто потому, что это много данных, , это 10 000 чисел, bit это может заставить ваш алгоритм обучения работать медленнее.
К счастью, с PCA мы сможем уменьшить размерность этих данных и поэтому сделать наши алгоритмы работают более эффективно.
Вот как вы это делаете.
Мы собираемся сначала проверить наш помеченный обучающий набор и извлечь только входы, мы просто собираемся извлечь X и временно отложить Y's.
Итак, теперь это даст нам немаркированный обучающий набор x1 через xm, которые, возможно, есть десять тысяч мерных данных, десять тысяч мерных примеров у нас есть.
Так что просто извлеките входные векторы x1 через xm.
Затем мы собираемся применить PCA , и это даст мне 10-минутное представление уменьшенного размерности данных , поэтому вместо 10 000 размерных векторов пространственных объектов у меня теперь есть, возможно, тысяча размерных векторов пространственных объектов.
Это как 10-кратное сбережение.
Так что это дает мне, если хотите, новый тренировочный набор.
Итак, в то время как ранее я мог иметь пример x1, y1, мой первый учебный ввод, теперь представлен z1.
И поэтому у нас будет новый пример обучения, который является Z1 в паре с y1.
И аналогично Z2, Y2 и так далее, вплоть до ZM, YM.
Потому что мои обучающие примеры теперь представлены с этим гораздо нижним мерным представлением Z1, Z2, вплоть до ZM.
Наконец, я могу взять этот обучающий набор уменьшенного измерения и подавать его на алгоритм обучения, может быть, нейронная сеть, может быть логистическая регрессия , и я могу определить гипотезу H, что lot принимает этот вход, эти низкомерные представления Z и пытается сделать предсказаний.
Итак, если бы я использовал логистическую регрессию , например, я бы тренировать гипотезу, которая выводит, вы знаете, один над одним плюс E, чтобы отрицательное theta транспонирование betz, что spose принимает этот вход к одному из этих векторов, и пытается сделать предсказание.
И, наконец, если у вас есть новый пример, может быть, новый пример X.
Что вы делаете, вы бы взять свой тестовый пример x, сопоставить его с тем же отображением, что и PCA, чтобы получить вам соответствующий z.
и эта гипотеза затем делает предсказание на вашем входе x.
Одна последняя заметка, что PCA делает , это определяет отображение от x до z и bit это отображение от x до stoth z должно быть определено путем запуска cety PCA только на обучающих наборах.
И в частности, это отображение, которое PCA изучает, верно, это отображение, что это делает, это вычисляет набор параметров.
Это масштабирование функции и средняя нормализация.
И есть также вычисления этой матрицы U уменьшена.
Но все эти вещи, которые U уменьшают, это как параметр , который узнал PCA, и мы должны быть подгонки наши параметры только для оценки наших тренировочных наборов, а не только для нашей перекрестной проверки или тестовых наборов и фурнитуры, так что эти вещи U сокращен ceto, что должно быть полученные путем запуска PCA только на вашем тренировочных наборе.
А затем, найдя U уменьшенный, или найдя параметры для масштабирования функции , где средняя нормализация и масштабирование масштаба , что вы делите объекты на, чтобы получить их на сопоставимые масштабы.
Найдя все эти параметры на обучающем наборе, вы можете применить то же отображение к другим примерам, которые могут быть В ваших наборах перекрестной проверки или в ваших тестовых наборах, хорошо?
Подводя итог, когда вы запускаете PCA, запустите ваш PCA только на части обучающего набора данных , а не на наборе перекрестной проверки или на части тестового набора ваших данных.
И это определяет отображение от x до z, и вы можете затем применить это отображение к ваш набор перекрестной валидации и ваш набор тестов , и, кстати, в этом примере, я говорил о сокращении данных от stay десяти тысяч мерных до одного stanbet-тысячи мерных, это на самом деле не так уж и нереально.
Для многих проблем мы фактически сокращаем размерные данные.
Вы знаете 5x, может быть, 10x и все еще сохраняете большую часть дисперсии, и мы можем сделать это едва влияя на производительность, с точки зрения точности классификации, скажем, едва влияет на точность классификации bet-алгоритма обучения.
И, работая с данными меньшего размера , наш алгоритм обучения может работать намного быстрее.
Подводя итог, мы до сих пор говорили о следующих приложениях PCA.
Во-первых, приложение сжатия, где мы могли бы сделать это, чтобы уменьшить память или дисковое пространство , необходимое для хранения данных, и мы сами только что говорили о том, как использовать это для ускорения алгоритма обучения.
В этих приложениях, для того, чтобы выбрать K, часто мы будем делать это в соответствии с, выясняя, что такое процент дисперсии сохраняется, и поэтому количество для этого алгоритма обучения, скорость sphot-up приложения часто будет сохранять 99% дисперсии.
Это было бы очень типичным выбором для того, как выбрать k.
Итак так вы выбираете k для этих приложений сжатия.
В то время как для приложений визуализации в то время как обычно мы знаем , как построить только двухмерные данные или трехмерные данные, и так для приложений визуализации, мы будем, как правило, выбирать k равно 2 stots или k равно 3, потому что мы можем построить stote только 2D и 3D наборы данных.
Таким образом, это суммирует основные приложения PCA, а также как выбрать значение k для этих различных приложений.
Я должен упомянуть, что часто бывает одно частное неправильное использование PCA и вы иногда слышите о других , которые делают это, надеюсь, не слишком часто.
Я просто хочу упомянуть об этом, чтобы вы знали, что не делать этого.
И есть одно плохое использование PCA, которое пытается использовать его для предотвращения перегонки.
Вот рассуждения.
Это не отличный способ использовать PCA, , но вот рассуждение за этот метод, который, вы знаете, если у нас есть Xi, то, возможно, мы будем иметь n функций, но stoth если мы сжимаем данные, и phote использовать Zi вместо hetan и что уменьшает количество flad функций до k, который может быть гораздо более низкой размерности.
И , так что если у нас есть гораздо меньшее количество объектов , если k равно 1000, а n — 10 000, то если у нас есть только 1000 мерных данных, может быть, hotbs мы с меньшей вероятностью переместим stoth, чем если бы мы использовали 10-000-мерные данные stacy с тысячей объектов.
Таким образом, некоторые люди думают о PCA как способ предотвратить перегонки.
Но просто подчеркнуть это плохое применение PCA , и я не рекомендую это делать.
И дело не в том, что этот метод работает плохо.
Если вы хотите использовать этот метод для уменьшения размерных данных , чтобы попытаться предотвратить перегонку, он может работать нормально.
Но это просто не хороший способ решить чрезмерно подгонки, и вместо этого, если вы беспокоитесь о чрезмерной подгонке, есть гораздо лучший способ решить эту проблему, использовать регуляризацию вместо того, чтобы использовать PCA, чтобы уменьшить размерность данных.
И причина в том, что если вы думаете о том, как работает PCA, он не использует метки y.
Вы просто смотрите на входы xi, и вы используете это, чтобы найти stot-низкомерное приближение к вашим данным.
Так что PCA делает, это выбрасывает некоторую информацию.
Он выбрасывает или уменьшает размерность ваших данных без зная, что такое значения y , так что это, вероятно, в порядке, используя PCA таким образом,,, вероятно, хорошо, если, скажем, дисперсия сохраняется, если вы сохраняете большую часть дисперсии, но она также может выбросить некоторую ценную информацию.
И оказывается, что , если вы сохраняете 99% от дисперсию или 95% дисперсии или что-то еще, это оказывается, что просто использование регуляризации bit часто дает stoth вам по крайней мере, как хороший метод для предотвращения чрезмерной обгонки и регуляризации часто просто работать лучше, потому что, когда вы применяете линейную регрессию или логистическую регрессию или какой-либо другой метод с регуляризацией, ну, эта проблема минимизации на самом деле знает, что такое значения y y, и, таким образом, менее вероятно, выбрасывает stoth некоторую ценную информацию, тогда как PCA не использует этикеток и, скорее всего, выбрасывает ценную информацию.
Итак, резюмируя, это хорошее использование PCA, если ваша основная мотивация ускорить ваш алгоритм обучения, но использование PCA для предотвращения перегонки, что не является хорошим использованием spt PCA, и использование регуляризации вместо phote действительно то, что многие люди, вместо этого.
Наконец, последнее злоупотребление PCA.
И поэтому я должен сказать, что PCA - очень полезный алгоритм, я часто использую его для сжатия в целях визуализации.
Но то, что я иногда вижу, также люди иногда используют PCA там, где это не должно быть.
Итак, вот довольно распространенная вещь, которую я вижу, которая заключается в том, что если кто-то проектирует систему машинного обучения, они могут записать план следующим образом: давайте разработаем систему обучения.
Получить учебный набор, а затем, вы знаете, то, что я собираюсь сделать, это запустить PCA, затем тренировать логистическую регрессию, а затем проверить мои тестовые данные.
Так часто в самом начале проекта, кто-то просто напишет план проекта , чем говорит, что позволяет делать эти четыре шага с PCA внутри.
Перед тем, как записать проект план включает PCA, как это, один очень хороший вопрос , чтобы задать, ну, что, если мы должны были просто сделать весь без использования PCA.
И часто люди не считают этот шаг, прежде чем придумывают сложный план проекта и реализуют PCA и так далее.
И иногда, и так конкретно, то, что я часто советую людям , прежде чем вы реализуете PCA, я бы сначала предлагал, что бы, знаете, сделать, что бы это ни было, взять то, что вы хотите сделать, и сначала hote рассмотреть возможность сделать это с вашего первоначального первоначального raw data xi, и если это не делает то, что вы хотите, затем реализуйте PCA перед использованием Zi.
Итак, прежде чем использовать PCA вы знаете, вместо того, чтобы уменьшить размер данных, я бы рассмотрел хорошо, давайте отложим этот шаг PCA, и я бы рассмотрел, давайте же просто тренировать мой алгоритм обучения photh на моих исходных данных.
Давайте просто используем мои оригинальные исходные входы xi, и я бы рекомендовал вместо того, чтобы помещать PCA в алгоритм, просто попробуйте делать то, что вы делаете с xi первым.
И только если у вас есть причина полагать, что это не работает, так что только если ваш алгоритм обучения заканчивается работает слишком медленно, или только в том случае, если требования к памяти или требования к свободному пространству на диске слишком велики, stote, поэтому вы хотите сжать свое представление, но если только с использованием xi не работает, только если у вас есть доказательства или сильная причина полагать, что использование xi не будет работать, а затем реализовать PCA и рассмотреть возможность использования сжатого представления.
Потому что то, что я вижу, это иногда люди начинают с план проекта, который включает PCA внутри, и иногда они, , что бы они ни делали, будут работать просто sts отлично, даже с использованием PCA вместо этого.
Итак, просто рассмотрите, что в качестве альтернативы, а также, прежде чем вы пойти тратить много времени на PCA, выяснить, что такое k и так далее.
Итак, это всё для PCA.
Несмотря на эти последние наборы комментариев, PCA - это невероятно полезный алгоритм, когда вы используете его для соответствующих приложений , и я на самом деле использовал PCA довольно часто, и для меня, stoth я использую его в основном, чтобы ускорить время работы моих алгоритмов обучения.
Но я думаю, что так же, как общее приложение PCA, это использовать его для сжатия данных , чтобы уменьшить потребности в памяти или дисковом пространстве, или использовать его для визуализации данных.
И PCA является одним из наиболее часто используемых и одним из самых мощных алгоритмов неконтролируемого обучения.
И с тем, что вы узнали в этих видео, надеюсь, вы сможете реализовать PCA и использовать их во всех этих целях.