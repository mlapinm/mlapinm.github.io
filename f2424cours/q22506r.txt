Random Initialization.В предыдущем видео мы собрали почти все части, которые вам нужны для того, чтобы реализовать и тренироваться в вашей сети. Есть только одна последняя идея, которую мне нужно поделиться с вами, , которая является идеей случайной инициализации. Когда вы запускаете алгоритм градиентного спуска, или также расширенные алгоритмы оптимизации, нам нужно выбрать некоторое начальное значение для параметров тета. Таким образом, для расширенного алгоритма оптимизации, он предполагает, что вы передадите ему какое-то начальное значение для параметров theta. Теперь рассмотрим градиентный спуск. Для этого нам также нужно инициализировать theta на что-то, и тогда мы можем медленно предпринять шаги, чтобы спуститься вниз с помощью градиентного спуска. Чтобы спуститься вниз, минимизировать функцию j теты. Итак, что мы можем установить начальное значение тета? Можно ли установить начальное значение теты в вектор всех нулей? В то время как это работало нормально, когда мы использовали логистическую регрессию, инициализация всех ваших параметров до нуля на самом деле не работает , когда вы торгуете в собственной сети. Рассмотрим торговлю по нейронной сети, и скажем, мы инициализируем все параметры сети на 0. И если вы это сделаете, то вы, что это означает, что при инициализации этот синий вес, окрашенный в синий будет равен этому весу, так что они оба равны 0. И этот вес, который я раскрашиваю красным, равен этому весу, окрашенному в красный цвет, а также этот вес, который я раскрашиваю зеленым, будет равен значению этого веса. И это означает, что обе ваши скрытые единицы, A1 и A2, будут вычислять одну и ту же функцию ваших входов. И, таким образом, вы получаете для каждого из ваших обучающих примеров , вы получаете A 2 1 равно A 2 2. И более того, потому что я не собираюсь показывать это слишком подробно, но , потому что эти исходящие веса одинаковы, вы также можете показать , что значения дельты также будут одинаковыми. Таким образом, вы в конечном итоге с дельта 1 1, дельта 2 1 равно дельта 2 2, , и если вы будете работать через карту дальше, то вы можете показать, что частичные производные по отношению к вашим параметрам будут удовлетворять следующему, , что частичная производная функции стоимости с уважением вырывает производные уважения к этим двум синим волнам в вашей сети. Вы обнаружите, что эти две частичные производные будут равны друг другу. И это означает, что даже после того, как сказать одно большее обновление спуска, вы собираетесь обновить, скажем, этот первый синий показатель был уровень обучения умножен на это, , и вы собираетесь обновить второй синий курс с некоторым показателем обучения раз это. И это означает, что даже после того, как один создал обновление спуска, эти две синие скорости , эти два параметра синего цвета окажутся такими же, как друг у друга. Таким образом, будет какое-то ненулевое значение, но это значение будет равно этому значению. И аналогичным образом, даже после одного обновления градиентного спуска, это значение будет равно этому значению. Все еще будут некоторые ненулевые значения, только то, что два красных значения равны друг другу. И аналогичным образом, два зеленых пути. Ну, они оба изменят значения, но они оба будут иметь то же значение, что и друг друга. Таким образом, после каждого обновления параметры, соответствующие входам, идущие в , каждый из двух скрытых единиц идентичны. Это просто говорит, что два зеленых веса все еще одинаковы, два красных веса все еще одинаковы, два синих веса все еще одинаковы, и что это означает, что даже после одной итерации скажем, градиентного спуска и спуска. Вы обнаружите, что ваши два головных блока все еще вычисляются точно такие же функции входов. У вас все еще есть a1 (2) = a2 (2). И ты вернулся к этому делу. И по мере того, как вы продолжаете бегать больше спуска, голубые волны,,, две синие волны, останутся такими же, как друг друга. Две красные волны останутся такими же, как друг друга, и две зеленые волны останутся такими же, как друг друга. И это означает, что ваша нейронная сеть действительно может вычислять очень интересные функции, верно? Представьте, что у вас было не только две скрытые единицы, но представьте, что у вас было много, много скрытых единиц. Тогда это говорит о том, что все ваши возглавляемые подразделения вычисляются точно такую же функцию. Все ваши скрытые единицы вычисляются точно такую же функцию ввода. И это очень избыточное представление , потому что вы находите подразделение логистической прогрессии. Он действительно должен видеть только одну функцию, потому что все они одинаковы. И это мешает вам и вашей сети сделать что-то интересное. Чтобы обойти эту проблему, мы инициализируем параметры нейронной сети, следовательно, со случайной инициализацией. Конкретно, проблема была видна на предыдущем слайде, это то, что называется проблемой симметричных способов, вот способы одинаковы. Таким образом, эта случайная инициализация - это то, как мы выполняем нарушение симметрии. Итак, что мы делаем, это инициализируем каждое значение theta случайным числом между минус epsilon и epsilon. Итак, это обозначение в b числа между минус эпсилон и плюс эпсилон. Таким образом, мой вес для моих параметров будет случайным образом инициализирован между минус эпсилон и плюс эпсилон. То, как я пишу код для этого в октаве, я сказал, что Theta1 должен быть равен этому. Итак, этот ранд 10 на 11, вот как вы вычисляете случайную матрицу 10 на 11. Все значения находятся в диапазоне от 0 до 1, так что это будут необработанные числа, которые принимают любые непрерывные значения от 0 до 1. И поэтому, если вы берете число от нуля до один, умножьте его в два раза INIT_EPSILON, затем минус INIT_EPSILON, тогда вы получите число, которое находится между минусом epsilon и плюс epsilon. И так, что приводит нас, этот эпсилон здесь не имеет ничего общего с эпсилоном, который мы использовали, когда мы делали проверку градиента. Таким образом, когда числовая проверка градиента, там мы добавляли некоторые значения эпсилон и тета. Это ваша неродная ценность эпсилон. Мы просто хотели указать init epsilon, чтобы отличить его от значения epsilon, которое мы использовали при проверке градиента. И аналогичным образом, если вы хотите инициализировать theta2 в случайную матрицу 1 на 11 , вы можете сделать это, используя этот фрагмент кода здесь. Итак, чтобы подытожить, чтобы создать нейросеть то, что вы должны сделать , это случайным образом инициализировать волны до небольших значений, близких к нулю, между -epsilon и +epsilon говорят. А затем реализуйте обратное распространение, отлично проверяйте, и используйте либо большие по спуску, либо 1b продвинутые алгоритмы оптимизации, чтобы попытаться минимизировать j (тета) в качестве функции параметров theta, начиная с просто случайно выбранное начальное значение для параметров. И делая нарушение симметрии, , который является этим процессом, надеюсь, большой градиентный спуск или продвинутые алгоритмы оптимизации смогут найти хорошее значение theta.