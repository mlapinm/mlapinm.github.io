Cost Function.Нейронные сети являются одним из самых мощных алгоритмов обучения, которые мы имеем сегодня. В этом и в следующих видео я хотел бы начать говорить о обучающем алгоритме для подбора параметров нейронной сети с учетом обучающего набора. Как и при обсуждении большинства наших алгоритмов обучения, мы собираемся начать с разговора о функции стоимости для подбора параметров сети. Я собираюсь сосредоточиться на применении нейронных сетей к задачам классификации Предположим, что у нас есть сеть, подобная той, которая показана слева. И предположим, что у нас есть тренировочный набор, как это x I, y I пары обучающих M. Я собираюсь использовать верхний регистр L для обозначения общего количества слоев в этой сети. Таким образом, для сети, показанной слева, у нас будет капитал L равен 4. Я собираюсь использовать S индекс L, чтобы обозначить количество единиц, , то есть количество нейронов. Не считая единицы смещения в их L сети. Например, у нас будет S один, который там равен, равен S три единицы, S два в моем примере пять единиц. И выходной слой S четыре, который также равен S L, потому что заглавная L равна четырем. Выходной слой в моем примере имеет четыре единицы. Рассмотрим два типа проблем классификации. Первая — бинарная классификация, где метки y либо 0, либо 1. В этом случае у нас будет 1 выходной блок, поэтому этот блок нейронной сети сверху имеет 4 выходных единицы, но если бы у нас была бинарная классификация, у нас было бы только один выходной блок, который вычисляет h (x). И выход нейронной сети будет h (x) будет реальным числом. И в этом случае количество выходных единиц, S L, где L снова индекс конечного слоя. Потому что это количество слоев, которые у нас есть в сети, поэтому количество единиц, которые у нас есть в выходном слое будет равно 1. В этом случае, чтобы упростить нотацию позже, я также собираюсь установить K = 1, поэтому вы можете думать о K как о числе единиц в выходном слое. Второй тип проблемы классификации, которую мы рассмотрим, будет проблема классификации нескольких классов, где мы можем иметь K различных классов. Итак, наш ранний пример имел это представление для y, если у нас есть 4 класса, и в этом случае мы будем иметь капитальные единицы вывода K и нашу гипотезу или выходные векторы, которые являются K размерными. И количество выходных единиц будет равно K. И обычно у нас будет K больше или равно 3 в этом случае, потому что если у нас были две причины, то нам не нужно использовать один стихи все метод. Мы используем один стих весь метод только в том случае, если у нас есть K больше или равно V классов, поэтому имея только два класса, нам нужно будет использовать только один верхний блок. Теперь определим функцию стоимости для нашей нейросети. Функция затрат, которую мы используем для нейронной сети, будет обобщением той, которую мы используем для логистической регрессии. Для логистической регрессии мы использовали для минимизации функции стоимости J (тета), которая была минус 1/м от этой функции затрат, а затем плюс этот дополнительный термин регуляризации здесь, где это была сумма от J=1 до n, , потому что мы не упорядочивали смещение theta0. Для нейросети наша функция затрат будет обобщением этого. Где вместо того, чтобы иметь в основном только один, который является выходным устройством сжатия, мы можем вместо них иметь K. Итак, вот наша функция затрат. Наша новая сеть теперь выводит векторы в R K, где R может быть равно 1, если у нас есть бинарная проблема классификации. Я собираюсь использовать эту нотацию h (x) индекс i, чтобы обозначить i-й вывод. То есть h (x) является k-мерным вектором и поэтому этот индекс i просто выбирает i-й элемент вектора, который выводится моей нейронной сетью. Моя функция стоимости J (тета) теперь будет следующей. Is - 1 над M суммы аналогичного термина, что у нас есть для логистической регрессии, за исключением того, что у нас есть сумма от K равна 1 до K. Это суммирование в основном сумма над моим K-выходом. Единица. Поэтому, если у меня есть четыре выходных единицы, , то есть, если последний слой моей нейронной сети имеет четыре выходных единицы, , то это сумма от k равна от одного до четырех в основном функция стоимости логистической регрессии алгоритма, но суммируя эту функцию затрат над каждым из моих четырех выходных единиц в свою очередь. И поэтому вы замечаете, в частности, что это относится к Yk Hk, , потому что мы в основном берем K верхние единицы и сравниваем это со значением из Yk, который является одним из тех векторов, которые говорят, что это должно стоить. И, наконец, второй термин здесь — термин регуляризации, , похожий на то, что мы имели для логистической регрессии. Этот термин суммирования выглядит очень сложным, но все, что он делает, это суммируя эти термины theta j i l для всех значений i j и l. За исключением того, что мы не суммируем термины, соответствующие этим значениям смещения , как мы имеем для логистической прогрессии. Полностью, мы не суммируем термины, ответившие на то, где i равно 0. Это потому, что, когда мы вычисляем активацию нейрона, у нас есть такие термины. Тета я 0. Плюс тета i1, x1 плюс и так далее. Там, где я думаю положить в два там, это первый удар там. И поэтому значения с нулем там, , что соответствует тому, что умножается на x0 или a0. И это похоже на единицу смещения и по аналогии с тем, что мы делали для логистической прогрессии, мы не будем суммировать эти термины в нашем термине регуляризации , потому что мы не хотим их упорядочить и строить их значения как нуль. Но это всего лишь одно возможное соглашение, и даже если бы вы суммировали i равно 0 до Sl, это будет работать примерно так же и не имеет большого значения. Но, возможно, это соглашение о нерегулировании термина смещения просто немного более распространено. Это функция затрат, которую мы будем использовать для нашей нейронной сети. В следующем видео мы начнем говорить о алгоритме для , пытающегося оптимизировать функцию затрат.