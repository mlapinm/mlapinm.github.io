Learning Curves.В этом видео я хотел бы вам рассказать о кривых обучения.
Play video starting at ::3 and follow transcript0:03
Кривая обучения часто очень полезная вещь для построения графика. Если Вы хотите провести точную проверку того, что Ваш алгоритм работает правильно, или если Вы хотите улучшить эффективность алгоритма.
Play video starting at ::13 and follow transcript0:13
Кривая обучения — это инструмент, который я использую очень часто, чтобы проверить страдает ли алгоритм обучения от смещения, или проблемы с дисперсией, или и то и другое вместе.
Play video starting at ::27 and follow transcript0:27
Итак, что такое кривая обучения. Чтобы построить кривую обучения, что я обычно делаю, это строю Jtrain, которое, скажем
Play video starting at ::35 and follow transcript0:35
среднеквадратичная ошибка моего обучающего набора или Jcv, которое среднеквадратическая ошибка моего набора перекрестной проверки. И я собираюсь построить это как функцию m, то есть как функцию количества примеров обучения, которые у меня есть. И m, как правило, постоянная, потому что я имею 100 обучающих примеров, но что я собираюсь сделать - это искусственно изменить мой набор тренировочных данных. Так я сознательно ограничиваю себя только использованием, скажем, 10 или 20 или 30 или 40 учебных примеров и построить график для ошибки обучения и перекрестной проверки для этого маленького тренировочного набора. Так давайте посмотрим, как эти участки могут выглядеть. Предположим, у меня есть только один пример обучения, как тот, что показан в этом первом примере и я подгоняю квадратичную функцию.
Play video starting at :1:22 and follow transcript1:22
У меня есть только один тренировочный набор. Я могу подобрать её превосходно, так? Вы знаете, просто соответствовать квадратичной функции, я собираюсь иметь нулевую ошибку для одного обучающего набора. Если же у меня два тренировочных набора, квадратичная функция также может очень хорошо подойти.
Play video starting at :1:37 and follow transcript1:37
Даже если я использую регуляризацию, я возможно могу подобрать это довольно хорошо. И если я не использую регуляризацию, я собираюсь соответствовать этому идеально и если у меня есть три набора обучения, то я снова могу подобрать квадратичную функцию хорошо, итак если m = 1 или m = 2 или m = 3,
Play video starting at :1:54 and follow transcript1:54
моя ошибка обучения на моём обучающем наборе стремится к 0, при условии, что я не использую регуляризацию или это может быть несколько больше 0, если я использую регуляризацию. Однако, если у меня есть большой набор тренировочных данных и я искусственно ограничиваю размер моего обучающего набора для расчета Jtrain. Вот если бы я поставил m = 3, скажем, и я тренировал только на трех примерах, тогда для этого графика я собираюсь измерить мою ошибку тренировки только на трех примерах, которые на самом деле соответствуют моим данным тоже.
Play video starting at :2:27 and follow transcript2:27
И даже если у меня есть 100 тренировочных примеров, но я хочу построить чему равно моя ошибка тренировки для m = 3. Что я хочу сделать
Play video starting at :2:34 and follow transcript2:34
это измерить ошибку тренировки только для 3-х примеров, что я на самом деле вписался в мою гипотезу.
Play video starting at :2:41 and follow transcript2:41
А не все другие примеры, которые у меня есть намеренно исключенные из процесса обучения. Так что просто подведем итог, что мы видим, что если размер учебного набора маленький, то ошибка обучения будет небольшой также. Потому что вы знаете, у нас есть небольшой тренировочный набор будет очень легко подобрать для вашего тренировочного набора очень хорошо может быть даже отлично. Теперь, допустим мы имеем m = 4, для примера. Хорошо, тогда квадратичная функция может быть соответствует этому набору данных отлично и если я имею m = 5 тогда квадратичная функция может подходить не очень, в то время как мой обучающий набор становится больше.
Play video starting at :3:16 and follow transcript3:16
Все труднее становится убедиться, что я могу найти квадратичную функцию, которая обрабатывает через все мои примеры отлично. Так на самом деле, размер тренировочного набора растет то, что вы обнаружите, что моя средняя ошибка обучения на самом деле увеличивается, и поэтому, если вы строите этот график, вы обнаружите, что ошибка обучающего набора, которая средняя ошибка вашей гипотезы растет вместе с ростом m. И просто повторим, что когда m маленький (когда у вас только несколько тренировочных наборов). Это довольно легко подобрать для каждого отдельного примера отлично. И ваша ошибка стремится быть маленькой, в то время как когда m большое, то становится труднее получить отличное соответствие всем тренировочным примерам отлично и ваша ошибка обучающего набора больше. Как насчет ошибки перекрестной проверки? Перекрестная проверка это ошибка на наборе который я не видел и вы знаете, когда у меня есть очень маленький тренировочный набор, я не буду обобщать хорошо, просто не собираюсь преуспеть в этом. Итак, эта гипотеза здесь не выглядеть как хорошая, и только когда я получаю больший тренировочный набор, я начинаю получать гипотезы, которые могут соответствовать данным несколько лучше. Так что ваша ошибка перекрестной проверки и Ваша ошибка набора тестов будет стремиться уменьшаться по мере вашей тренировки с увеличением размера набора, потому что чем больше данных, тем лучше вы делаете при обобщении на новые примеры. Таким образом, чем больше у вас данных, тем лучше гипотеза, которая вам подходит. И если вы построите Jtrain и Jcv, то вы получите это. Теперь давайте посмотрим на что кривые обучения могут быть похожи, если у нас либо большое смещение или большую дисперсию (расхождение). Предположим, ваша гипотеза имеет большое смещение, и чтобы объяснить это я собираюсь использовать набор примеров, подогнанных под прямую линию для данных, которые, как вы знаете не могут хорошо подходить под прямую линию.
Play video starting at :5:9 and follow transcript5:09
Таким образом, мы получаем гипотезы, которые, возможно, выглядят так.
Play video starting at :5:13 and follow transcript5:13
Теперь давайте подумаем, что бы произошло, если бы мы увеличили размер тренировочного набора. Так что если вместо пяти примеров, таких как я нарисовал там, представьте, что у нас есть намного больше обучающих примеров.
Play video starting at :5:25 and follow transcript5:25
Хорошо, что случится, если вы подгоните к этому прямую линию. То, что вы обнаружите, это то, что вы в конечном итоге вы знаете, это та же самая прямая линия. Я имею в виду прямую линию, которая просто не может соответствовать этим данным и получив тонны дополнительных данных, прямая линия не изменится значительно. Это самая лучшая прямая линия соответствующая этим данным, но прямая линия просто не подходит набору данных так хорошо. Если вы построите ошибку перекрестной проверки
Play video starting at :5:49 and follow transcript5:49
вот как это будет выглядеть.
Play video starting at :5:51 and follow transcript5:51
Вариант слева, если у вас уже есть минимальный размер тренировочного набора, может быть, только один пример обучения и не будет хорошим. Но к тому времени вы достигните определенного количества тренировочных примеров, у вас почти прямая линия и даже если вы в конечном итоге с более большим тренировочным набором, более большим значением m, вы в основном получаете ту же прямую линию, и так, ошибка перекрестной проверки - позвольте мне обозначить это - или ошибка набора тестов выравнивается довольно скоро, как только вы достигли за определенное число примеров обучения, если вы в значительной степени соответствуют наилучшей возможной прямой линии. А как насчет ошибки обучения? Что ж, ошибка обучения снова будет небольшой.
Play video starting at :6:34 and follow transcript6:34
И что вы обнаружите в случае высокого смещения что ошибка обучения закончится близко к ошибке перекрестной проверки, потому что вы так мало параметров и так много данных, по крайней мере, когда m большой. Результат на тренировочном наборе и наборе перекрестной проверки будут очень похожи.
Play video starting at :6:53 and follow transcript6:53
И вот, это то, как ваши кривые обучения будут выглядеть, если у вас есть алгоритм, который имеет большое смещение.
Play video starting at :7: and follow transcript7:00
И, наконец, проблема с с большим смещением отражена в тот факте, что оба ошибка перекрестной проверки и ошибка обучения большие, и так вы закончите с относительно высокой стоимостью обоих Jcv и Jtrain.
Play video starting at :7:15 and follow transcript7:15
Это также подразумевает что-то очень интересное, и вот что: если алгоритм обучения имеет большое смещение, в то время как мы берем больше и больше тренировочных примеров, мы двигается направо этого графика замечаем, что ошибка перекрестной проверки не сильно стремится вниз, в основном выпрямляется и если алгоритмы обучения действительно страдают от большого сдвига,
Play video starting at :7:36 and follow transcript7:36
беря больше обучающих данных мы в действительности не помогаем этому сильно, и как на нашем графике отображенном справа мы имеем только пять тренировочных примеров, и мы описали определенной прямой линией. и когда у нас тонны данных с примерами, мы в итоге заканчиваем приблизительно с той же прямой линией. И так, если алгоритм обучения имеет большое смещение, то дав гораздо больше данных тренировки, в действительности не поможет вам получить более минимальное значение ошибки перекрестной проверки или ошибки тестов. Знать, когда ваш алгоритм обучения страдает от большого смещения является полезной вещью, потому что это может предотвратить от траты большого количества времени, собирая больше тренировочных данных, когда это бесполезно. Далее давайте посмотрим на настройка алгоритма обучения имеющего высокую дисперсию.
Play video starting at :8:21 and follow transcript8:21
Давайте просто посмотрим на ошибка тренировки в случае если у тебя очень умная тренировка на пяти учебных примерах, показанных на картинке справа и мы мы подбираем полином очень высокого порядка
Play video starting at :8:34 and follow transcript8:34
и я написал полином 100 порядка, который в действительно никто не использует, просто для иллюстрации.
Play video starting at :8:39 and follow transcript8:39
И если мы используем довольно небольшое значение лямбда, может быть, не ноль, но довольно малое значение лямбда, то мы закончим, вы знаете, подгонку этих данных очень хорошо, что с функция, которая подходит для этого. Итак, если размер набора тренировки мал, наша ошибка тренировки Jtrain(theta) будет мала.
Play video starting at :9:3 and follow transcript9:03
И по мере увеличения размера этого тренировочного набора немного, вы знаете, мы можем все еще соотвествовать этим данным немного, но это также становится немного труднее подходить этому набору данных идеально, и так, как размер тренировочного набора увеличивается, мы увидим, что Jtrain увеличивается, потому что это просто немного сложнее соответствовать тренировочным данным идеально, когда у нас есть больше примеров, но ошибка обучающего набора все еще будет довольно низкой. Теперь, как насчет ошибки перекрестной проверки? При большой дисперсии, гипотеза будет переобучена и ошибка перекрестной проверки будет оставаться большой, даже когда мы будем использовать умеренное количество тренировочных примеров, и ошибка перекрестной проверки может выглядеть так. И ориентировочная проверка того, что мы имеем высокую проблему дисперсии,
Play video starting at :9:50 and follow transcript9:50
тот факт, что есть этот большой разрыв между ошибкой обучения и ошибкой перекрестной проверки.
Play video starting at :9:57 and follow transcript9:57
И глядя на этот график, если мы думаем о добавлении больше тренировочных данных, взять этот график и экстраполировать на право, мы можем сказать о том, что вы знаете, две кривые, синяя кривая и пурпурная кривая, сходятся друг к другу. И так, если бы мы экстраполируем эту цифру на право, то это это выглядит как будто ошибка тренировки продолжит идти вверх и
Play video starting at :10:27 and follow transcript10:27
ошибка перекрестной проверки будет продолжать снижаться. И что нас действительно волнует, так это ошибка перекрестной проверки или ошибка набора тестов, верно? На этом типе графика, мы можем сказать, что если мы продолжим добавлять тренировочные примеры и экстраполировать на право, наша ошибка перекрестной проверки будет продолжать спускаться. И вот, в случае большой дисперсии, получение дополнительных тренировочных данных, на самом деле, скорее всего, поможет. И снова, это действительно полезно знать, что ваш алгоритм обучения страдает из-за проблемы большой дисперсии, потому что это говорит вам, например, что это может быть стоит вашего времени что возможно стоит получить больше данных обучения.
Play video starting at :11:3 and follow transcript11:03
Теперь на предыдущем слайде и этом слайде я нарисовал довольно идеализированные кривые. Если вы строите эти кривые для фактического алгоритма обучения, иногда вы на самом деле увидите довольно много кривых, как те, что я нарисовал здесь. Хотя иногда видишь кривые которые немного шумнее и немного грязнее, чем это. Но построение кривых обучения вроде этих очень часто говорит нам и часто может помочь вам выяснить, что ваш алгоритм обучения страдает от большого смещения, или дисперсии или даже немного того и другого. Итак, когда я пытаясь улучшить производительность алгоритма обучения, одна вещь которую я почти всегда буду делать строить эти кривые обучения и обычно это даёт вам лучшее понимание того, есть ли проблема смещения или дисперсии.
Play video starting at :11:44 and follow transcript11:44
И в следующем видео посмотрим как это может помочь подсказать конкретные действия брать или не брать, для того, чтобы попытаться улучшить эффективность вашего алгоритма обучения.