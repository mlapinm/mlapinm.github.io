Regularized Linear Regression.Для линейной регрессии мы ранее разработали два алгоритма  обучения, базирующихся на градиентном спуске и на нормальном уравнении
Play video starting at ::8 and follow transcript0:08
В данном видео мы возьмем эти два алгоритма и обобщим их к случаю регуляризованной линейной регрессии. Вот задача оптимизации, которую мы придумали в прошлый раз для регуляризованной линейной регрессии. Вот это первая часть - наша обычная задача для случая линейной регрессии, а теперь у нас появилось вот это дополнительное слагаемое,  где ламбда - это наш параметр регуляризации, и мы хотим найти параметры тета, которые минимизируют функцию затрат, вот эту регуляризованную функцию затрат, J от тета. Раньше мы использовали
Play video starting at ::46 and follow transcript0:46
градиентный спуск для исходной функции затрат, без компонента регуляризации  и у нас был следующий алгоритм для обычной линейной регрессии без регуляризации.
Play video starting at ::54 and follow transcript0:54
Мы будем последовательно вычислять параметры тета-j по этой формуле, где j=1,2 и так далее до n. Я сейчас запишу уравнение для тета-0 отдельно. Я собираюсь просто отделить формулу для тета-0 отдельно от формул для параметров тета-1, 2, 3 ... n.Ничего ведь не изменилось, верно? Мы просто записываем формулу для тета-0 отдельно от формул для тета-1, тета-2, тета-3 и так далее до тета-n. Вот почему я хочу это сделать. Вы возможно помните, что для нашей регуляризованной линейной
Play video starting at :1:32 and follow transcript1:32
регрессии мы накладываем штраф на параметры тета-1. тета-2, и так далее до тета-n, но мы не штрафуем тета-0. Таким образом, когда мы изменяем алгоритм для случая регуляризованной линейной регрессии, мы в конце концов будем  вычислять с тета-0 немного по-другому.
Play video starting at :1:48 and follow transcript1:48
А именно, чтобы изменить  этот алгоритм с использованием регуляризационной задачи,  все что нам нужно, это взять вот эту нижнюю формулу  и изменить ее следующим образом. Мы возьмем этот элемент и прибавим  минус
Play video starting at :2:6 and follow transcript2:06
ламбда деленная на M, умноженная на тета-j. Если мы так сделаем, то получим градиентный спуск для минимизации регуляризированной функции затрат J от тета  в частности
Play video starting at :2:19 and follow transcript2:19
(я не собираюсь делать вычисления, чтобы доказать это), но если вы посмотрите на выражение в квадратных скобках.
Play video starting at :2:27 and follow transcript2:27
Если произвести вычисления, можно  доказать, что это выражение является частной производной по J от тета, используя новое определение  J от тета  с регуляризационным компонентом. И, кстати, по поводу этого компонента в верхней формуле  который я обвожу другим цветом, это тоже частная производная от J от тета к тета-0. Если вы посмотрите на формулу расчета тета-j, то можете увидеть нечто примечательное. А именно, тета-j изменяется как  тета-j минус альфа умноженное на .. здесь еще один компонент зависимый от тета-j. Таким образом,  если сгруппировать все элементы зависящие от тета-j, то мы можем показать, что  эту формулу можно заменить эквивалентно следующим образом. Все что я сделал - это вынес тета-j, здесь тета-j умноженное на 1 и в этом компоненте ламбда деленная на M. Здесь еще и альфа, итак наконец получаем  альфа на ламбда деленная на m,  это все умножить на тета-j и это выражение следующее: единица минус альфа на ламбда делить на M - довольно интересный член дает интересный эффект.
Play video starting at :3:42 and follow transcript3:42
А именно, этот член:  единица минус альфа на ламбду делить на M  - это число, это ограничение, которое дает всегда число меньше единицы, верно? Так как альфа на ламбду делить на M  будет положительным и обычно при обучении коэффициент мал, а M большое
Play video starting at :3:58 and follow transcript3:58
Он обычно довольно мал. Итак. этот член будет числом, которое совсем немного меньше единицы. Мы будем его представлять числом типа 0,99 или что-то такое.
Play video starting at :4:7 and follow transcript4:07
Результат наших изменений тета-j таков, что мы  говорим, что тета-j  заменяется тета-j умноженная на 0,99. Хорошо, тета-j умноженная на 0,99  уменьшает тета-j приближает ее немного в сторону нуля. Таким образом делая тета-j немного меньше. Более формально эта квадратная норма тета-j меньше. А за ней, второй член многочлена представляет собой ровно такой же,  как и в исходном градиентном спуске, который мы вывели. До того, как мы добавили сюда регуляризацию.
Play video starting at :4:44 and follow transcript4:44
Итак, я надеюсь, что этот градиентный спуск,эти преобразования вам понятны, когда мы используем регуляризованную линейную регрессию, то мы для каждого J мы умножали данные на число,немного меньше единицы,  так мы уменьшали параметр совсем немного. А затем мы производим преобразование, такое же как и раньше.
Play video starting at :5:4 and follow transcript5:04
Конечно же, это упрощенное объяснение того, что это преобразование делает. Математически, это именно градиентный спуск функции затрат J от тета, который мы определили на прошлом слайде, с использованием регуляризационной составляющей. Градиентный спуск - это только один из двух алгоритмов
Play video starting at :5:24 and follow transcript5:24
подходящих для модели линейной регрессии.
Play video starting at :5:26 and follow transcript5:26
Второй алгоритм основан на нормальном уравнении, где мы записали матрицу плана "X", в которой каждой строке соответствует один обучающий пример. И мы записали вектор "Y",  размерности M, который состоит из меток обучающей выборки. А "x" - это матрица размерностью M на N+1. Y - это вектор размерности M Для того, чтобы минимизировать функцию затрат J, мы установили, что один из способов  - это приравнять ее к следующему выражению. X транспонированное на X ...
Play video starting at :6:10 and follow transcript6:10
обратная матрица на X транспонированная на Y. Я оставляю тут пустое место, чтобы потом его заполнить. Это значение тета минимизирует функцию затрат J от тета в том случае, когда мы не используем регуляризацию. Теперь будем использовать регуляризацию. Чтобы вычислить минимум надо... я только напомню, как вычислять минимум. Надо взять частные производные относительного каждого из параметров, приравнять это к нулю, а затем немного математики, и можно показать, что эта вот формула минимизирует функцию затрат. А именно. если вы используете регуляризацию, то эта формула меняется следующим образом В этих скобках мы получаем вот такую матрицу Ноль, один, один, один и так далее до конца. Итак, получилась матрица, у которой левый верхний элемент - это ноль. Единицы на главной диагонали, а все остальные нули.
Play video starting at :7:13 and follow transcript7:13
Я рисую немного небрежно
Play video starting at :7:15 and follow transcript7:15
Рассмотрим конкретный пример, при N=2. Тогда эта матрица будет матрицей три на три. Обобщенно, это матрица размерности N+1 на N+1.
Play video starting at :7:31 and follow transcript7:31
Соответственно когда N равно двум, то матрица будет выглядеть так: Ноль, затем единицы на главной диагонали, и нули на всех остальных диагоналях.
Play video starting at :7:42 and follow transcript7:42
И опять-таки я не буду вычислять эти производные Это откровенно длинно и сложно Однако можно доказать, что с использованием новой формулы для J от тета, с регуляризационной составляющей.
Play video starting at :7:54 and follow transcript7:54
То эта новая формула для тета - именно та, которая даст нам глобальный минимум J от тета.
Play video starting at :8:1 and follow transcript8:01
В конце, я хочу вкратце описать  аспект необратимости.
Play video starting at :8:6 and follow transcript8:06
Это довольно продвинутый материал Можно считать, что этот материал необязателен, так что можете его пропустить. А если вы его прослушаете, но ничего не поймете, не переживайте об этом. Ранее когда я говорил про метод нормального уравнения У нас уже было одно необязательное видео про необратимость матрицы. Так вот это еще одна необязательная часть, в дополнение к предыдущему необязательному видео про необратимость.
Play video starting at :8:31 and follow transcript8:31
Допустим, что M (количество примеров) меньше N (количества признаков).
Play video starting at :8:38 and follow transcript8:38
Если у нас примеров меньше, чем признаков, тогда матрица X транспонированная умноженное на X будет необратимой или исключительной, или другое название такой матрицы - вырожденная. Если Вы реализуете это на языке Octave и используете функцию pinv для того, чтобы найти псевдо-обратную матрицу Он выдаст нечто похожее на правду, это не совсем понятно, но он выдаст вам очень хорошую гипотезу пусть даже численно реализованная функция языка Octave pinv выдаст вам разумный результат Но если реализовать это на другом языке И если воспользоваться обычным обращением матрицы,
Play video starting at :9:20 and follow transcript9:20
которая в Octave представлена при помощи функции Inv
Play video starting at :9:23 and follow transcript9:23
Мы пытаемся обратить обычным образом матрицу X транспонированное на X, затем на этом шаге мы находим, что X транспонированное на X - это особенная, необратимая матрица и если мы попытаемся сделать это на другом языке программирования и с использованием какой-нибудь библиотеки линейной алгебры, попытаемся обратить эту матрицу. Оно вполне возможно и не заработает, так как матрица необратимая или особенная.
Play video starting at :9:44 and follow transcript9:44
К счастью, регуляризация заботится и об этом, а именно, до тех пор, пока параметр регуляризации строго больше нуля. Можно доказать, что эта матрица X транспонированная на X плюс ламбда умноженное на вот эту забавную матрицу, можно доказать, что эта матрица не будет особенной и то что она будет обратимой.
Play video starting at :10:7 and follow transcript10:07
Итак при использовании регуляризации также можно не думать о проблеме необратимости матрицы X транспонированная на X. Теперь вы знаете, как реализовать регуляризацию линейной регрессии. Используя ее, вы сможете избежать избыточной точности, даже когда у вас много признаков и сравнительно небольшая обучающая выборка. И это позволит линейной регрессии работать гораздо лучше для большего количества задач.
Play video starting at :10:30 and follow transcript10:30
В следующем видео мы рассмотрим эту идею регуляризации и применим ее к логистической регрессии. С тем, чтобы вы смогли понять, как логистическая регрессия может помочь избежать избыточной точности и просто работать лучше.