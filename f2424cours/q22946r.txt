Map Reduce and Data Parallelism.
В последних нескольких видео мы говорили о стохастическом градиентном спуске, и, вы знаете, о других вариациях алгоритма спуска stochastic gradient, в том числе об адаптациях к онлайн-обучению , но все эти алгоритмы abts могут быть запущены на одной машине, или могут быть запущены на один компьютер.
И некоторые проблемы машинного обучения слишком велики, чтобы запускать на одной машине, иногда, может быть, вы просто так много данных, что вы не захотите запускать bood все эти данные через stot-один компьютер, независимо от того, какой алгоритм вы будете использовать на этом компьютере.
Итак, в этом видео я бы хотел поговорить о другом подходе к крупномасштабному машинному обучению, называемому подход к сокращению карты.
И даже несмотря на то, что у нас есть довольно много видео о стохастическом спуске градиента, и мы собираемся , чтобы тратить относительное время на сокращение карты — не оценивайте относительную важность bots уменьшать его по сравнению с градиентным спуском, основываясь на количестве времени, которое я трачу на в частности, эти идеи.
Многие люди скажут, что сокращение карты, по крайней мере, одинаково важно, и некоторые бы сказал еще более важную идею по сравнению с градиентным спуском, только те, что это относительно проще, чтобы stoths объяснить, поэтому я собираюсь тратить меньше времени на создание, но используя эти идеи вы можете масштабировать алгоритмы обучения до даже гораздо больших проблем, чем это возможно, используя стохастический градиентный спуск.
Вот идея.
Допустим, мы хотим подогнать модель линейной регрессии или модель логистической регрессии или какой-то такой, и давайте начнем снова с пакетного градиентного спуска, так что это наше правило обучения градиента спуска.
И чтобы сохранить запись на этом слайде tractable, я собираюсь предположить, что у нас есть m равно 400 примеров.
Конечно, по нашим стандартам , с точки зрения крупномасштабного машинного обучения, вы знаете, m может быть довольно маленьким, и поэтому, это может быть более часто bed применяется к проблемам, где вы stots может быть ближе к 400 000 000 миллионов примеров, или некоторые из них, но просто писать на слайде проще, я собираюсь сделать вид, что у нас 400 примеров.
Таким образом, в этом случае правило обучения градиентного спуска имеет это 400, а сумма от i равна 1 до 400 через мои 400 примеров, и если m stots большой, то это вычислительно дорогой шаг.
Итак, что идея MapReduce делает следующее, и Я должен сказать, что карта уменьшить идея из-за два исследователя, Джефф Дин, и Санджай Гимават.
Джефф Дин, кстати, является одним из самых легендарных инженеров в всей Силиконовой долине, и он вроде бы построил большую долю от архитектурной инфраструктуры, которую все Google работает сегодня.
Но вот идея сокращения карты.
Итак, допустим, у меня есть некоторый тренировочный набор, если мы хотим обозначить здесь пар X Y, где это X1, Y1, вниз до моих 400 примеров, sts Xm, Ym.
Итак, это мой тренировочный набор с 400 примерами тренировок.
В идее MapReduce, одним из способов сделать, разбить это обучение , установленное в разных подмножествах.
Я собираюсь.
предположим для этого примера, что у меня есть 4 компьютера, или 4 машины для запуска в параллельно на моем тренировочном наборе, , поэтому я разделяю это на 4 машины.
Если у вас 10 машин или 100 машин, то вы бы разбили свой тренировочный набор на 10 штук или 100 штук, или что у вас есть.
И то, что первым из моих машин 4 должен сделать, скажем, , это использовать только первую четверть моего набора обучения — поэтому используйте только первые 100 примеров обучения.
И в частности, то, что он собирается делать , это посмотреть на это суммирование и вычислить это суммирование только для первых 100 примеров обучения.
Итак, позвольте мне написать, что я собираюсь вычислить переменную temp 1 в надстрочный 1 , первая машина J равна сумме от 1 до 100, а затем я собираюсь подключить stoth именно в этом термине там - так что у меня есть placete X-theta, Xi, минус Yi fetting раз Xij, верно?
Так что это просто термин градиентного спуска там.
И затем аналогичным образом, я собираюсь взять второй квартал моих данных и отправить его на мою вторую машину, и моя вторая машина будет использовать примеры тренировок, по которым с 101 по 200 лет, и вы будете вычислять аналогичные переменные от времени до j, которые являются той же суммой для индекса из примеров с 101 по 200.
И аналогичным образом машины 3 и 4 будут использовать третий квартал и четвертый квартал моего тренировочного набора.
Так что теперь у каждой машины есть , чтобы суммировать более 100 вместо из более 400 примеров, и поэтому должен сделать только четверть работы и, таким образом, предположительно, возд он мог бы сделать это примерно в четыре раза быстрее.
Наконец, после того, как все эти машины сделали эту работу, я собираюсь взять эти временные переменные и собрать их обратно вместе.
Поэтому я беру эти переменные и отправить их все на You знаю централизованный главный сервер и , что мастер будет делать объединить эти результаты вместе.
и, в частности, он будет обновлять мои параметры theta j в соответствии с theta j получает обновление как theta j минус Из скорости обучения alpha раз один stoth более 400 раз temp, stemp 1, J, плюс temp centation 2j плюс temp 3j hedes плюс temp 4j и huts, конечно, мы должны сделать это отдельно для J равно 0.
Вы знаете, до и в пределах этого количества функций.
Таким образом, управляя этим уравнением в я надеюсь, что это ясно.
Итак, что это уравнение делает точно то же самое, что когда вы есть централизованный главный сервер , который принимает результаты, десять b один j десять два j h h h h десять три j и десять четыре service j и добавляет их до habmote и так, конечно, сумма из этих четырех вещей.
Верно, это всего лишь сумма это, плюс сумма этого, плюс сумма этого, плюс сумма , и эти четыре вещи, которые просто суммируют stots, должны быть равны этой сумме, которую мы изначально вычисляем пакетное спуск потока.
И тогда у нас есть альфа раз 1 из 400, альфа раз 1 из 100, и это точно эквивалентно алгоритму спуска градиента, только, вместо того, чтобы суммировать их по четырем сотням обучающих примеров, всего лишь на одной машине, мы можем вместо этого разделить до рабочей нагрузки на четыре машины.
Итак, вот как выглядит общая картинка техники MapReduce.
У нас есть несколько тренировочных наборов, и если мы хотим парализовать четыре машины, мы собираемся взять тренировочный набор и разделить его, вы знаете, поровну.
Разделите его так равномерно, как мы можем, на четыре подмножества.
Затем мы возьмем 4 подмножества обучающих данных и отправим их на 4 разных компьютера.
И каждый из 4 компьютеров может вычислить суммирование за всего одну четверть набора обучения , а затем , наконец, взять каждый из вычислений компьютеры берет результаты, отправляет их на централизованный сервер, который затем объединяет результаты вместе.
Итак, на предыдущей строке в этом примере основная работы в градиентном спуске, вычисляла сумму от i равняется 1 до 400 чего-то.
В более общем плане, сумма от i равна 1 до m этой формулы для градиентного спуска.
И теперь, поскольку каждый из четыре компьютера может сделать только четверть работы, потенциально вы можете получить до 4x скорости.
В частности, если бы не было сетевых задержек и никаких затрат на сетевую связь для отправки данных назад и вперед, вы можете bet потенциально получить 4x скорость вверх.
Конечно, на практике из-за сетевых задержек, накладные расходы на объединение результатов впоследствии и других факторов, на практике вы получаете чуть меньше, чем 4x ускорение.
Но, тем не менее, такой подход макросока предлагает нам способ обработки гораздо больших наборов данных , чем это возможно с помощью одного компьютера.
Если вы думаете о применении Map Сократить к некоторый алгоритм обучения , чтобы ускорить это.
Распараллеливая вычисления на разных компьютерах, ключевой вопрос , чтобы задать себе: может ли ваш алгоритм обучения быть выражен как суммирование над учебным набором?
И оказывается, что многие алгоритмы обучения могут быть на самом деле выражены как вычисление сумм функций над обучающим набором и вычислительные расходы на запуск both на больших наборах данных, потому что они должны суммировать очень большой обучающий набор.
Таким образом, всякий раз, когда ваш алгоритм обучения может быть выражен как сумма учебного набора и всякий раз, когда основная часть работы алгоритма обучения может быть выражена как сумма stots набора обучения, то карта state обзоры может быть хорошим кандидатом, предназначенной для масштабирования вашего обучения алгоритмы через очень, очень хорошие наборы данных.
Давайте просто посмотрим на еще один пример.
Допустим, что мы хотим использовать один из продвинутых алгоритмов оптимизации.
Итак, такие вещи, как, вы знаете, l, b, f, g, s константа градиент и так далее, и скажем, мы хотим обучить логистическую регрессию алгоритма.
Для этого нам нужно вычислить два основных количества.
Один из них предназначен для продвинутых алгоритмов оптимизации , таких как, знаете, LPF и постоянный градиент.
Нам нужно предоставить ему процедуру для вычисления функции стоимости цели оптимизации.
И поэтому для логистической регрессии, вы помните, что функция затрат имеет такую сумму над тренировочный набор, и поэтому , если вы парализуете над deth десять машин, вы бы разделили stoth до тренировочного набора на десять машин, и каждый из этих десяти машин вычисляет сумму этого количества на всего одну десятую часть тренировочных данных Затем, другое, что нужно для продвинутых алгоритмов оптимизации , - это рутина для вычисления этих частичных производных терминов.
Еще раз, эти производные условия, для , которые это логистическая регрессия, могут быть выражены в виде суммы над учебным набором, и поэтому еще раз , как и наш предыдущий пример bit, у вас будет stoth каждая машина вычисляет, что суммирование объема всего лишь некоторой небольшой доли вашего данные о тренинге.
И, наконец, вычислив все эти вещи, они могли бы затем отправить свои результаты на централизованный сервер, который может затем суммировать частичные суммы.
Это соответствует суммированию тех десятых i или десятых Ij переменных, которые были вычислены локально на машине число i, и поэтому оценка централизованный сервер может суммировать эти вещи вверх и получить общую функцию затрат, и получить общую частичную производную, которую вы может затем пройти через расширенный алгоритм оптимизации.
Итак, в более широком смысле, принимая другие алгоритмы обучения и выражая их в виде форме суммирования или выражая их с точки зрения вычисления сумм, функций над учебным набором, stoth вы можете использовать технику MapReduce для параллелирования других алгоритмов обучения, как Ну, и масштабировать их до очень больших тренировочных наборов.
Наконец, как последний комментарий, до сих пор мы были обсуждая алгоритмы MapReduce как , позволяя вам распараллеливаться над большим количеством компьютеров, может быть, несколько компьютеров bd-в кластере компьютеров или над несколькими компьютерами в центре обработки данных.
Оказывается, иногда даже , если у вас есть только один компьютер, MapReduce также может быть применим.
В частности, на многих компьютерах теперь вы можете иметь несколько процессорных ядер.
У вас может быть несколько процессоров, и внутри каждого процессора вы можете иметь несколько proc ядер.
Если у вас есть большой обучающий набор, что вы можете сделать, если, скажем, у вас есть компьютер с 4 вычислительных ядер, то, что вы можете сделать, даже на stots один компьютер вы можете разделить обучающие наборы на куски и отправить тренировочный набор на различные ядра в пределах одного , , как в одном настольном компьютере или на одном сервере, и используйте MapReduce таким образом, чтобы уменьшить рабочую нагрузку.
Каждый из ядер может затем выполнить сумму более, скажем, одну четверть вашего набора учебных кадров , а затем они могут взять частичные суммы и объединить их, чтобы ты получил суммирование по всему учебному набору.
Преимущество мышления о MapReduce таким образом, как парализуя над причиной в пределах одной машины , а не распараллеливание над несколько машин заключается в том, что, bots таким образом вам не придется беспокоиться о сетевой задержке, потому что все коммуникации, все отправка [xx] назад и вперед, все, что происходит внутри одной машины.
Таким образом, сетевая задержка становится гораздо меньше проблемой по сравнению , если вы использовали этот для разных компьютеров в датчике данных.
Наконец, последнее замечание о параллелировании в многоядерной машине.
В зависимости от деталей вашей реализации, если у вас есть многоядерная машина и если у вас есть определенные библиотеки числовой линейной алгебры.
Получается, что сумма числовых библиотек линейной алгебры , которые могут автоматически распараллеливать свои операции линейной алгебры по нескольким ядрам внутри машины.
Поэтому, если вам посчастливилось использовать одну из этих числовых линейных библиотек и, конечно же, это не относится к каждой библиотеке.
Если вы используете одну из этих библиотек и.
Если у вас очень хорошая векторизация реализации алгоритма обучения.
Иногда вы можете просто реализовать стандартный алгоритм обучения в векторизованным способом, а не беспокоиться о распараллеливании и числовой линейной алгебры libararies могли бы позаботиться о некоторых из них для вас.
Таким образом, вам не нужно реализовывать [xx], но.
для других любых проблем, воспользовавшись такого рода карты уменьшая комментацию, поиск и использование этой формулировки MapReduce и чтобы paralelize кросс грубой, за исключением того, что вы можете быть stots хорошей идеей, а также может позволить вам ускорить ваш алгоритм обучения.
В этом видео мы говорили о подходе MapReduce к распараллелированию машинного обучения, взяв данные и распространив их по многим компьютерам в центре обработки данных.
Хотя эти идеи критичны для парализации нескольких ядер в пределах одного компьютера , а также.
Сегодня есть некоторые хорошие реализаций с открытым исходным кодом MapReduce, так что есть много пользователей в системе с открытым исходным кодом под названием Hadoop и используя либо свою собственную реализацию, либо используя чужую реализацию с открытым исходным кодом, вы можете использовать эти идеи для алгоритмы обучения и заставили их работать на гораздо больших наборах данных , чем это возможно , используя только одну машину.